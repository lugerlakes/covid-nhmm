{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae9c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CVAE-LSTM (heteroscedastic) + nHMM-like soft transitions +\n",
    "# ARIMA & Persistence baselines + metrics + visualizations\n",
    "#\n",
    "# What you get:\n",
    "#   - Robust checkpoint loader (adapts dims, adds log-variance head)\n",
    "#   - Two evaluation modes:\n",
    "#       (A) Tail-only (last TEST_DAYS)    -> matches your original setup\n",
    "#       (B) Rolling / Regime-stratified   -> recommended for the paper\n",
    "#   - Free-run reconstruction of full curves (shape metrics)\n",
    "#   - Publication-ready plots\n",
    "#   - Comparative summary table by commune\n",
    "# ============================================================\n",
    "\n",
    "import os, warnings, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# sklearn compatibility (sparse_output -> sparse for older versions)\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder as _OHE\n",
    "def OneHotEncoder_compat(**kwargs):\n",
    "    try:\n",
    "        # Newer sklearn uses 'sparse_output'\n",
    "        return _OHE(sparse_output=False, **{k:v for k,v in kwargs.items() if k != \"sparse\"})\n",
    "    except TypeError:\n",
    "        # Older sklearn uses 'sparse'\n",
    "        return _OHE(sparse=False, **kwargs)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.set_printoptions(suppress=True, linewidth=160)\n",
    "\n",
    "# -------------------------\n",
    "# Paths & configuration\n",
    "# -------------------------\n",
    "ROOT = 'path/to'\n",
    "DATA_CSV = os.path.join(ROOT, \"covid_data_with_6_states.csv\")\n",
    "MODEL_PTH = os.path.join(ROOT, \"cvae_lstm_model_normalized.pth\")\n",
    "OUT_DIR   = os.path.join(ROOT, \"eval_out\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 123\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# Time series & conditioning\n",
    "FEATURES = [\"Gross_Daily_Cases_Mobile_Average_7_Days\",\n",
    "            \"Internal_Mobility_Index\", \"External_Mobility_Index\"]\n",
    "CASE_COL = FEATURES[0]\n",
    "SEQ_LEN  = 7  # LSTM window length\n",
    "\n",
    "# Default CVAE hyperparams (used if training from scratch)\n",
    "LATENT_DIM = 6\n",
    "HIDDEN_DIM = 64\n",
    "DEC_HIDDEN = 64\n",
    "BETA_KL    = 0.25     # lower beta -> freer latent space\n",
    "ALPHA_MSE  = 0.20     # small weight on full-feature MSE\n",
    "LR         = 1e-3\n",
    "EPOCHS     = 50\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Optional quick fine-tune after loading a checkpoint (to calibrate new logvar row)\n",
    "FINETUNE_AFTER_LOAD_EPOCHS = 6\n",
    "\n",
    "# Ensembles & horizons\n",
    "N_ENSEMBLE = 200\n",
    "H_LIST     = [7, 14]\n",
    "TEST_DAYS  = 28              # tail used in \"A) Tail-only\" evaluation\n",
    "TEMP_Z     = 1.75            # latent temperature\n",
    "TEMP_OUT   = 1.50            # output variance temperature (inflates σ)\n",
    "\n",
    "# ARIMA search grid (compact & robust)\n",
    "ARIMA_GRID = [(1,0,0),(1,1,0),(0,1,1),(1,1,1),(2,1,1)]\n",
    "\n",
    "# Target communes for figures\n",
    "TARGET_COMMUNES = [\"La Florida\", \"Cerrillos\", \"Vitacura\",\n",
    "                   \"Providencia\", \"Las Condes\", \"Santiago\"]\n",
    "\n",
    "# ---------------------------------\n",
    "# Utilities: sequences\n",
    "# ---------------------------------\n",
    "def create_sequences(X, C, Y, seq_len=SEQ_LEN):\n",
    "    \"\"\"Build (X_seq, cond_at_t, Y_t) for one-step-ahead supervised learning.\"\"\"\n",
    "    Xs, Cs, Ys = [], [], []\n",
    "    for i in range(len(X)-seq_len):\n",
    "        Xs.append(X[i:i+seq_len])\n",
    "        Cs.append(C[i+seq_len])    # condition at prediction time\n",
    "        Ys.append(Y[i+seq_len])    # next-step target (full feature vector)\n",
    "    return np.array(Xs), np.array(Cs), np.array(Ys)\n",
    "\n",
    "# ---------------------------------\n",
    "# Scoring metrics\n",
    "# ---------------------------------\n",
    "def mae(y, yhat):  return float(np.mean(np.abs(np.asarray(y)-np.asarray(yhat))))\n",
    "def rmse(y, yhat): return float(np.sqrt(np.mean((np.asarray(y)-np.asarray(yhat))**2)))\n",
    "\n",
    "def crps_ensemble(y, samples):\n",
    "    \"\"\"CRPS for ensemble samples (Gneiting & Raftery).\"\"\"\n",
    "    s = np.sort(np.asarray(samples).ravel())\n",
    "    n = len(s)\n",
    "    y = float(y)\n",
    "    e1 = np.mean(np.abs(s - y))                # E|X - y|\n",
    "    diffs = np.diff(s)\n",
    "    weights = np.arange(1, n) * (n - np.arange(1, n))\n",
    "    e2 = 2.0 * np.sum(weights * diffs) / (n*n) # E|X - X'|\n",
    "    return float(e1 - 0.5*e2)\n",
    "\n",
    "def interval_score(y, lo, hi, alpha):\n",
    "    y = float(y); lo = float(lo); hi = float(hi)\n",
    "    width = hi - lo\n",
    "    penalty = 0.0\n",
    "    if y < lo: penalty = (2.0/alpha) * (lo - y)\n",
    "    elif y > hi: penalty = (2.0/alpha) * (y - hi)\n",
    "    return width + penalty\n",
    "\n",
    "def wis_from_quantiles(y, interval_levels, qdict):\n",
    "    \"\"\"\n",
    "    Weighted Interval Score with median component.\n",
    "    interval_levels e.g. [0.5, 0.9]. qdict keys must include:\n",
    "    0.5 and (1±alpha)/2 for each alpha.\n",
    "    \"\"\"\n",
    "    if 0.5 not in qdict:\n",
    "        raise KeyError(\"qdict must contain the median under key 0.5\")\n",
    "    total = float(abs(float(y) - float(qdict[0.5])))\n",
    "    for alpha in interval_levels:\n",
    "        lo_key = round((1.0 - alpha)/2.0, 4)\n",
    "        hi_key = round(1.0 - (1.0 - alpha)/2.0, 4)\n",
    "        if lo_key not in qdict or hi_key not in qdict:\n",
    "            raise KeyError(f\"Missing quantiles for alpha={alpha}: need keys {lo_key} and {hi_key}\")\n",
    "        total += (alpha/2.0) * interval_score(y, qdict[lo_key], qdict[hi_key], alpha)\n",
    "    denom = 1.0 + np.sum([a/2.0 for a in interval_levels])\n",
    "    return float(total / denom)\n",
    "\n",
    "def empirical_quantiles(samples, qs=(0.05,0.25,0.5,0.75,0.95)):\n",
    "    s = np.asarray(samples).ravel()\n",
    "    return {round(q,4): float(np.quantile(s, q)) for q in qs}\n",
    "\n",
    "def coverage(y, lo, hi):\n",
    "    y = float(y); lo = float(lo); hi = float(hi)\n",
    "    return 1.0 if (lo <= y <= hi) else 0.0\n",
    "\n",
    "# ============================\n",
    "# Heteroscedastic CVAE-LSTM\n",
    "# + robust checkpoint loader\n",
    "# ============================\n",
    "class CVAE_LSTM_HET(nn.Module):\n",
    "    \"\"\"\n",
    "    Heteroscedastic CVAE with LSTM encoder.\n",
    "      - Encoder: LSTM -> [mu_z, logvar_z]\n",
    "      - Decoder: [z, cond] -> dense -> [mu_full (D), logvar_case (1)]\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, cond_dim, latent_dim, hidden_dim, dec_hidden=64):\n",
    "        super().__init__()\n",
    "        self.input_dim  = input_dim\n",
    "        self.cond_dim   = cond_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dec_hidden = dec_hidden\n",
    "\n",
    "        self.lstm_enc = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_mu    = nn.Linear(hidden_dim + cond_dim, latent_dim)\n",
    "        self.fc_logv  = nn.Linear(hidden_dim + cond_dim, latent_dim)\n",
    "\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(latent_dim + cond_dim, dec_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dec_hidden, input_dim + 1)  # +1 for log-variance of 'case'\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        # x: (B, T, D), c: (B, C)\n",
    "        _, (h, _) = self.lstm_enc(x)\n",
    "        hc = torch.cat([h[-1], c], dim=1)\n",
    "        mu = self.fc_mu(hc)\n",
    "        logv = self.fc_logv(hc)\n",
    "        return mu, logv\n",
    "\n",
    "    def reparam(self, mu, logv, temp=1.0):\n",
    "        std = torch.exp(0.5 * logv)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + temp * std * eps\n",
    "\n",
    "    def forward(self, x, c, temp=1.0):\n",
    "        mu_z, logv_z = self.encode(x, c)\n",
    "        z = self.reparam(mu_z, logv_z, temp=temp)\n",
    "        out = self.dec(torch.cat([z, c], dim=1))\n",
    "        mu_full     = out[:, :-1]  # means for all features\n",
    "        logvar_case = out[:, -1:]  # log-variance for case only\n",
    "        return mu_full, logvar_case, mu_z, logv_z\n",
    "\n",
    "def nll_gaussian_case(y_case, mu_case, logvar_case, out_temp=1.0):\n",
    "    \"\"\"Gaussian NLL on the 'case' dimension, with variance temperature scaling.\"\"\"\n",
    "    adj_logvar = logvar_case + 2.0*math.log(out_temp)\n",
    "    return 0.5 * (adj_logvar + (y_case - mu_case)**2 / torch.exp(adj_logvar))\n",
    "\n",
    "def cvae_loss_hetero(mu_full, logvar_case, y_full, mu_z, logv_z,\n",
    "                     alpha_mse=0.2, beta_kl=0.25, out_temp=1.0):\n",
    "    \"\"\"Total loss = NLL(case) + alpha*MSE(full features) + beta*KL.\"\"\"\n",
    "    y_case  = y_full[:, :1]\n",
    "    mu_case = mu_full[:, :1]\n",
    "    nll = nll_gaussian_case(y_case, mu_case, logvar_case, out_temp=out_temp).mean()\n",
    "    mse = nn.functional.mse_loss(mu_full, y_full)\n",
    "    kl  = -0.5 * torch.mean(1 + logv_z - mu_z.pow(2) - torch.exp(logv_z))\n",
    "    return nll + alpha_mse*mse + beta_kl*kl\n",
    "\n",
    "def infer_ckpt_dims(state_dict, input_dim):\n",
    "    \"\"\"\n",
    "    Infer hidden_dim, latent_dim, dec_hidden, cond_dim, and whether the\n",
    "    checkpoint already contains the extra log-variance row in the last\n",
    "    decoder layer.\n",
    "    \"\"\"\n",
    "    w_ih = state_dict['lstm_enc.weight_ih_l0']          # (4*H, input_dim)\n",
    "    hidden_old = w_ih.shape[0] // 4\n",
    "\n",
    "    latent_old = state_dict['fc_mu.weight'].shape[0]    # rows = latent_dim\n",
    "\n",
    "    dec0_w = state_dict['dec.0.weight']                 # (dec_hidden, latent + cond)\n",
    "    dec_hidden_old = dec0_w.shape[0]\n",
    "    cond_old = dec0_w.shape[1] - latent_old\n",
    "\n",
    "    # Rows of the last decoder layer (output size in the checkpoint)\n",
    "    out_old = state_dict['dec.2.weight'].shape[0]\n",
    "\n",
    "    # Accept either μ-only (input_dim) or μ+logvar (input_dim+1)\n",
    "    if out_old not in (input_dim, input_dim + 1):\n",
    "        raise ValueError(\n",
    "            f\"Incompatible checkpoint: decoder rows={out_old}, \"\n",
    "            f\"but expected {input_dim} (μ only) or {input_dim+1} (μ+logvar). \"\n",
    "            \"Did FEATURES change?\"\n",
    "        )\n",
    "\n",
    "    has_logvar_in_ckpt = (out_old == input_dim + 1)\n",
    "    return hidden_old, latent_old, dec_hidden_old, cond_old, has_logvar_in_ckpt\n",
    "\n",
    "\n",
    "def build_and_load_robust(INPUT_DIM, COND_DIM, ckpt_path, device):\n",
    "    \"\"\"\n",
    "    - Load state_dict from ckpt.\n",
    "    - Rebuild model using dims from ckpt (so shapes fit).\n",
    "    - Ensure the model's decoder outputs INPUT_DIM + 1 (μ of all features + logvar for cases).\n",
    "      * If ckpt already has that extra row, copy as-is.\n",
    "      * If ckpt has only INPUT_DIM rows, expand and zero-init the new row.\n",
    "    - Copy all compatible weights; incompatible remain randomly initialized.\n",
    "    \"\"\"\n",
    "    assert os.path.exists(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n",
    "    sd = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    H_old, Z_old, DEC_old, COND_old, has_logvar = infer_ckpt_dims(sd, INPUT_DIM)\n",
    "\n",
    "    if COND_old != COND_DIM:\n",
    "        print(f\"⚠️  Current COND_DIM={COND_DIM} differs from checkpoint={COND_old}. \"\n",
    "              \"I will load all shape-compatible tensors; the first decoder layer \"\n",
    "              \"may remain randomly initialized.\")\n",
    "\n",
    "    print(f\"→ Checkpoint dims detected: hidden={H_old}, latent={Z_old}, \"\n",
    "          f\"dec_hidden={DEC_old}, cond_dim(ckpt)={COND_old}, \"\n",
    "          f\"logvar_in_ckpt={has_logvar}\")\n",
    "\n",
    "    # Build model with ckpt dims (decoder output = INPUT_DIM + 1)\n",
    "    model = CVAE_LSTM_HET(INPUT_DIM, COND_DIM, Z_old, H_old, dec_hidden=DEC_old).to(device)\n",
    "    new_sd = model.state_dict()\n",
    "\n",
    "    # Copy 1:1 matching tensors first\n",
    "    for k, v in sd.items():\n",
    "        if k in new_sd and new_sd[k].shape == v.shape:\n",
    "            new_sd[k] = v.clone()\n",
    "\n",
    "    # Handle the last decoder layer explicitly\n",
    "    # Our model expects (INPUT_DIM + 1, DEC_old) in dec.2.weight and bias\n",
    "    W_new = new_sd['dec.2.weight']\n",
    "    b_new = new_sd['dec.2.bias']\n",
    "\n",
    "    if has_logvar:\n",
    "        # CKPT already has μ+logvar → if shapes match, we already copied above.\n",
    "        # If not (e.g., cond_dim mismatch changed the in_features of dec.2), we at least\n",
    "        # copy the rows that match in width.\n",
    "        W_ck = sd['dec.2.weight']\n",
    "        b_ck = sd['dec.2.bias']\n",
    "        if W_ck.shape == W_new.shape:\n",
    "            pass  # already copied\n",
    "        else:\n",
    "            # Fallback: copy overlapping columns, keep the rest as initialized\n",
    "            cols = min(W_ck.shape[1], W_new.shape[1])\n",
    "            rows = min(W_ck.shape[0], W_new.shape[0])\n",
    "            W_new[:rows, :cols] = W_ck[:rows, :cols]\n",
    "            b_new[:rows] = b_ck[:rows]\n",
    "            new_sd['dec.2.weight'] = W_new\n",
    "            new_sd['dec.2.bias']   = b_new\n",
    "    else:\n",
    "        # CKPT had only μ (INPUT_DIM rows) → expand with one extra row for logvar\n",
    "        W_ck = sd['dec.2.weight']   # (INPUT_DIM, DEC_old_ck)\n",
    "        b_ck = sd['dec.2.bias']     # (INPUT_DIM,)\n",
    "        rows_old, cols_old = W_ck.shape\n",
    "        # Copy existing μ rows into the first INPUT_DIM rows\n",
    "        cols = min(cols_old, W_new.shape[1])\n",
    "        W_new[:rows_old, :cols] = W_ck[:, :cols]\n",
    "        b_new[:rows_old] = b_ck\n",
    "        # Initialize the extra logvar row to zeros\n",
    "        nn.init.zeros_(W_new[rows_old:, :])\n",
    "        nn.init.zeros_(b_new[rows_old:])\n",
    "        new_sd['dec.2.weight'] = W_new\n",
    "        new_sd['dec.2.bias']   = b_new\n",
    "\n",
    "    # Finally load into the model\n",
    "    missing, unexpected = model.load_state_dict(new_sd, strict=False)\n",
    "    print(f\"✓ Checkpoint loaded with adaptation. Missing={list(missing)}, Unexpected={list(unexpected)}\")\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# Load data & encoders\n",
    "# ---------------------------------\n",
    "df = pd.read_csv(DATA_CSV, parse_dates=[\"Date\"])\n",
    "df = df.sort_values([\"Commune\",\"Date\"]).reset_index(drop=True)\n",
    "\n",
    "# Guardamos la columna de casos en UNIDADES ORIGINALES bajo un nombre fijo\n",
    "OBS_COL = \"Observed_Cases\"\n",
    "df[OBS_COL] = df[CASE_COL].values  # copia explícita\n",
    "\n",
    "# Global MinMax scaler (fit on ALL features, incluyendo casos)\n",
    "scaler = MinMaxScaler()\n",
    "X_all = df[FEATURES].values\n",
    "X_scaled = scaler.fit_transform(X_all)\n",
    "\n",
    "# df_scaled: igual a df pero con FEATURES escalados; mantenemos OBS_COL en original\n",
    "df_scaled = df.copy()\n",
    "df_scaled[FEATURES] = X_scaled  # OJO: CASE_COL queda escalado aquí a propósito\n",
    "# pero OBS_COL queda en original y es la que usaremos para verdad/ARIMA\n",
    "\n",
    "# (opcional) sanity check útil\n",
    "assert df_scaled[OBS_COL].max() > 5.0, \"Observed_Cases parece estar escalado; revisa el flujo.\"\n",
    "\n",
    "# Hidden states → enteros 0..K-1\n",
    "state_vals = sorted(df_scaled[\"Hidden_State\"].dropna().unique().tolist())\n",
    "state_to_id = {s:i for i,s in enumerate(state_vals)}\n",
    "id_to_state = {i:s for s,i in state_to_id.items()}\n",
    "K_STATES = len(state_vals)\n",
    "print(f\"Detected hidden-state categories (ordered): {state_vals}  -> K={K_STATES}\")\n",
    "\n",
    "df_scaled[\"state_id\"] = df_scaled[\"Hidden_State\"].map(state_to_id).astype(int)\n",
    "\n",
    "# Commune encoder (one-hot)\n",
    "enc_comm = OneHotEncoder_compat(handle_unknown=\"ignore\")\n",
    "comm_cond = enc_comm.fit_transform(df_scaled[[\"Commune\"]])  # (N, C)\n",
    "\n",
    "# State encoder (para entrenamiento duro)\n",
    "enc_state = OneHotEncoder_compat(categories=[state_vals], handle_unknown=\"ignore\")\n",
    "state_oh = enc_state.fit_transform(df_scaled[[\"Hidden_State\"]])\n",
    "\n",
    "# Full conditioning = [comm_onehot, state_onehot]\n",
    "COND_FULL = np.concatenate([comm_cond, state_oh], axis=1)\n",
    "COND_DIM  = COND_FULL.shape[1]\n",
    "INPUT_DIM = len(FEATURES)\n",
    "\n",
    "# Guardamos min/max por si los quieres inspeccionar\n",
    "CASE_MIN, CASE_MAX = scaler.data_min_[0], scaler.data_max_[0]\n",
    "\n",
    "# ---------------------------------\n",
    "# Train/test split by time (tail-only mode)\n",
    "# ---------------------------------\n",
    "def time_split_by_commune(df_in, test_days=TEST_DAYS):\n",
    "    marks = []\n",
    "    for comm, d in df_in.groupby(\"Commune\", sort=False):\n",
    "        n = len(d)\n",
    "        cut = max(0, n - test_days)\n",
    "        marks.append(pd.Series([0]*cut + [1]*(n-cut), index=d.index))\n",
    "    return pd.concat(marks).sort_index().values\n",
    "\n",
    "is_test = time_split_by_commune(df_scaled, TEST_DAYS)\n",
    "df_scaled[\"is_test\"] = is_test\n",
    "\n",
    "# ---------------------------------\n",
    "# Tensors for training CVAE (train slice only)\n",
    "# ---------------------------------\n",
    "train_df = df_scaled[df_scaled[\"is_test\"]==0].copy()\n",
    "X_train_raw = train_df[FEATURES].values\n",
    "C_train_raw = COND_FULL[train_df.index]\n",
    "Y_train_raw = train_df[FEATURES].values  # next-step target built in sequence\n",
    "\n",
    "X_seq, C_seq, Y_seq = create_sequences(X_train_raw, C_train_raw, Y_train_raw, seq_len=SEQ_LEN)\n",
    "Xt = torch.tensor(X_seq, dtype=torch.float32)\n",
    "Ct = torch.tensor(C_seq, dtype=torch.float32)\n",
    "Yt = torch.tensor(Y_seq, dtype=torch.float32)\n",
    "\n",
    "train_ds = torch.utils.data.TensorDataset(Xt, Ct, Yt)\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ---------------------------------\n",
    "# Build model & load checkpoint (or train from scratch)\n",
    "# ---------------------------------\n",
    "if Path(MODEL_PTH).exists():\n",
    "    model = build_and_load_robust(INPUT_DIM, COND_DIM, MODEL_PTH, DEVICE)\n",
    "    # Optional: quick fine-tune to calibrate the new logvar row\n",
    "    if FINETUNE_AFTER_LOAD_EPOCHS > 0:\n",
    "        print(f\"→ Quick fine-tune for {FINETUNE_AFTER_LOAD_EPOCHS} epochs to calibrate logvar row...\")\n",
    "        opt = optim.Adam(model.parameters(), lr=LR)\n",
    "        model.train()\n",
    "        for ep in range(FINETUNE_AFTER_LOAD_EPOCHS):\n",
    "            tot = 0.0\n",
    "            for xb, cb, yb in train_loader:\n",
    "                xb, cb, yb = xb.to(DEVICE), cb.to(DEVICE), yb.to(DEVICE)\n",
    "                opt.zero_grad()\n",
    "                mu_full, logvar_case, mu_z, logv_z = model(xb, cb, temp=1.0)\n",
    "                loss = cvae_loss_hetero(mu_full, logvar_case, yb, mu_z, logv_z,\n",
    "                                        alpha_mse=ALPHA_MSE, beta_kl=BETA_KL, out_temp=1.0)\n",
    "                loss.backward(); opt.step()\n",
    "                tot += loss.item() * xb.size(0)\n",
    "            print(f\"[FT {ep+1:02d}] loss={tot/len(train_ds):.4f}\")\n",
    "        torch.save(model.state_dict(), MODEL_PTH)\n",
    "        print(\"✓ Fine-tuned weights saved.\")\n",
    "    model.eval()\n",
    "else:\n",
    "    print(\"No checkpoint found. Training from scratch...\")\n",
    "    model = CVAE_LSTM_HET(INPUT_DIM, COND_DIM, LATENT_DIM, HIDDEN_DIM, dec_hidden=DEC_HIDDEN).to(DEVICE)\n",
    "    opt = optim.Adam(model.parameters(), lr=LR)\n",
    "    model.train()\n",
    "    for ep in range(EPOCHS):\n",
    "        tot = 0.0\n",
    "        for xb, cb, yb in train_loader:\n",
    "            xb, cb, yb = xb.to(DEVICE), cb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            mu_full, logvar_case, mu_z, logv_z = model(xb, cb, temp=1.0)\n",
    "            loss = cvae_loss_hetero(mu_full, logvar_case, yb, mu_z, logv_z,\n",
    "                                    alpha_mse=ALPHA_MSE, beta_kl=BETA_KL, out_temp=1.0)\n",
    "            loss.backward(); opt.step()\n",
    "            tot += loss.item() * xb.size(0)\n",
    "        print(f\"[Epoch {ep+1:02d}] loss={tot/len(train_ds):.4f}\")\n",
    "    torch.save(model.state_dict(), MODEL_PTH)\n",
    "    print(f\"✓ Trained & saved: {MODEL_PTH}\")\n",
    "    model.eval()\n",
    "\n",
    "# ---------------------------------\n",
    "# Non-homogeneous transition model (multinomial LR)\n",
    "# P(S_t | S_{t-1}, mobility, commune)\n",
    "# ---------------------------------\n",
    "def fit_transition_softmax(df_all):\n",
    "    \"\"\"\n",
    "    Fit multinomial logistic regression for state transitions using:\n",
    "    one-hot(S_{t-1}), mobility at t (IM, EM), and commune one-hot.\n",
    "    Uses ONLY the training slice to avoid leakage.\n",
    "    \"\"\"\n",
    "    use_df = df_all[df_all[\"is_test\"]==0].copy()\n",
    "    use_df[\"s_prev\"] = use_df.groupby(\"Commune\")[\"state_id\"].shift(1)\n",
    "    use_df[\"im_prev\"] = use_df.groupby(\"Commune\")[\"Internal_Mobility_Index\"].shift(0)\n",
    "    use_df[\"em_prev\"] = use_df.groupby(\"Commune\")[\"External_Mobility_Index\"].shift(0)\n",
    "    use_df = use_df.dropna(subset=[\"s_prev\"]).copy()\n",
    "    use_df[\"s_prev\"] = use_df[\"s_prev\"].astype(int)\n",
    "\n",
    "    X_comm = enc_comm.transform(use_df[[\"Commune\"]])\n",
    "    X_mob  = use_df[[\"im_prev\",\"em_prev\"]].values\n",
    "    enc_prev = OneHotEncoder_compat(categories=[list(range(K_STATES))])\n",
    "    X_sp   = enc_prev.fit_transform(use_df[[\"s_prev\"]])\n",
    "\n",
    "    X = np.hstack([X_sp, X_mob, X_comm])\n",
    "    y = use_df[\"state_id\"].values.astype(int)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=300, multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "trans_clf = fit_transition_softmax(df_scaled)\n",
    "\n",
    "def transition_proba_soft(df_slice_row):\n",
    "    \"\"\"Return P(S_t | S_{t-1}, mobility, commune) as vector length K_STATES.\"\"\"\n",
    "    s_prev = int(df_slice_row[\"state_id\"])\n",
    "    im = float(df_slice_row[\"Internal_Mobility_Index\"])\n",
    "    em = float(df_slice_row[\"External_Mobility_Index\"])\n",
    "    comm_vec = enc_comm.transform([[df_slice_row[\"Commune\"]]])\n",
    "    sp = np.zeros((1,K_STATES)); sp[0, s_prev] = 1.0\n",
    "    X = np.hstack([sp, np.array([[im,em]]), comm_vec])\n",
    "    p = trans_clf.predict_proba(X)[0]\n",
    "    p = np.clip(p, 1e-8, 1.0)\n",
    "    return p / p.sum()\n",
    "\n",
    "# ---------------------------------\n",
    "# Forecast helpers (ARIMA & Persistence)\n",
    "# ---------------------------------\n",
    "def persistence_forecast(y_hist, horizon, n_samples=200):\n",
    "    \"\"\"Deterministic point forecast + residual bootstrap for intervals.\"\"\"\n",
    "    last = float(y_hist[-1])\n",
    "    mean = np.full(horizon, last)\n",
    "    resid = np.diff(y_hist[-30:]) if len(y_hist) > 30 else np.diff(y_hist)\n",
    "    if resid.size < 2:\n",
    "        resid = np.array([0.0, 0.0, 0.0])\n",
    "    samples = np.zeros((horizon, n_samples))\n",
    "    for h in range(horizon):\n",
    "        noise = np.random.choice(resid, size=n_samples, replace=True)\n",
    "        samples[h,:] = mean[h] + noise\n",
    "    return mean, samples\n",
    "\n",
    "def arima_forecast(y_hist, horizon, n_samples=200, max_train=180):\n",
    "    \"\"\"\n",
    "    Compact ARIMA grid on recent window; sample N(μ, σ^2_pred) for intervals.\n",
    "    \"\"\"\n",
    "    y_train = np.asarray(y_hist[-max_train:], dtype=float)\n",
    "    best_aic, best_res = np.inf, None\n",
    "    for order in ARIMA_GRID:\n",
    "        try:\n",
    "            res = ARIMA(y_train, order=order).fit()\n",
    "            if res.aic < best_aic:\n",
    "                best_aic, best_res = res.aic, res\n",
    "        except Exception:\n",
    "            continue\n",
    "    if best_res is None:\n",
    "        return persistence_forecast(y_hist, horizon, n_samples)\n",
    "\n",
    "    fc = best_res.get_forecast(steps=horizon)\n",
    "    mean_fc = np.asarray(fc.predicted_mean)\n",
    "    var_fc  = np.asarray(fc.var_pred_mean)\n",
    "    std_fc  = np.sqrt(np.maximum(var_fc, 1e-8))\n",
    "    samples = np.zeros((horizon, n_samples))\n",
    "    for h in range(horizon):\n",
    "        samples[h,:] = np.random.normal(mean_fc[h], std_fc[h], size=n_samples)\n",
    "    return mean_fc, samples\n",
    "\n",
    "# ---------------------------------\n",
    "# CVAE rollout (soft states, autoregressive), mobility persistence\n",
    "# Returns samples in ORIGINAL units (already inverse-transformed)\n",
    "# ---------------------------------\n",
    "@torch.no_grad()\n",
    "def cvae_rollout(df_comm, origin_idx, horizon, n_samples=N_ENSEMBLE,\n",
    "                 temp_z=TEMP_Z, temp_out=TEMP_OUT):\n",
    "    \"\"\"\n",
    "    df_comm: dataframe of a single commune, scaled features & state_id present.\n",
    "    origin_idx: index (in df_comm) of the last observed time used as origin.\n",
    "    horizon: steps ahead.\n",
    "\n",
    "    - For t>origin, mobility persists at last observed values.\n",
    "    - Next-state probs via multinomial LR with (prev state, mobility, commune).\n",
    "    - Conditioning uses SOFT state probs.\n",
    "    - Output samples are inverse-transformed to ORIGINAL units for the 'case' var.\n",
    "    \"\"\"\n",
    "    if origin_idx < SEQ_LEN:\n",
    "        return None, None\n",
    "\n",
    "    # Initial LSTM window (scaled)\n",
    "    Xwin = df_comm[FEATURES].values[origin_idx-SEQ_LEN+1:origin_idx+1].astype(float)\n",
    "    im_last = float(df_comm.iloc[origin_idx][\"Internal_Mobility_Index\"])\n",
    "    em_last = float(df_comm.iloc[origin_idx][\"External_Mobility_Index\"])\n",
    "    comm_name = df_comm.iloc[0][\"Commune\"]\n",
    "    comm_onehot = enc_comm.transform([[comm_name]])[0]\n",
    "\n",
    "    # Previous state (hard) -> soft thereafter\n",
    "    s_prev_id = int(df_comm.iloc[origin_idx][\"state_id\"])\n",
    "    s_prev_probs = np.zeros(K_STATES); s_prev_probs[s_prev_id] = 1.0\n",
    "\n",
    "    samples = np.zeros((horizon, n_samples), dtype=float)\n",
    "    x_buf = Xwin.copy()\n",
    "\n",
    "    for m in range(n_samples):\n",
    "        s_probs = s_prev_probs.copy()\n",
    "        x_buf_m = x_buf.copy()\n",
    "\n",
    "        for h in range(horizon):\n",
    "            # Conditioning vector with soft state probs\n",
    "            state_soft = s_probs.reshape(1,-1)  # (1,K)\n",
    "            cond_vec = np.concatenate([comm_onehot.reshape(1,-1), state_soft], axis=1)\n",
    "\n",
    "            xb = torch.tensor(x_buf_m[np.newaxis, :, :], dtype=torch.float32, device=DEVICE)\n",
    "            cb = torch.tensor(cond_vec, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "            mu_full, logvar_case, mu_z, logv_z = model(xb, cb, temp=temp_z)\n",
    "            mu_full = mu_full.cpu().numpy()[0]\n",
    "            logvar_case = logvar_case.cpu().numpy()[0,0]\n",
    "            mu_case = mu_full[0]\n",
    "            sigma_case = math.sqrt(max(1e-8, math.exp(logvar_case))) * temp_out\n",
    "\n",
    "            # Sample next 'case' in SCALED space and clip to [0,1] for safety\n",
    "            y_case_scaled = float(np.clip(np.random.normal(mu_case, sigma_case), 0.0, 1.0))\n",
    "\n",
    "            # Build next-step scaled vector (use model means for mobility dims)\n",
    "            mu_full[0] = y_case_scaled\n",
    "            next_vec = mu_full.copy()\n",
    "\n",
    "            # Inverse-scale to ORIGINAL units for the case variable\n",
    "            tmp = np.zeros((1, len(FEATURES))); tmp[0,:] = next_vec\n",
    "            unscaled_case = scaler.inverse_transform(tmp)[0,0]\n",
    "            samples[h, m] = unscaled_case\n",
    "\n",
    "            # Autoregressive buffer update (keep scaled buffer)\n",
    "            x_buf_m = np.vstack([x_buf_m[1:], next_vec])\n",
    "\n",
    "            # Project next state's soft probs (mobility persistence)\n",
    "            pseudo = {\n",
    "                \"state_id\": np.argmax(s_probs),\n",
    "                \"Internal_Mobility_Index\": im_last, \"External_Mobility_Index\": em_last,\n",
    "                \"Commune\": comm_name\n",
    "            }\n",
    "            s_probs = transition_proba_soft(pd.Series(pseudo))\n",
    "\n",
    "    mean_path = np.mean(samples, axis=1)\n",
    "    return mean_path, samples\n",
    "\n",
    "# ============================================================\n",
    "# A) Tail-only Evaluation (as in your original setup)\n",
    "# ============================================================\n",
    "def evaluate_tail(horizon_list=H_LIST, test_days=TEST_DAYS):\n",
    "    rows = []\n",
    "    viz_cache = {}\n",
    "\n",
    "    for comm, dfc in df_scaled.groupby(\"Commune\"):\n",
    "        dfc = dfc.sort_values(\"Date\").reset_index(drop=True)\n",
    "        y_true_full = dfc[OBS_COL].values  # original units\n",
    "\n",
    "        idxs = np.where(dfc[\"is_test\"].values == 1)[0]\n",
    "        if len(idxs) == 0:\n",
    "            continue\n",
    "\n",
    "        for H in horizon_list:\n",
    "            cvae_err=[]; arima_err=[]; pers_err=[]\n",
    "            cvae_crps=[]; cvae_wis=[]; cvae_cov50=[]; cvae_cov90=[]\n",
    "            ar_crps=[];   ar_wis=[];   ar_cov50=[];   ar_cov90=[]\n",
    "            pe_crps=[];   pe_wis=[];   pe_cov50=[];   pe_cov90=[]\n",
    "\n",
    "            if comm in TARGET_COMMUNES and H==14:\n",
    "                viz_cache[(comm,H)] = {\"dates\": [], \"y\": [], \"cvae_mean\": [], \"cvae_p05\": [], \"cvae_p95\": [],\n",
    "                                       \"ar_mean\": [], \"ar_p05\": [], \"ar_p95\": []}\n",
    "\n",
    "            for origin_idx in idxs:\n",
    "                if origin_idx < SEQ_LEN or origin_idx + H >= len(dfc):\n",
    "                    continue\n",
    "\n",
    "                mean_c, samp_c = cvae_rollout(dfc, origin_idx, H, n_samples=N_ENSEMBLE,\n",
    "                                              temp_z=TEMP_Z, temp_out=TEMP_OUT)\n",
    "                if mean_c is None:\n",
    "                    continue\n",
    "\n",
    "                y_hist = y_true_full[:origin_idx+1]\n",
    "                mean_a, samp_a = arima_forecast(y_hist, H, n_samples=N_ENSEMBLE)\n",
    "                mean_p, samp_p = persistence_forecast(y_hist, H, n_samples=N_ENSEMBLE)\n",
    "\n",
    "                truth = y_true_full[origin_idx+1:origin_idx+1+H]\n",
    "\n",
    "                # step-wise metrics\n",
    "                for h in range(H):\n",
    "                    y_h = truth[h]\n",
    "\n",
    "                    # CVAE (already in original units)\n",
    "                    q_c = empirical_quantiles(samp_c[h,:], qs=(0.05,0.25,0.5,0.75,0.95))\n",
    "                    cvae_wis.append(wis_from_quantiles(y_h, [0.5,0.9], q_c))\n",
    "                    cvae_crps.append(crps_ensemble(y_h, samp_c[h,:]))\n",
    "                    cvae_cov50.append(coverage(y_h, q_c[0.25], q_c[0.75]))\n",
    "                    cvae_cov90.append(coverage(y_h, q_c[0.05], q_c[0.95]))\n",
    "                    cvae_err.append(abs(y_h - np.mean(samp_c[h,:])))\n",
    "\n",
    "                    # ARIMA\n",
    "                    q_a = empirical_quantiles(samp_a[h,:], qs=(0.05,0.25,0.5,0.75,0.95))\n",
    "                    ar_wis.append(wis_from_quantiles(y_h, [0.5,0.9], q_a))\n",
    "                    ar_crps.append(crps_ensemble(y_h, samp_a[h,:]))\n",
    "                    ar_cov50.append(coverage(y_h, q_a[0.25], q_a[0.75]))\n",
    "                    ar_cov90.append(coverage(y_h, q_a[0.05], q_a[0.95]))\n",
    "                    arima_err.append(abs(y_h - np.mean(samp_a[h,:])))\n",
    "\n",
    "                    # Persistence\n",
    "                    q_p = empirical_quantiles(samp_p[h,:], qs=(0.05,0.25,0.5,0.75,0.95))\n",
    "                    pe_wis.append(wis_from_quantiles(y_h, [0.5,0.9], q_p))\n",
    "                    pe_crps.append(crps_ensemble(y_h, samp_p[h,:]))\n",
    "                    pe_cov50.append(coverage(y_h, q_p[0.25], q_p[0.75]))\n",
    "                    pe_cov90.append(coverage(y_h, q_p[0.05], q_p[0.95]))\n",
    "                    pers_err.append(abs(y_h - np.mean(samp_p[h,:])))\n",
    "\n",
    "                # cache for visualization\n",
    "                if (comm in TARGET_COMMUNES) and (H==14):\n",
    "                    dates = dfc.loc[origin_idx+1:origin_idx+H, \"Date\"].tolist()\n",
    "                    viz_cache[(comm,H)][\"dates\"].append(dates)\n",
    "                    viz_cache[(comm,H)][\"y\"].append(truth.tolist())\n",
    "                    viz_cache[(comm,H)][\"cvae_mean\"].append(mean_c.tolist())\n",
    "                    viz_cache[(comm,H)][\"cvae_p05\"].append([np.quantile(samp_c[h,:],0.05) for h in range(H)])\n",
    "                    viz_cache[(comm,H)][\"cvae_p95\"].append([np.quantile(samp_c[h,:],0.95) for h in range(H)])\n",
    "                    viz_cache[(comm,H)][\"ar_mean\"].append(mean_a.tolist())\n",
    "                    viz_cache[(comm,H)][\"ar_p05\"].append([np.quantile(samp_a[h,:],0.05) for h in range(H)])\n",
    "                    viz_cache[(comm,H)][\"ar_p95\"].append([np.quantile(samp_a[h,:],0.95) for h in range(H)])\n",
    "\n",
    "            if len(cvae_err)==0:\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"Commune\": comm, \"horizon\": H,\n",
    "                \"MAE_CVAE\": float(np.mean(cvae_err)),\n",
    "                \"CRPS_CVAE\": float(np.mean(cvae_crps)),\n",
    "                \"WIS_CVAE\": float(np.mean(cvae_wis)),\n",
    "                \"COV50_CVAE\": float(np.mean(cvae_cov50)),\n",
    "                \"COV90_CVAE\": float(np.mean(cvae_cov90)),\n",
    "                \"MAE_ARIMA\": float(np.mean(arima_err)),\n",
    "                \"CRPS_ARIMA\": float(np.mean(ar_crps)),\n",
    "                \"WIS_ARIMA\": float(np.mean(ar_wis)),\n",
    "                \"COV50_ARIMA\": float(np.mean(ar_cov50)),\n",
    "                \"COV90_ARIMA\": float(np.mean(ar_cov90)),\n",
    "                \"MAE_PERSIST\": float(np.mean(pers_err)),\n",
    "                \"CRPS_PERSIST\": float(np.mean(pe_crps)),\n",
    "                \"WIS_PERSIST\": float(np.mean(pe_wis)),\n",
    "                \"COV50_PERSIST\": float(np.mean(pe_cov50)),\n",
    "                \"COV90_PERSIST\": float(np.mean(pe_cov90))\n",
    "            })\n",
    "\n",
    "    res_df = pd.DataFrame(rows)\n",
    "    res_df.to_csv(os.path.join(OUT_DIR, \"A_tail_metrics_by_commune.csv\"), index=False)\n",
    "    agg = res_df.groupby(\"horizon\").mean(numeric_only=True).reset_index()\n",
    "    agg.to_csv(os.path.join(OUT_DIR, \"A_tail_metrics_pooled_by_horizon.csv\"), index=False)\n",
    "    overall = res_df.mean(numeric_only=True).to_frame().T\n",
    "    overall.to_csv(os.path.join(OUT_DIR, \"A_tail_metrics_pooled_overall.csv\"), index=False)\n",
    "\n",
    "    print(\"=== [A] Tail-only: pooled by horizon ===\")\n",
    "    print(agg.round(4))\n",
    "    print(\"\\n=== [A] Tail-only: pooled overall ===\")\n",
    "    print(overall.round(4))\n",
    "\n",
    "    return res_df, agg, overall, viz_cache\n",
    "\n",
    "# ============================================================\n",
    "# B) Rolling / Regime-stratified Evaluation (recommended)\n",
    "#    (no retraining; just different origins)\n",
    "# ============================================================\n",
    "def pick_origins_regime_stratified(dfc, per_regime=4, min_gap=14, max_h=max(H_LIST)):\n",
    "    \"\"\"Pick ~per_regime origins per regime, separated by >= min_gap, with room for max_h.\"\"\"\n",
    "    idxs = []\n",
    "    n = len(dfc)\n",
    "    for s in sorted(dfc[\"state_id\"].unique().tolist()):\n",
    "        cand = np.where(dfc[\"state_id\"].values == s)[0]\n",
    "        cand = cand[(cand >= SEQ_LEN) & (cand <= n - max_h - 1)]\n",
    "        chosen = []\n",
    "        last = -10**9\n",
    "        for i in cand:\n",
    "            if i - last >= min_gap:\n",
    "                chosen.append(i); last = i\n",
    "            if len(chosen) >= per_regime:\n",
    "                break\n",
    "        idxs.extend(chosen)\n",
    "    return sorted(set(idxs))\n",
    "\n",
    "def pick_origins_rolling_every_k(dfc, step=7, max_h=max(H_LIST)):\n",
    "    \"\"\"Pick origins every 'step' days along the series.\"\"\"\n",
    "    n = len(dfc)\n",
    "    valid = list(range(SEQ_LEN, n - max_h))\n",
    "    return valid[::step]\n",
    "\n",
    "def pick_origins_incidence_quantiles(dfc, per_bucket=4, min_gap=14, max_h=max(H_LIST)):\n",
    "    \"\"\"Pick origins stratified by incidence quantiles (low/mid/high).\"\"\"\n",
    "    vals = dfc[CASE_COL].values\n",
    "    q1, q2 = np.quantile(vals, [0.33, 0.66])\n",
    "    buckets = {\n",
    "        \"low\":  np.where(vals <= q1)[0],\n",
    "        \"mid\":  np.where((vals > q1) & (vals <= q2))[0],\n",
    "        \"high\": np.where(vals > q2)[0]\n",
    "    }\n",
    "    idxs = []\n",
    "    n = len(dfc)\n",
    "    for _, cand in buckets.items():\n",
    "        cand = cand[(cand >= SEQ_LEN) & (cand <= n - max_h - 1)]\n",
    "        chosen = []\n",
    "        last = -10**9\n",
    "        for i in cand:\n",
    "            if i - last >= min_gap:\n",
    "                chosen.append(i); last = i\n",
    "            if len(chosen) >= per_bucket:\n",
    "                break\n",
    "        idxs.extend(chosen)\n",
    "    return sorted(set(idxs))\n",
    "\n",
    "def evaluate_with_selector(horizon_list=H_LIST, origin_selector=\"regime\", **selector_kwargs):\n",
    "    \"\"\"\n",
    "    origin_selector: 'regime' | 'rolling' | 'quantiles'\n",
    "    selector_kwargs: parameters for the selector.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    viz_cache = {}\n",
    "\n",
    "    for comm, dfc in df_scaled.groupby(\"Commune\"):\n",
    "        dfc = dfc.sort_values(\"Date\").reset_index(drop=True)\n",
    "        y_true_full = dfc[OBS_COL].values  # original units\n",
    "\n",
    "        # Select origins\n",
    "        if origin_selector == \"regime\":\n",
    "            origins = pick_origins_regime_stratified(dfc, **selector_kwargs)\n",
    "        elif origin_selector == \"rolling\":\n",
    "            origins = pick_origins_rolling_every_k(dfc, **selector_kwargs)\n",
    "        elif origin_selector == \"quantiles\":\n",
    "            origins = pick_origins_incidence_quantiles(dfc, **selector_kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown origin_selector\")\n",
    "\n",
    "        if len(origins) == 0:\n",
    "            continue\n",
    "\n",
    "        for H in horizon_list:\n",
    "            cvae_err=[]; arima_err=[]; pers_err=[]\n",
    "            cvae_crps=[]; cvae_wis=[]; cvae_cov50=[]; cvae_cov90=[]\n",
    "            ar_crps=[];   ar_wis=[];   ar_cov50=[];   ar_cov90=[]\n",
    "            pe_crps=[];   pe_wis=[];   pe_cov50=[];   pe_cov90=[]\n",
    "\n",
    "            if comm in TARGET_COMMUNES and H==14:\n",
    "                viz_cache[(comm,H)] = {\"dates\": [], \"y\": [], \"cvae_mean\": [], \"cvae_p05\": [], \"cvae_p95\": [],\n",
    "                                       \"ar_mean\": [], \"ar_p05\": [], \"ar_p95\": []}\n",
    "\n",
    "            for origin_idx in origins:\n",
    "                if origin_idx < SEQ_LEN or origin_idx + H >= len(dfc):\n",
    "                    continue\n",
    "\n",
    "                mean_c, samp_c = cvae_rollout(dfc, origin_idx, H, n_samples=N_ENSEMBLE,\n",
    "                                              temp_z=TEMP_Z, temp_out=TEMP_OUT)\n",
    "                if mean_c is None:\n",
    "                    continue\n",
    "\n",
    "                y_hist = y_true_full[:origin_idx+1]\n",
    "                mean_a, samp_a = arima_forecast(y_hist, H, n_samples=N_ENSEMBLE)\n",
    "                mean_p, samp_p = persistence_forecast(y_hist, H, n_samples=N_ENSEMBLE)\n",
    "\n",
    "                truth = y_true_full[origin_idx+1:origin_idx+1+H]\n",
    "\n",
    "                # step-wise metrics (all in ORIGINAL units)\n",
    "                for h in range(H):\n",
    "                    y_h = truth[h]\n",
    "\n",
    "                    q_c = empirical_quantiles(samp_c[h,:], qs=(0.05,0.25,0.5,0.75,0.95))\n",
    "                    cvae_wis.append(wis_from_quantiles(y_h, [0.5,0.9], q_c))\n",
    "                    cvae_crps.append(crps_ensemble(y_h, samp_c[h,:]))\n",
    "                    cvae_cov50.append(coverage(y_h, q_c[0.25], q_c[0.75]))\n",
    "                    cvae_cov90.append(coverage(y_h, q_c[0.05], q_c[0.95]))\n",
    "                    cvae_err.append(abs(y_h - np.mean(samp_c[h,:])))\n",
    "\n",
    "                    q_a = empirical_quantiles(samp_a[h,:], qs=(0.05,0.25,0.5,0.75,0.95))\n",
    "                    ar_wis.append(wis_from_quantiles(y_h, [0.5,0.9], q_a))\n",
    "                    ar_crps.append(crps_ensemble(y_h, samp_a[h,:]))\n",
    "                    ar_cov50.append(coverage(y_h, q_a[0.25], q_a[0.75]))\n",
    "                    ar_cov90.append(coverage(y_h, q_a[0.05], q_a[0.95]))\n",
    "                    arima_err.append(abs(y_h - np.mean(samp_a[h,:])))\n",
    "\n",
    "                    q_p = empirical_quantiles(samp_p[h,:], qs=(0.05,0.25,0.5,0.75,0.95))\n",
    "                    pe_wis.append(wis_from_quantiles(y_h, [0.5,0.9], q_p))\n",
    "                    pe_crps.append(crps_ensemble(y_h, samp_p[h,:]))\n",
    "                    pe_cov50.append(coverage(y_h, q_p[0.25], q_p[0.75]))\n",
    "                    pe_cov90.append(coverage(y_h, q_p[0.05], q_p[0.95]))\n",
    "                    pers_err.append(abs(y_h - np.mean(samp_p[h,:])))\n",
    "\n",
    "                # cache for figures (use last origin later)\n",
    "                if (comm in TARGET_COMMUNES) and (H==14):\n",
    "                    dates = dfc.loc[origin_idx+1:origin_idx+H, \"Date\"].tolist()\n",
    "                    viz_cache[(comm,H)][\"dates\"].append(dates)\n",
    "                    viz_cache[(comm,H)][\"y\"].append(truth.tolist())\n",
    "                    viz_cache[(comm,H)][\"cvae_mean\"].append(mean_c.tolist())\n",
    "                    viz_cache[(comm,H)][\"cvae_p05\"].append([np.quantile(samp_c[h,:],0.05) for h in range(H)])\n",
    "                    viz_cache[(comm,H)][\"cvae_p95\"].append([np.quantile(samp_c[h,:],0.95) for h in range(H)])\n",
    "                    viz_cache[(comm,H)][\"ar_mean\"].append(mean_a.tolist())\n",
    "                    viz_cache[(comm,H)][\"ar_p05\"].append([np.quantile(samp_a[h,:],0.05) for h in range(H)])\n",
    "                    viz_cache[(comm,H)][\"ar_p95\"].append([np.quantile(samp_a[h,:],0.95) for h in range(H)])\n",
    "\n",
    "            if len(cvae_err)==0:\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"Commune\": comm, \"horizon\": H,\n",
    "                \"MAE_CVAE\": float(np.mean(cvae_err)),\n",
    "                \"CRPS_CVAE\": float(np.mean(cvae_crps)),\n",
    "                \"WIS_CVAE\": float(np.mean(cvae_wis)),\n",
    "                \"COV50_CVAE\": float(np.mean(cvae_cov50)),\n",
    "                \"COV90_CVAE\": float(np.mean(cvae_cov90)),\n",
    "                \"MAE_ARIMA\": float(np.mean(arima_err)),\n",
    "                \"CRPS_ARIMA\": float(np.mean(ar_crps)),\n",
    "                \"WIS_ARIMA\": float(np.mean(ar_wis)),\n",
    "                \"COV50_ARIMA\": float(np.mean(ar_cov50)),\n",
    "                \"COV90_ARIMA\": float(np.mean(ar_cov90)),\n",
    "                \"MAE_PERSIST\": float(np.mean(pers_err)),\n",
    "                \"CRPS_PERSIST\": float(np.mean(pe_crps)),\n",
    "                \"WIS_PERSIST\": float(np.mean(pe_wis)),\n",
    "                \"COV50_PERSIST\": float(np.mean(pe_cov50)),\n",
    "                \"COV90_PERSIST\": float(np.mean(pe_cov90))\n",
    "            })\n",
    "\n",
    "    res_df = pd.DataFrame(rows)\n",
    "    res_df.to_csv(os.path.join(OUT_DIR, \"B_selector_metrics_by_commune.csv\"), index=False)\n",
    "    agg = res_df.groupby(\"horizon\").mean(numeric_only=True).reset_index()\n",
    "    agg.to_csv(os.path.join(OUT_DIR, \"B_selector_metrics_pooled_by_horizon.csv\"), index=False)\n",
    "    overall = res_df.mean(numeric_only=True).to_frame().T\n",
    "    overall.to_csv(os.path.join(OUT_DIR, \"B_selector_metrics_pooled_overall.csv\"), index=False)\n",
    "\n",
    "    print(\"=== [B] Selector-based: pooled by horizon ===\")\n",
    "    print(agg.round(4))\n",
    "    print(\"\\n=== [B] Selector-based: pooled overall ===\")\n",
    "    print(overall.round(4))\n",
    "\n",
    "    return res_df, agg, overall, viz_cache\n",
    "\n",
    "# ============================================================\n",
    "# Free-run reconstruction of full curve + shape metrics\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def cvae_free_run(dfc, start_idx=None):\n",
    "    \"\"\"\n",
    "    Recursive 1-step rollout until the end, starting from start_idx (default SEQ_LEN-1).\n",
    "    Returns predictions in ORIGINAL units (same as cvae_rollout output).\n",
    "    \"\"\"\n",
    "    if start_idx is None:\n",
    "        start_idx = SEQ_LEN - 1\n",
    "    n = len(dfc)\n",
    "    preds = []\n",
    "    for t in range(start_idx, n-1):\n",
    "        mean1, samp1 = cvae_rollout(dfc, t, horizon=1, n_samples=N_ENSEMBLE,\n",
    "                                    temp_z=TEMP_Z, temp_out=TEMP_OUT)\n",
    "        preds.append(float(mean1[0]) if mean1 is not None else np.nan)\n",
    "    return np.array(preds, dtype=float)\n",
    "\n",
    "def arima_free_run(y, start_idx=None):\n",
    "    \"\"\"Recursive ARIMA: refit on y[:t+1], forecast 1-step, advance.\"\"\"\n",
    "    if start_idx is None:\n",
    "        start_idx = SEQ_LEN - 1\n",
    "    n = len(y)\n",
    "    out = []\n",
    "    for t in range(start_idx, n-1):\n",
    "        mean, _ = arima_forecast(y[:t+1], horizon=1, n_samples=64)\n",
    "        out.append(float(mean[0]))\n",
    "    return np.array(out, dtype=float)\n",
    "\n",
    "def peak_metrics(y_true, y_pred):\n",
    "    \"\"\"Peak timing (days) and magnitude error.\"\"\"\n",
    "    i_t = int(np.nanargmax(y_true)); i_p = int(np.nanargmax(y_pred))\n",
    "    return {\"peak_timing_error_days\": i_p - i_t,\n",
    "            \"peak_mag_error\": float(abs(y_pred[i_p] - y_true[i_t]))}\n",
    "\n",
    "def series_shape_metrics(y_true, y_pred):\n",
    "    \"\"\"Correlation, MAE, RMSE, and MASE against naive-1.\"\"\"\n",
    "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float)\n",
    "    n = min(len(y_true), len(y_pred))\n",
    "    yt = y_true[:n]; yp = y_pred[:n]\n",
    "    corr = np.corrcoef(yt, yp)[0,1] if np.std(yt)>0 and np.std(yp)>0 else np.nan\n",
    "    mae_v = float(np.mean(np.abs(yt-yp)))\n",
    "    rmse_v = float(np.sqrt(np.mean((yt-yp)**2)))\n",
    "    mase_den = np.mean(np.abs(yt[1:] - yt[:-1])) if len(yt)>1 else np.nan\n",
    "    mase = mae_v / mase_den if (mase_den is not None and mase_den>0) else np.nan\n",
    "    return {\"corr\": float(corr), \"MAE\": mae_v, \"RMSE\": rmse_v, \"MASE\": float(mase)}\n",
    "\n",
    "# ============================================================\n",
    "# Visualization helpers\n",
    "# ============================================================\n",
    "def plot_panels(viz_cache, H=14, n_panels=6):\n",
    "    \"\"\"\n",
    "    Multi-commune panels: Observed vs CVAE vs ARIMA (90% bands),\n",
    "    using the last origin only for each commune (clearer for publication).\n",
    "    \"\"\"\n",
    "    communes = [c for c in TARGET_COMMUNES if (c,H) in viz_cache]\n",
    "    n = min(n_panels, len(communes))\n",
    "    if n == 0:\n",
    "        print(\"No communes cached for visualization.\")\n",
    "        return\n",
    "\n",
    "    nrows = math.ceil(n/2); ncols = 2\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(16, 5.5*nrows), sharex=False)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for ax, comm in zip(axes, communes[:n]):\n",
    "        block = viz_cache[(comm,H)]\n",
    "        dates  = block[\"dates\"][-1]\n",
    "        y      = block[\"y\"][-1]\n",
    "        cm     = block[\"cvae_mean\"][-1]; c05 = block[\"cvae_p05\"][-1]; c95 = block[\"cvae_p95\"][-1]\n",
    "        am     = block[\"ar_mean\"][-1];   a05 = block[\"ar_p05\"][-1];   a95 = block[\"ar_p95\"][-1]\n",
    "\n",
    "        ax.plot(dates, y, lw=2.0, color=\"black\", label=\"Observed\")\n",
    "        ax.fill_between(dates, c05, c95, alpha=0.20, label=\"CVAE 90%\", edgecolor='none')\n",
    "        ax.plot(dates, cm, lw=2.0, linestyle=\"--\", label=\"CVAE mean\")\n",
    "        ax.fill_between(dates, a05, a95, alpha=0.15, label=\"ARIMA 90%\", edgecolor='none')\n",
    "        ax.plot(dates, am, lw=2.0, linestyle=\"-.\", label=\"ARIMA mean\")\n",
    "\n",
    "        ax.set_title(f\"{comm} — {H}-day ahead\", fontsize=13)\n",
    "        ax.set_ylabel(\"Daily cases (7-day MA)\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"lower center\", ncol=4, frameon=False)\n",
    "    fig.autofmt_xdate()\n",
    "    fig.tight_layout(rect=[0,0.05,1,1])\n",
    "    path = os.path.join(OUT_DIR, f\"viz_obs_vs_cvae_arima_H{H}.png\")\n",
    "    fig.savefig(path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved: {path}\")\n",
    "\n",
    "def plot_last_origin(commune, H=14, viz_cache=None, outdir=OUT_DIR):\n",
    "    \"\"\"Single-commune, last-origin chart with 90% bands (Observed, CVAE, ARIMA).\"\"\"\n",
    "    if viz_cache is None or (commune, H) not in viz_cache or len(viz_cache[(commune,H)][\"dates\"]) == 0:\n",
    "        print(f\"No cache for {commune} (H={H}). Run evaluation first and ensure commune is in TARGET_COMMUNES.\")\n",
    "        return\n",
    "    block = viz_cache[(commune,H)]\n",
    "    dates  = block[\"dates\"][-1]\n",
    "    y      = block[\"y\"][-1]\n",
    "    cm     = block[\"cvae_mean\"][-1]; c05 = block[\"cvae_p05\"][-1]; c95 = block[\"cvae_p95\"][-1]\n",
    "    am     = block[\"ar_mean\"][-1];   a05 = block[\"ar_p05\"][-1];   a95 = block[\"ar_p95\"][-1]\n",
    "\n",
    "    plt.figure(figsize=(11,5))\n",
    "    plt.plot(dates, y, color=\"black\", lw=2.0, label=\"Observed\")\n",
    "    plt.fill_between(dates, c05, c95, alpha=0.20, label=\"CVAE 90%\", edgecolor='none')\n",
    "    plt.plot(dates, cm, lw=2.0, linestyle=\"--\", label=\"CVAE mean\")\n",
    "    plt.fill_between(dates, a05, a95, alpha=0.15, label=\"ARIMA 90%\", edgecolor='none')\n",
    "    plt.plot(dates, am, lw=2.0, linestyle=\"-.\", label=\"ARIMA mean\")\n",
    "    plt.title(f\"{commune} — {H}-day ahead (last origin)\")\n",
    "    plt.ylabel(\"Daily cases (7-day MA)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(frameon=False, ncol=4, loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    png_path = os.path.join(outdir, f\"last_origin_{commune.replace(' ','_')}_H{H}.png\")\n",
    "    plt.savefig(png_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved: {png_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# RUN — choose one or both evaluation modes\n",
    "# ============================================================\n",
    "\n",
    "# --- (A) Tail-only (as in your current paper draft) ---\n",
    "res_tail, agg_tail, overall_tail, viz_tail = evaluate_tail(H_LIST, TEST_DAYS)\n",
    "print(f\"✓ Tail-only CSVs saved in: {OUT_DIR}\")\n",
    "\n",
    "# Visualizations for (A)\n",
    "plot_panels(viz_tail, H=14, n_panels=6)\n",
    "plot_panels(viz_tail, H=7,  n_panels=6)\n",
    "for c in TARGET_COMMUNES:\n",
    "    plot_last_origin(c, H=14, viz_cache=viz_tail)\n",
    "\n",
    "# --- (B) Selector-based backtesting (recommended for paper claims) ---\n",
    "# Regime-stratified: 4 origins per regime, >=14d spacing\n",
    "res_sel, agg_sel, overall_sel, viz_sel = evaluate_with_selector(\n",
    "    horizon_list=[7,14],\n",
    "    origin_selector=\"regime\",\n",
    "    per_regime=4, min_gap=14\n",
    ")\n",
    "print(f\"✓ Selector-based CSVs saved in: {OUT_DIR}\")\n",
    "\n",
    "# Visualizations for (B)\n",
    "plot_panels(viz_sel, H=14, n_panels=6)\n",
    "\n",
    "# ============================================================\n",
    "# Comparative summary table by commune (both horizons)\n",
    "# ============================================================\n",
    "def wide_summary(res_df, tag):\n",
    "    \"\"\"\n",
    "    Build a wide table with per-commune metrics for H=7 and H=14\n",
    "    for CVAE, ARIMA, and Persistence (MAE/CRPS/WIS + coverages).\n",
    "    \"\"\"\n",
    "    keep_cols = [\"Commune\",\"horizon\",\n",
    "                 \"MAE_CVAE\",\"CRPS_CVAE\",\"WIS_CVAE\",\"COV50_CVAE\",\"COV90_CVAE\",\n",
    "                 \"MAE_ARIMA\",\"CRPS_ARIMA\",\"WIS_ARIMA\",\"COV50_ARIMA\",\"COV90_ARIMA\",\n",
    "                 \"MAE_PERSIST\",\"CRPS_PERSIST\",\"WIS_PERSIST\",\"COV50_PERSIST\",\"COV90_PERSIST\"]\n",
    "    tmp = res_df[keep_cols].copy()\n",
    "    # Aggregate by commune-horizon (mean across origins)\n",
    "    tmp = tmp.groupby([\"Commune\",\"horizon\"]).mean(numeric_only=True).reset_index()\n",
    "    # Pivot to wide: columns like MAE_CVAE_H7, MAE_CVAE_H14, ...\n",
    "    out = []\n",
    "    for h in sorted(tmp[\"horizon\"].unique()):\n",
    "        dfh = tmp[tmp[\"horizon\"]==h].drop(columns=[\"horizon\"]).copy()\n",
    "        dfh = dfh.set_index(\"Commune\")\n",
    "        dfh = dfh.add_suffix(f\"_H{h}\")\n",
    "        out.append(dfh)\n",
    "    wide = pd.concat(out, axis=1).reset_index()\n",
    "    wide = wide.sort_values(\"Commune\")\n",
    "    path = os.path.join(OUT_DIR, f\"{tag}_summary_by_commune_wide.csv\")\n",
    "    wide.to_csv(path, index=False)\n",
    "    print(f\"✓ Saved table: {path}\")\n",
    "    return wide\n",
    "\n",
    "wide_tail = wide_summary(res_tail, tag=\"A_tail\")\n",
    "wide_sel  = wide_summary(res_sel,  tag=\"B_selector\")\n",
    "\n",
    "print(\"\\n=== Head of selector-based summary (by commune) ===\")\n",
    "print(wide_sel.head(10).round(3))\n",
    "\n",
    "# ============================================================\n",
    "# Free-run reconstruction (shape metrics) for a subset or all communes\n",
    "# ============================================================\n",
    "shape_rows = []\n",
    "for comm, dfc in df_scaled.groupby(\"Commune\"):\n",
    "    dfc = dfc.sort_values(\"Date\").reset_index(drop=True)\n",
    "    start = SEQ_LEN - 1  # warm-up\n",
    "    # CVAE free-run (ORIGINAL units)\n",
    "    cvae_pred = cvae_free_run(dfc, start_idx=start)\n",
    "    y_true    = dfc[OBS_COL].values[start+1:]\n",
    "    # ARIMA free-run\n",
    "    ar_pred   = arima_free_run(dfc[OBS_COL].values, start_idx=start)\n",
    "\n",
    "    # Align lengths\n",
    "    nmin = min(len(y_true), len(cvae_pred), len(ar_pred))\n",
    "    y_true = y_true[:nmin]; cvae_pred = cvae_pred[:nmin]; ar_pred = ar_pred[:nmin]\n",
    "\n",
    "    pm_c = peak_metrics(y_true, cvae_pred);  sm_c = series_shape_metrics(y_true, cvae_pred)\n",
    "    pm_a = peak_metrics(y_true, ar_pred);    sm_a = series_shape_metrics(y_true, ar_pred)\n",
    "\n",
    "    shape_rows.append({\n",
    "        \"Commune\": comm,\n",
    "        \"corr_CVAE\": sm_c[\"corr\"], \"MASE_CVAE\": sm_c[\"MASE\"],\n",
    "        \"peak_dt_CVAE\": pm_c[\"peak_timing_error_days\"], \"peak_err_CVAE\": pm_c[\"peak_mag_error\"],\n",
    "        \"corr_ARIMA\": sm_a[\"corr\"], \"MASE_ARIMA\": sm_a[\"MASE\"],\n",
    "        \"peak_dt_ARIMA\": pm_a[\"peak_timing_error_days\"], \"peak_err_ARIMA\": pm_a[\"peak_mag_error\"],\n",
    "    })\n",
    "\n",
    "shape_df = pd.DataFrame(shape_rows).sort_values(\"Commune\")\n",
    "shape_path = os.path.join(OUT_DIR, \"C_free_run_shape_metrics.csv\")\n",
    "shape_df.to_csv(shape_path, index=False)\n",
    "print(f\"\\n✓ Free-run shape metrics saved: {shape_path}\")\n",
    "print(shape_df.head(10).round(3))\n",
    "\n",
    "# Optional: quick plot of free-run for target communes\n",
    "def plot_free_run(commune):\n",
    "    dfc = df_scaled[df_scaled[\"Commune\"]==commune].sort_values(\"Date\").reset_index(drop=True)\n",
    "    start = SEQ_LEN - 1\n",
    "    pred_c = cvae_free_run(dfc, start_idx=start)\n",
    "    pred_a = arima_free_run(dfc[CASE_COL].values, start_idx=start)\n",
    "    y = dfc[OBS_COL].values[start+1:]\n",
    "    nmin = min(len(y), len(pred_c), len(pred_a))\n",
    "    dates = dfc[\"Date\"].iloc[start+1:start+1+nmin]\n",
    "\n",
    "    plt.figure(figsize=(11,5))\n",
    "    plt.plot(dates, y[:nmin], color=\"black\", lw=2.0, label=\"Observed\")\n",
    "    plt.plot(dates, pred_c[:nmin], lw=2.0, linestyle=\"--\", label=\"CVAE (free-run)\")\n",
    "    plt.plot(dates, pred_a[:nmin], lw=2.0, linestyle=\"-.\", label=\"ARIMA (free-run)\")\n",
    "    plt.title(f\"{commune} — Free-run reconstruction\")\n",
    "    plt.ylabel(\"Daily cases (7-day MA)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(frameon=False, ncol=3, loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    p = os.path.join(OUT_DIR, f\"free_run_{commune.replace(' ','_')}.png\")\n",
    "    plt.savefig(p, dpi=300); plt.show()\n",
    "    print(f\"✓ Saved: {p}\")\n",
    "\n",
    "for c in TARGET_COMMUNES:\n",
    "    plot_free_run(c)\n",
    "\n",
    "print(\"\\nAll done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae20518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# Load data & encoders\n",
    "# ---------------------------------\n",
    "df = pd.read_csv(DATA_CSV, parse_dates=[\"Date\"])\n",
    "df = df.sort_values([\"Commune\",\"Date\"]).reset_index(drop=True)\n",
    "\n",
    "# Guardamos la columna de casos en UNIDADES ORIGINALES bajo un nombre fijo\n",
    "OBS_COL = \"Observed_Cases\"\n",
    "df[OBS_COL] = df[CASE_COL].values  # copia explícita\n",
    "\n",
    "# Global MinMax scaler (fit on ALL features, incluyendo casos)\n",
    "scaler = MinMaxScaler()\n",
    "X_all = df[FEATURES].values\n",
    "X_scaled = scaler.fit_transform(X_all)\n",
    "\n",
    "# df_scaled: igual a df pero con FEATURES escalados; mantenemos OBS_COL en original\n",
    "df_scaled = df.copy()\n",
    "df_scaled[FEATURES] = X_scaled  # OJO: CASE_COL queda escalado aquí a propósito\n",
    "# pero OBS_COL queda en original y es la que usaremos para verdad/ARIMA\n",
    "\n",
    "# (opcional) sanity check útil\n",
    "assert df_scaled[OBS_COL].max() > 5.0, \"Observed_Cases parece estar escalado; revisa el flujo.\"\n",
    "\n",
    "# Hidden states → enteros 0..K-1\n",
    "state_vals = sorted(df_scaled[\"Hidden_State\"].dropna().unique().tolist())\n",
    "state_to_id = {s:i for i,s in enumerate(state_vals)}\n",
    "id_to_state = {i:s for s,i in state_to_id.items()}\n",
    "K_STATES = len(state_vals)\n",
    "print(f\"Detected hidden-state categories (ordered): {state_vals}  -> K={K_STATES}\")\n",
    "\n",
    "df_scaled[\"state_id\"] = df_scaled[\"Hidden_State\"].map(state_to_id).astype(int)\n",
    "\n",
    "# Commune encoder (one-hot)\n",
    "enc_comm = OneHotEncoder_compat(handle_unknown=\"ignore\")\n",
    "comm_cond = enc_comm.fit_transform(df_scaled[[\"Commune\"]])  # (N, C)\n",
    "\n",
    "# State encoder (para entrenamiento duro)\n",
    "enc_state = OneHotEncoder_compat(categories=[state_vals], handle_unknown=\"ignore\")\n",
    "state_oh = enc_state.fit_transform(df_scaled[[\"Hidden_State\"]])\n",
    "\n",
    "# Full conditioning = [comm_onehot, state_onehot]\n",
    "COND_FULL = np.concatenate([comm_cond, state_oh], axis=1)\n",
    "COND_DIM  = COND_FULL.shape[1]\n",
    "INPUT_DIM = len(FEATURES)\n",
    "\n",
    "# Guardamos min/max por si los quieres inspeccionar\n",
    "CASE_MIN, CASE_MAX = scaler.data_min_[0], scaler.data_max_[0]\n",
    "\n",
    "# ---------------------------------\n",
    "# Train/test split by time (tail-only mode)\n",
    "# ---------------------------------\n",
    "def time_split_by_commune(df_in, test_days=TEST_DAYS):\n",
    "    marks = []\n",
    "    for comm, d in df_in.groupby(\"Commune\", sort=False):\n",
    "        n = len(d)\n",
    "        cut = max(0, n - test_days)\n",
    "        marks.append(pd.Series([0]*cut + [1]*(n-cut), index=d.index))\n",
    "    return pd.concat(marks).sort_index().values\n",
    "\n",
    "is_test = time_split_by_commune(df_scaled, TEST_DAYS)\n",
    "df_scaled[\"is_test\"] = is_test\n",
    "\n",
    "# ---------------------------------\n",
    "# Tensors for training CVAE (train slice only)\n",
    "# ---------------------------------\n",
    "train_df = df_scaled[df_scaled[\"is_test\"]==0].copy()\n",
    "X_train_raw = train_df[FEATURES].values\n",
    "C_train_raw = COND_FULL[train_df.index]\n",
    "Y_train_raw = train_df[FEATURES].values  # next-step target built in sequence\n",
    "\n",
    "X_seq, C_seq, Y_seq = create_sequences(X_train_raw, C_train_raw, Y_train_raw, seq_len=SEQ_LEN)\n",
    "Xt = torch.tensor(X_seq, dtype=torch.float32)\n",
    "Ct = torch.tensor(C_seq, dtype=torch.float32)\n",
    "Yt = torch.tensor(Y_seq, dtype=torch.float32)\n",
    "\n",
    "train_ds = torch.utils.data.TensorDataset(Xt, Ct, Yt)\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ---------------------------------\n",
    "# Build model & load checkpoint (or train from scratch)\n",
    "# ---------------------------------\n",
    "if Path(MODEL_PTH).exists():\n",
    "    model = build_and_load_robust(INPUT_DIM, COND_DIM, MODEL_PTH, DEVICE)\n",
    "    # Optional: quick fine-tune to calibrate the new logvar row\n",
    "    if FINETUNE_AFTER_LOAD_EPOCHS > 0:\n",
    "        print(f\"→ Quick fine-tune for {FINETUNE_AFTER_LOAD_EPOCHS} epochs to calibrate logvar row...\")\n",
    "        opt = optim.Adam(model.parameters(), lr=LR)\n",
    "        model.train()\n",
    "        for ep in range(FINETUNE_AFTER_LOAD_EPOCHS):\n",
    "            tot = 0.0\n",
    "            for xb, cb, yb in train_loader:\n",
    "                xb, cb, yb = xb.to(DEVICE), cb.to(DEVICE), yb.to(DEVICE)\n",
    "                opt.zero_grad()\n",
    "                mu_full, logvar_case, mu_z, logv_z = model(xb, cb, temp=1.0)\n",
    "                loss = cvae_loss_hetero(mu_full, logvar_case, yb, mu_z, logv_z,\n",
    "                                        alpha_mse=ALPHA_MSE, beta_kl=BETA_KL, out_temp=1.0)\n",
    "                loss.backward(); opt.step()\n",
    "                tot += loss.item() * xb.size(0)\n",
    "            print(f\"[FT {ep+1:02d}] loss={tot/len(train_ds):.4f}\")\n",
    "        torch.save(model.state_dict(), MODEL_PTH)\n",
    "        print(\"✓ Fine-tuned weights saved.\")\n",
    "    model.eval()\n",
    "else:\n",
    "    print(\"No checkpoint found. Training from scratch...\")\n",
    "    model = CVAE_LSTM_HET(INPUT_DIM, COND_DIM, LATENT_DIM, HIDDEN_DIM, dec_hidden=DEC_HIDDEN).to(DEVICE)\n",
    "    opt = optim.Adam(model.parameters(), lr=LR)\n",
    "    model.train()\n",
    "    for ep in range(EPOCHS):\n",
    "        tot = 0.0\n",
    "        for xb, cb, yb in train_loader:\n",
    "            xb, cb, yb = xb.to(DEVICE), cb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            mu_full, logvar_case, mu_z, logv_z = model(xb, cb, temp=1.0)\n",
    "            loss = cvae_loss_hetero(mu_full, logvar_case, yb, mu_z, logv_z,\n",
    "                                    alpha_mse=ALPHA_MSE, beta_kl=BETA_KL, out_temp=1.0)\n",
    "            loss.backward(); opt.step()\n",
    "            tot += loss.item() * xb.size(0)\n",
    "        print(f\"[Epoch {ep+1:02d}] loss={tot/len(train_ds):.4f}\")\n",
    "    torch.save(model.state_dict(), MODEL_PTH)\n",
    "    print(f\"✓ Trained & saved: {MODEL_PTH}\")\n",
    "    model.eval()\n",
    "\n",
    "# ---------------------------------\n",
    "# Non-homogeneous transition model (multinomial LR)\n",
    "# P(S_t | S_{t-1}, mobility, commune)\n",
    "# ---------------------------------\n",
    "def fit_transition_softmax(df_all):\n",
    "    \"\"\"\n",
    "    Fit multinomial logistic regression for state transitions using:\n",
    "    one-hot(S_{t-1}), mobility at t (IM, EM), and commune one-hot.\n",
    "    Uses ONLY the training slice to avoid leakage.\n",
    "    \"\"\"\n",
    "    use_df = df_all[df_all[\"is_test\"]==0].copy()\n",
    "    use_df[\"s_prev\"] = use_df.groupby(\"Commune\")[\"state_id\"].shift(1)\n",
    "    use_df[\"im_prev\"] = use_df.groupby(\"Commune\")[\"Internal_Mobility_Index\"].shift(0)\n",
    "    use_df[\"em_prev\"] = use_df.groupby(\"Commune\")[\"External_Mobility_Index\"].shift(0)\n",
    "    use_df = use_df.dropna(subset=[\"s_prev\"]).copy()\n",
    "    use_df[\"s_prev\"] = use_df[\"s_prev\"].astype(int)\n",
    "\n",
    "    X_comm = enc_comm.transform(use_df[[\"Commune\"]])\n",
    "    X_mob  = use_df[[\"im_prev\",\"em_prev\"]].values\n",
    "    enc_prev = OneHotEncoder_compat(categories=[list(range(K_STATES))])\n",
    "    X_sp   = enc_prev.fit_transform(use_df[[\"s_prev\"]])\n",
    "\n",
    "    X = np.hstack([X_sp, X_mob, X_comm])\n",
    "    y = use_df[\"state_id\"].values.astype(int)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=300, multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "trans_clf = fit_transition_softmax(df_scaled)\n",
    "\n",
    "def transition_proba_soft(df_slice_row):\n",
    "    \"\"\"Return P(S_t | S_{t-1}, mobility, commune) as vector length K_STATES.\"\"\"\n",
    "    s_prev = int(df_slice_row[\"state_id\"])\n",
    "    im = float(df_slice_row[\"Internal_Mobility_Index\"])\n",
    "    em = float(df_slice_row[\"External_Mobility_Index\"])\n",
    "    comm_vec = enc_comm.transform([[df_slice_row[\"Commune\"]]])\n",
    "    sp = np.zeros((1,K_STATES)); sp[0, s_prev] = 1.0\n",
    "    X = np.hstack([sp, np.array([[im,em]]), comm_vec])\n",
    "    p = trans_clf.predict_proba(X)[0]\n",
    "    p = np.clip(p, 1e-8, 1.0)\n",
    "    return p / p.sum()\n",
    "\n",
    "# ---------------------------------\n",
    "# Forecast helpers (ARIMA & Persistence)\n",
    "# ---------------------------------\n",
    "def persistence_forecast(y_hist, horizon, n_samples=200):\n",
    "    \"\"\"Deterministic point forecast + residual bootstrap for intervals.\"\"\"\n",
    "    last = float(y_hist[-1])\n",
    "    mean = np.full(horizon, last)\n",
    "    resid = np.diff(y_hist[-30:]) if len(y_hist) > 30 else np.diff(y_hist)\n",
    "    if resid.size < 2:\n",
    "        resid = np.array([0.0, 0.0, 0.0])\n",
    "    samples = np.zeros((horizon, n_samples))\n",
    "    for h in range(horizon):\n",
    "        noise = np.random.choice(resid, size=n_samples, replace=True)\n",
    "        samples[h,:] = mean[h] + noise\n",
    "    return mean, samples\n",
    "\n",
    "def arima_forecast(y_hist, horizon, n_samples=200, max_train=180):\n",
    "    \"\"\"\n",
    "    Compact ARIMA grid on recent window; sample N(μ, σ^2_pred) for intervals.\n",
    "    \"\"\"\n",
    "    y_train = np.asarray(y_hist[-max_train:], dtype=float)\n",
    "    best_aic, best_res = np.inf, None\n",
    "    for order in ARIMA_GRID:\n",
    "        try:\n",
    "            res = ARIMA(y_train, order=order).fit()\n",
    "            if res.aic < best_aic:\n",
    "                best_aic, best_res = res.aic, res\n",
    "        except Exception:\n",
    "            continue\n",
    "    if best_res is None:\n",
    "        return persistence_forecast(y_hist, horizon, n_samples)\n",
    "\n",
    "    fc = best_res.get_forecast(steps=horizon)\n",
    "    mean_fc = np.asarray(fc.predicted_mean)\n",
    "    var_fc  = np.asarray(fc.var_pred_mean)\n",
    "    std_fc  = np.sqrt(np.maximum(var_fc, 1e-8))\n",
    "    samples = np.zeros((horizon, n_samples))\n",
    "    for h in range(horizon):\n",
    "        samples[h,:] = np.random.normal(mean_fc[h], std_fc[h], size=n_samples)\n",
    "    return mean_fc, samples\n",
    "\n",
    "# ---------------------------------\n",
    "# CVAE rollout (soft states, autoregressive), mobility persistence\n",
    "# Returns samples in ORIGINAL units (already inverse-transformed)\n",
    "# ---------------------------------\n",
    "@torch.no_grad()\n",
    "def cvae_rollout(df_comm, origin_idx, horizon, n_samples=N_ENSEMBLE,\n",
    "                 temp_z=TEMP_Z, temp_out=TEMP_OUT):\n",
    "    \"\"\"\n",
    "    df_comm: dataframe of a single commune, scaled features & state_id present.\n",
    "    origin_idx: index (in df_comm) of the last observed time used as origin.\n",
    "    horizon: steps ahead.\n",
    "\n",
    "    - For t>origin, mobility persists at last observed values.\n",
    "    - Next-state probs via multinomial LR with (prev state, mobility, commune).\n",
    "    - Conditioning uses SOFT state probs.\n",
    "    - Output samples are inverse-transformed to ORIGINAL units for the 'case' var.\n",
    "    \"\"\"\n",
    "    if origin_idx < SEQ_LEN:\n",
    "        return None, None\n",
    "\n",
    "    # Initial LSTM window (scaled)\n",
    "    Xwin = df_comm[FEATURES].values[origin_idx-SEQ_LEN+1:origin_idx+1].astype(float)\n",
    "    im_last = float(df_comm.iloc[origin_idx][\"Internal_Mobility_Index\"])\n",
    "    em_last = float(df_comm.iloc[origin_idx][\"External_Mobility_Index\"])\n",
    "    comm_name = df_comm.iloc[0][\"Commune\"]\n",
    "    comm_onehot = enc_comm.transform([[comm_name]])[0]\n",
    "\n",
    "    # Previous state (hard) -> soft thereafter\n",
    "    s_prev_id = int(df_comm.iloc[origin_idx][\"state_id\"])\n",
    "    s_prev_probs = np.zeros(K_STATES); s_prev_probs[s_prev_id] = 1.0\n",
    "\n",
    "    samples = np.zeros((horizon, n_samples), dtype=float)\n",
    "    x_buf = Xwin.copy()\n",
    "\n",
    "    for m in range(n_samples):\n",
    "        s_probs = s_prev_probs.copy()\n",
    "        x_buf_m = x_buf.copy()\n",
    "\n",
    "        for h in range(horizon):\n",
    "            # Conditioning vector with soft state probs\n",
    "            state_soft = s_probs.reshape(1,-1)  # (1,K)\n",
    "            cond_vec = np.concatenate([comm_onehot.reshape(1,-1), state_soft], axis=1)\n",
    "\n",
    "            xb = torch.tensor(x_buf_m[np.newaxis, :, :], dtype=torch.float32, device=DEVICE)\n",
    "            cb = torch.tensor(cond_vec, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "            mu_full, logvar_case, mu_z, logv_z = model(xb, cb, temp=temp_z)\n",
    "            mu_full = mu_full.cpu().numpy()[0]\n",
    "            logvar_case = logvar_case.cpu().numpy()[0,0]\n",
    "            mu_case = mu_full[0]\n",
    "            sigma_case = math.sqrt(max(1e-8, math.exp(logvar_case))) * temp_out\n",
    "\n",
    "            # Sample next 'case' in SCALED space and clip to [0,1] for safety\n",
    "            y_case_scaled = float(np.clip(np.random.normal(mu_case, sigma_case), 0.0, 1.0))\n",
    "\n",
    "            # Build next-step scaled vector (use model means for mobility dims)\n",
    "            mu_full[0] = y_case_scaled\n",
    "            next_vec = mu_full.copy()\n",
    "\n",
    "            # Inverse-scale to ORIGINAL units for the case variable\n",
    "            tmp = np.zeros((1, len(FEATURES))); tmp[0,:] = next_vec\n",
    "            unscaled_case = scaler.inverse_transform(tmp)[0,0]\n",
    "            samples[h, m] = unscaled_case\n",
    "\n",
    "            # Autoregressive buffer update (keep scaled buffer)\n",
    "            x_buf_m = np.vstack([x_buf_m[1:], next_vec])\n",
    "\n",
    "            # Project next state's soft probs (mobility persistence)\n",
    "            pseudo = {\n",
    "                \"state_id\": np.argmax(s_probs),\n",
    "                \"Internal_Mobility_Index\": im_last, \"External_Mobility_Index\": em_last,\n",
    "                \"Commune\": comm_name\n",
    "            }\n",
    "            s_probs = transition_proba_soft(pd.Series(pseudo))\n",
    "\n",
    "    mean_path = np.mean(samples, axis=1)\n",
    "    return mean_path, samples\n",
    "\n",
    "# ============================================================\n",
    "# A) Tail-only Evaluation (as in your original setup)\n",
    "# ============================================================\n",
    "def evaluate_tail(horizon_list=H_LIST, test_days=TEST_DAYS):\n",
    "    rows = []\n",
    "    viz_cache = {}\n",
    "\n",
    "    for comm, dfc in df_scaled.groupby(\"Commune\"):\n",
    "        dfc = dfc.sort_values(\"Date\").reset_index(drop=True)\n",
    "        y_true_full = dfc[OBS_COL].values  # original units\n",
    "\n",
    "        idxs = np.where(dfc[\"is_test\"].values == 1)[0]\n",
    "        if len(idxs) == 0:\n",
    "            continue\n",
    "\n",
    "        for H in horizon_list:\n",
    "            cvae_err=[]; arima_err=[]; pers_err=[]\n",
    "            cvae_crps=[]; cvae_wis=[]; cvae_cov50=[]; cvae_cov90=[]\n",
    "            ar_crps=[];   ar_wis=[];   ar_cov50=[];   ar_cov90=[]\n",
    "            pe_crps=[];   pe_wis=[];   pe_cov50=[];   pe_cov90=[]\n",
    "\n",
    "            if comm in TARGET_COMMUNES and H==14:\n",
    "                viz_cache[(comm,H)] = {\"dates\": [], \"y\": [], \"cvae_mean\": [], \"cvae_p05\": [], \"cvae_p95\": [],\n",
    "                                       \"ar_mean\": [], \"ar_p05\": [], \"ar_p95\": []}\n",
    "\n",
    "            for origin_idx in idxs:\n",
    "                if origin_idx < SEQ_LEN or origin_idx + H >= len(dfc):\n",
    "                    continue\n",
    "\n",
    "                mean_c, samp_c = cvae_rollout(dfc, origin_idx, H, n_samples=N_ENSEMBLE,\n",
    "                                              temp_z=TEMP_Z, temp_out=TEMP_OUT)\n",
    "                if mean_c is None:\n",
    "                    continue\n",
    "\n",
    "                y_hist = y_true_full[:origin_idx+1]\n",
    "                mean_a, samp_a = arima_forecast(y_hist, H, n_samples=N_ENSEMBLE)\n",
    "                mean_p, samp_p = persistence_forecast(y_hist, H, n_samples=N_ENSEMBLE)\n",
    "\n",
    "                truth = y_true_full[origin_idx+1:origin_idx+1+H]\n",
    "\n",
    "                # step-wise metrics\n",
    "                for h in range(H):\n",
    "                    y_h = truth[h]\n",
    "\n",
    "                    # CVAE (already in original units)\n",
    "                    q_c = empirical_quantiles(samp_c[h,:], qs=(0.05,0.25,0.5,0.75,0.95))\n",
    "                    cvae_wis.append(wis_from_quantiles(y_h, [0.5,0.9], q_c))\n",
    "                    cvae_crps.append(crps_ensemble(y_h, samp_c[h,:]))\n",
    "                    cvae_cov50.append(coverage(y_h, q_c[0.25], q_c[0.75]))\n",
    "                    cvae_cov90.append(coverage(y_h, q_c[0.05], q_c[0.95]))\n",
    "                    cvae_err.append(abs(y_h - np.mean(samp_c[h,:])))\n",
    "\n",
    "                    # ARIMA\n",
    "                    q_a = empirical_quantiles(samp_a[h,:], qs=(0.05,0.25,0.5,0.75,0.95))\n",
    "                    ar_wis.append(wis_from_quantiles(y_h, [0.5,0.9], q_a))\n",
    "                    ar_crps.append(crps_ensemble(y_h, samp_a[h,:]))\n",
    "                    ar_cov50.append(coverage(y_h, q_a[0.25], q_a[0.75]))\n",
    "                    ar_cov90.append(coverage(y_h, q_a[0.05], q_a[0.95]))\n",
    "                    arima_err.append(abs(y_h - np.mean(samp_a[h,:])))\n",
    "\n",
    "                    # Persistence\n",
    "                    q_p = empirical_quantiles(samp_p[h,:], qs=(0.05,0.25,0.5,0.75,0.95))\n",
    "                    pe_wis.append(wis_from_quantiles(y_h, [0.5,0.9], q_p))\n",
    "                    pe_crps.append(crps_ensemble(y_h, samp_p[h,:]))\n",
    "                    pe_cov50.append(coverage(y_h, q_p[0.25], q_p[0.75]))\n",
    "                    pe_cov90.append(coverage(y_h, q_p[0.05], q_p[0.95]))\n",
    "                    pers_err.append(abs(y_h - np.mean(samp_p[h,:])))\n",
    "\n",
    "                # cache for visualization\n",
    "                if (comm in TARGET_COMMUNES) and (H==14):\n",
    "                    dates = dfc.loc[origin_idx+1:origin_idx+H, \"Date\"].tolist()\n",
    "                    viz_cache[(comm,H)][\"dates\"].append(dates)\n",
    "                    viz_cache[(comm,H)][\"y\"].append(truth.tolist())\n",
    "                    viz_cache[(comm,H)][\"cvae_mean\"].append(mean_c.tolist())\n",
    "                    viz_cache[(comm,H)][\"cvae_p05\"].append([np.quantile(samp_c[h,:],0.05) for h in range(H)])\n",
    "                    viz_cache[(comm,H)][\"cvae_p95\"].append([np.quantile(samp_c[h,:],0.95) for h in range(H)])\n",
    "                    viz_cache[(comm,H)][\"ar_mean\"].append(mean_a.tolist())\n",
    "                    viz_cache[(comm,H)][\"ar_p05\"].append([np.quantile(samp_a[h,:],0.05) for h in range(H)])\n",
    "                    viz_cache[(comm,H)][\"ar_p95\"].append([np.quantile(samp_a[h,:],0.95) for h in range(H)])\n",
    "\n",
    "            if len(cvae_err)==0:\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"Commune\": comm, \"horizon\": H,\n",
    "                \"MAE_CVAE\": float(np.mean(cvae_err)),\n",
    "                \"CRPS_CVAE\": float(np.mean(cvae_crps)),\n",
    "                \"WIS_CVAE\": float(np.mean(cvae_wis)),\n",
    "                \"COV50_CVAE\": float(np.mean(cvae_cov50)),\n",
    "                \"COV90_CVAE\": float(np.mean(cvae_cov90)),\n",
    "                \"MAE_ARIMA\": float(np.mean(arima_err)),\n",
    "                \"CRPS_ARIMA\": float(np.mean(ar_crps)),\n",
    "                \"WIS_ARIMA\": float(np.mean(ar_wis)),\n",
    "                \"COV50_ARIMA\": float(np.mean(ar_cov50)),\n",
    "                \"COV90_ARIMA\": float(np.mean(ar_cov90)),\n",
    "                \"MAE_PERSIST\": float(np.mean(pers_err)),\n",
    "                \"CRPS_PERSIST\": float(np.mean(pe_crps)),\n",
    "                \"WIS_PERSIST\": float(np.mean(pe_wis)),\n",
    "                \"COV50_PERSIST\": float(np.mean(pe_cov50)),\n",
    "                \"COV90_PERSIST\": float(np.mean(pe_cov90))\n",
    "            })\n",
    "\n",
    "    res_df = pd.DataFrame(rows)\n",
    "    res_df.to_csv(os.path.join(OUT_DIR, \"A_tail_metrics_by_commune.csv\"), index=False)\n",
    "    agg = res_df.groupby(\"horizon\").mean(numeric_only=True).reset_index()\n",
    "    agg.to_csv(os.path.join(OUT_DIR, \"A_tail_metrics_pooled_by_horizon.csv\"), index=False)\n",
    "    overall = res_df.mean(numeric_only=True).to_frame().T\n",
    "    overall.to_csv(os.path.join(OUT_DIR, \"A_tail_metrics_pooled_overall.csv\"), index=False)\n",
    "\n",
    "    print(\"=== [A] Tail-only: pooled by horizon ===\")\n",
    "    print(agg.round(4))\n",
    "    print(\"\\n=== [A] Tail-only: pooled overall ===\")\n",
    "    print(overall.round(4))\n",
    "\n",
    "    return res_df, agg, overall, viz_cache\n",
    "\n",
    "# ============================================================\n",
    "# B) Rolling / Regime-stratified Evaluation (recommended)\n",
    "#    (no retraining; just different origins)\n",
    "# ============================================================\n",
    "def pick_origins_regime_stratified(dfc, per_regime=4, min_gap=14, max_h=max(H_LIST)):\n",
    "    \"\"\"Pick ~per_regime origins per regime, separated by >= min_gap, with room for max_h.\"\"\"\n",
    "    idxs = []\n",
    "    n = len(dfc)\n",
    "    for s in sorted(dfc[\"state_id\"].unique().tolist()):\n",
    "        cand = np.where(dfc[\"state_id\"].values == s)[0]\n",
    "        cand = cand[(cand >= SEQ_LEN) & (cand <= n - max_h - 1)]\n",
    "        chosen = []\n",
    "        last = -10**9\n",
    "        for i in cand:\n",
    "            if i - last >= min_gap:\n",
    "                chosen.append(i); last = i\n",
    "            if len(chosen) >= per_regime:\n",
    "                break\n",
    "        idxs.extend(chosen)\n",
    "    return sorted(set(idxs))\n",
    "\n",
    "def pick_origins_rolling_every_k(dfc, step=7, max_h=max(H_LIST)):\n",
    "    \"\"\"Pick origins every 'step' days along the series.\"\"\"\n",
    "    n = len(dfc)\n",
    "    valid = list(range(SEQ_LEN, n - max_h))\n",
    "    return valid[::step]\n",
    "\n",
    "def pick_origins_incidence_quantiles(dfc, per_bucket=4, min_gap=14, max_h=max(H_LIST)):\n",
    "    \"\"\"Pick origins stratified by incidence quantiles (low/mid/high).\"\"\"\n",
    "    vals = dfc[CASE_COL].values\n",
    "    q1, q2 = np.quantile(vals, [0.33, 0.66])\n",
    "    buckets = {\n",
    "        \"low\":  np.where(vals <= q1)[0],\n",
    "        \"mid\":  np.where((vals > q1) & (vals <= q2))[0],\n",
    "        \"high\": np.where(vals > q2)[0]\n",
    "    }\n",
    "    idxs = []\n",
    "    n = len(dfc)\n",
    "    for _, cand in buckets.items():\n",
    "        cand = cand[(cand >= SEQ_LEN) & (cand <= n - max_h - 1)]\n",
    "        chosen = []\n",
    "        last = -10**9\n",
    "        for i in cand:\n",
    "            if i - last >= min_gap:\n",
    "                chosen.append(i); last = i\n",
    "            if len(chosen) >= per_bucket:\n",
    "                break\n",
    "        idxs.extend(chosen)\n",
    "    return sorted(set(idxs))\n",
    "\n",
    "def evaluate_with_selector(horizon_list=H_LIST, origin_selector=\"regime\", **selector_kwargs):\n",
    "    \"\"\"\n",
    "    origin_selector: 'regime' | 'rolling' | 'quantiles'\n",
    "    selector_kwargs: parameters for the selector.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    viz_cache = {}\n",
    "\n",
    "    for comm, dfc in df_scaled.groupby(\"Commune\"):\n",
    "        dfc = dfc.sort_values(\"Date\").reset_index(drop=True)\n",
    "        y_true_full = dfc[OBS_COL].values  # original units\n",
    "\n",
    "        # Select origins\n",
    "        if origin_selector == \"regime\":\n",
    "            origins = pick_origins_regime_stratified(dfc, **selector_kwargs)\n",
    "        elif origin_selector == \"rolling\":\n",
    "            origins = pick_origins_rolling_every_k(dfc, **selector_kwargs)\n",
    "        elif origin_selector == \"quantiles\":\n",
    "            origins = pick_origins_incidence_quantiles(dfc, **selector_kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown origin_selector\")\n",
    "\n",
    "        if len(origins) == 0:\n",
    "            continue\n",
    "\n",
    "        for H in horizon_list:\n",
    "            cvae_err=[]; arima_err=[]; pers_err=[]\n",
    "            cvae_crps=[]; cvae_wis=[]; cvae_cov50=[]; cvae_cov90=[]\n",
    "            ar_crps=[];   ar_wis=[];   ar_cov50=[];   ar_cov90=[]\n",
    "            pe_crps=[];   pe_wis=[];   pe_cov50=[];   pe_cov90=[]\n",
    "\n",
    "            if comm in TARGET_COMMUNES and H==14:\n",
    "                viz_cache[(comm,H)] = {\"dates\": [], \"y\": [], \"cvae_mean\": [], \"cvae_p05\": [], \"cvae_p95\": [],\n",
    "                                       \"ar_mean\": [], \"ar_p05\": [], \"ar_p95\": []}\n",
    "\n",
    "            for origin_idx in origins:\n",
    "                if origin_idx < SEQ_LEN or origin_idx + H >= len(dfc):\n",
    "                    continue\n",
    "\n",
    "                mean_c, samp_c = cvae_rollout(dfc, origin_idx, H, n_samples=N_ENSEMBLE,\n",
    "                                              temp_z=TEMP_Z, temp_out=TEMP_OUT)\n",
    "                if mean_c is None:\n",
    "                    continue\n",
    "\n",
    "                y_hist = y_true_full[:origin_idx+1]\n",
    "                mean_a, samp_a = arima_forecast(y_hist, H, n_samples=N_ENSEMBLE)\n",
    "                mean_p, samp_p = persistence_forecast(y_hist, H, n_samples=N_ENSEMBLE)\n",
    "\n",
    "                truth = y_true_full[origin_idx+1:origin_idx+1+H]\n",
    "\n",
    "                # step-wise metrics (all in ORIGINAL units)\n",
    "                for h in range(H):\n",
    "                    y_h = truth[h]\n",
    "\n",
    "                    q_c = empirical_quantiles(samp_c[h,:], qs=(0.05,0.25,0.5,0.75,0.95))\n",
    "                    cvae_wis.append(wis_from_quantiles(y_h, [0.5,0.9], q_c))\n",
    "                    cvae_crps.append(crps_ensemble(y_h, samp_c[h,:]))\n",
    "                    cvae_cov50.append(coverage(y_h, q_c[0.25], q_c[0.75]))\n",
    "                    cvae_cov90.append(coverage(y_h, q_c[0.05], q_c[0.95]))\n",
    "                    cvae_err.append(abs(y_h - np.mean(samp_c[h,:])))\n",
    "\n",
    "                    q_a = empirical_quantiles(samp_a[h,:], qs=(0.05,0.25,0.5,0.75,0.95))\n",
    "                    ar_wis.append(wis_from_quantiles(y_h, [0.5,0.9], q_a))\n",
    "                    ar_crps.append(crps_ensemble(y_h, samp_a[h,:]))\n",
    "                    ar_cov50.append(coverage(y_h, q_a[0.25], q_a[0.75]))\n",
    "                    ar_cov90.append(coverage(y_h, q_a[0.05], q_a[0.95]))\n",
    "                    arima_err.append(abs(y_h - np.mean(samp_a[h,:])))\n",
    "\n",
    "                    q_p = empirical_quantiles(samp_p[h,:], qs=(0.05,0.25,0.5,0.75,0.95))\n",
    "                    pe_wis.append(wis_from_quantiles(y_h, [0.5,0.9], q_p))\n",
    "                    pe_crps.append(crps_ensemble(y_h, samp_p[h,:]))\n",
    "                    pe_cov50.append(coverage(y_h, q_p[0.25], q_p[0.75]))\n",
    "                    pe_cov90.append(coverage(y_h, q_p[0.05], q_p[0.95]))\n",
    "                    pers_err.append(abs(y_h - np.mean(samp_p[h,:])))\n",
    "\n",
    "                # cache for figures (use last origin later)\n",
    "                if (comm in TARGET_COMMUNES) and (H==14):\n",
    "                    dates = dfc.loc[origin_idx+1:origin_idx+H, \"Date\"].tolist()\n",
    "                    viz_cache[(comm,H)][\"dates\"].append(dates)\n",
    "                    viz_cache[(comm,H)][\"y\"].append(truth.tolist())\n",
    "                    viz_cache[(comm,H)][\"cvae_mean\"].append(mean_c.tolist())\n",
    "                    viz_cache[(comm,H)][\"cvae_p05\"].append([np.quantile(samp_c[h,:],0.05) for h in range(H)])\n",
    "                    viz_cache[(comm,H)][\"cvae_p95\"].append([np.quantile(samp_c[h,:],0.95) for h in range(H)])\n",
    "                    viz_cache[(comm,H)][\"ar_mean\"].append(mean_a.tolist())\n",
    "                    viz_cache[(comm,H)][\"ar_p05\"].append([np.quantile(samp_a[h,:],0.05) for h in range(H)])\n",
    "                    viz_cache[(comm,H)][\"ar_p95\"].append([np.quantile(samp_a[h,:],0.95) for h in range(H)])\n",
    "\n",
    "            if len(cvae_err)==0:\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"Commune\": comm, \"horizon\": H,\n",
    "                \"MAE_CVAE\": float(np.mean(cvae_err)),\n",
    "                \"CRPS_CVAE\": float(np.mean(cvae_crps)),\n",
    "                \"WIS_CVAE\": float(np.mean(cvae_wis)),\n",
    "                \"COV50_CVAE\": float(np.mean(cvae_cov50)),\n",
    "                \"COV90_CVAE\": float(np.mean(cvae_cov90)),\n",
    "                \"MAE_ARIMA\": float(np.mean(arima_err)),\n",
    "                \"CRPS_ARIMA\": float(np.mean(ar_crps)),\n",
    "                \"WIS_ARIMA\": float(np.mean(ar_wis)),\n",
    "                \"COV50_ARIMA\": float(np.mean(ar_cov50)),\n",
    "                \"COV90_ARIMA\": float(np.mean(ar_cov90)),\n",
    "                \"MAE_PERSIST\": float(np.mean(pers_err)),\n",
    "                \"CRPS_PERSIST\": float(np.mean(pe_crps)),\n",
    "                \"WIS_PERSIST\": float(np.mean(pe_wis)),\n",
    "                \"COV50_PERSIST\": float(np.mean(pe_cov50)),\n",
    "                \"COV90_PERSIST\": float(np.mean(pe_cov90))\n",
    "            })\n",
    "\n",
    "    res_df = pd.DataFrame(rows)\n",
    "    res_df.to_csv(os.path.join(OUT_DIR, \"B_selector_metrics_by_commune.csv\"), index=False)\n",
    "    agg = res_df.groupby(\"horizon\").mean(numeric_only=True).reset_index()\n",
    "    agg.to_csv(os.path.join(OUT_DIR, \"B_selector_metrics_pooled_by_horizon.csv\"), index=False)\n",
    "    overall = res_df.mean(numeric_only=True).to_frame().T\n",
    "    overall.to_csv(os.path.join(OUT_DIR, \"B_selector_metrics_pooled_overall.csv\"), index=False)\n",
    "\n",
    "    print(\"=== [B] Selector-based: pooled by horizon ===\")\n",
    "    print(agg.round(4))\n",
    "    print(\"\\n=== [B] Selector-based: pooled overall ===\")\n",
    "    print(overall.round(4))\n",
    "\n",
    "    return res_df, agg, overall, viz_cache\n",
    "\n",
    "# ============================================================\n",
    "# Free-run reconstruction of full curve + shape metrics\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def cvae_free_run(dfc, start_idx=None):\n",
    "    \"\"\"\n",
    "    Recursive 1-step rollout until the end, starting from start_idx (default SEQ_LEN-1).\n",
    "    Returns predictions in ORIGINAL units (same as cvae_rollout output).\n",
    "    \"\"\"\n",
    "    if start_idx is None:\n",
    "        start_idx = SEQ_LEN - 1\n",
    "    n = len(dfc)\n",
    "    preds = []\n",
    "    for t in range(start_idx, n-1):\n",
    "        mean1, samp1 = cvae_rollout(dfc, t, horizon=1, n_samples=N_ENSEMBLE,\n",
    "                                    temp_z=TEMP_Z, temp_out=TEMP_OUT)\n",
    "        preds.append(float(mean1[0]) if mean1 is not None else np.nan)\n",
    "    return np.array(preds, dtype=float)\n",
    "\n",
    "def arima_free_run(y, start_idx=None):\n",
    "    \"\"\"Recursive ARIMA: refit on y[:t+1], forecast 1-step, advance.\"\"\"\n",
    "    if start_idx is None:\n",
    "        start_idx = SEQ_LEN - 1\n",
    "    n = len(y)\n",
    "    out = []\n",
    "    for t in range(start_idx, n-1):\n",
    "        mean, _ = arima_forecast(y[:t+1], horizon=1, n_samples=64)\n",
    "        out.append(float(mean[0]))\n",
    "    return np.array(out, dtype=float)\n",
    "\n",
    "def peak_metrics(y_true, y_pred):\n",
    "    \"\"\"Peak timing (days) and magnitude error.\"\"\"\n",
    "    i_t = int(np.nanargmax(y_true)); i_p = int(np.nanargmax(y_pred))\n",
    "    return {\"peak_timing_error_days\": i_p - i_t,\n",
    "            \"peak_mag_error\": float(abs(y_pred[i_p] - y_true[i_t]))}\n",
    "\n",
    "def series_shape_metrics(y_true, y_pred):\n",
    "    \"\"\"Correlation, MAE, RMSE, and MASE against naive-1.\"\"\"\n",
    "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float)\n",
    "    n = min(len(y_true), len(y_pred))\n",
    "    yt = y_true[:n]; yp = y_pred[:n]\n",
    "    corr = np.corrcoef(yt, yp)[0,1] if np.std(yt)>0 and np.std(yp)>0 else np.nan\n",
    "    mae_v = float(np.mean(np.abs(yt-yp)))\n",
    "    rmse_v = float(np.sqrt(np.mean((yt-yp)**2)))\n",
    "    mase_den = np.mean(np.abs(yt[1:] - yt[:-1])) if len(yt)>1 else np.nan\n",
    "    mase = mae_v / mase_den if (mase_den is not None and mase_den>0) else np.nan\n",
    "    return {\"corr\": float(corr), \"MAE\": mae_v, \"RMSE\": rmse_v, \"MASE\": float(mase)}\n",
    "\n",
    "# ============================================================\n",
    "# Visualization helpers\n",
    "# ============================================================\n",
    "def plot_panels(viz_cache, H=14, n_panels=6):\n",
    "    \"\"\"\n",
    "    Multi-commune panels: Observed vs CVAE vs ARIMA (90% bands),\n",
    "    using the last origin only for each commune (clearer for publication).\n",
    "    \"\"\"\n",
    "    communes = [c for c in TARGET_COMMUNES if (c,H) in viz_cache]\n",
    "    n = min(n_panels, len(communes))\n",
    "    if n == 0:\n",
    "        print(\"No communes cached for visualization.\")\n",
    "        return\n",
    "\n",
    "    nrows = math.ceil(n/2); ncols = 2\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(16, 5.5*nrows), sharex=False)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for ax, comm in zip(axes, communes[:n]):\n",
    "        block = viz_cache[(comm,H)]\n",
    "        dates  = block[\"dates\"][-1]\n",
    "        y      = block[\"y\"][-1]\n",
    "        cm     = block[\"cvae_mean\"][-1]; c05 = block[\"cvae_p05\"][-1]; c95 = block[\"cvae_p95\"][-1]\n",
    "        am     = block[\"ar_mean\"][-1];   a05 = block[\"ar_p05\"][-1];   a95 = block[\"ar_p95\"][-1]\n",
    "\n",
    "        ax.plot(dates, y, lw=2.0, color=\"black\", label=\"Observed\")\n",
    "        ax.fill_between(dates, c05, c95, alpha=0.20, label=\"CVAE 90%\", edgecolor='none')\n",
    "        ax.plot(dates, cm, lw=2.0, linestyle=\"--\", label=\"CVAE mean\")\n",
    "        ax.fill_between(dates, a05, a95, alpha=0.15, label=\"ARIMA 90%\", edgecolor='none')\n",
    "        ax.plot(dates, am, lw=2.0, linestyle=\"-.\", label=\"ARIMA mean\")\n",
    "\n",
    "        ax.set_title(f\"{comm} — {H}-day ahead\", fontsize=13)\n",
    "        ax.set_ylabel(\"Daily cases (7-day MA)\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"lower center\", ncol=4, frameon=False)\n",
    "    fig.autofmt_xdate()\n",
    "    fig.tight_layout(rect=[0,0.05,1,1])\n",
    "    path = os.path.join(OUT_DIR, f\"viz_obs_vs_cvae_arima_H{H}.png\")\n",
    "    fig.savefig(path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved: {path}\")\n",
    "\n",
    "def plot_last_origin(commune, H=14, viz_cache=None, outdir=OUT_DIR):\n",
    "    \"\"\"Single-commune, last-origin chart with 90% bands (Observed, CVAE, ARIMA).\"\"\"\n",
    "    if viz_cache is None or (commune, H) not in viz_cache or len(viz_cache[(commune,H)][\"dates\"]) == 0:\n",
    "        print(f\"No cache for {commune} (H={H}). Run evaluation first and ensure commune is in TARGET_COMMUNES.\")\n",
    "        return\n",
    "    block = viz_cache[(commune,H)]\n",
    "    dates  = block[\"dates\"][-1]\n",
    "    y      = block[\"y\"][-1]\n",
    "    cm     = block[\"cvae_mean\"][-1]; c05 = block[\"cvae_p05\"][-1]; c95 = block[\"cvae_p95\"][-1]\n",
    "    am     = block[\"ar_mean\"][-1];   a05 = block[\"ar_p05\"][-1];   a95 = block[\"ar_p95\"][-1]\n",
    "\n",
    "    plt.figure(figsize=(11,5))\n",
    "    plt.plot(dates, y, color=\"black\", lw=2.0, label=\"Observed\")\n",
    "    plt.fill_between(dates, c05, c95, alpha=0.20, label=\"CVAE 90%\", edgecolor='none')\n",
    "    plt.plot(dates, cm, lw=2.0, linestyle=\"--\", label=\"CVAE mean\")\n",
    "    plt.fill_between(dates, a05, a95, alpha=0.15, label=\"ARIMA 90%\", edgecolor='none')\n",
    "    plt.plot(dates, am, lw=2.0, linestyle=\"-.\", label=\"ARIMA mean\")\n",
    "    plt.title(f\"{commune} — {H}-day ahead (last origin)\")\n",
    "    plt.ylabel(\"Daily cases (7-day MA)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(frameon=False, ncol=4, loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    png_path = os.path.join(outdir, f\"last_origin_{commune.replace(' ','_')}_H{H}.png\")\n",
    "    plt.savefig(png_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved: {png_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# RUN — choose one or both evaluation modes\n",
    "# ============================================================\n",
    "\n",
    "# --- (A) Tail-only (as in your current paper draft) ---\n",
    "res_tail, agg_tail, overall_tail, viz_tail = evaluate_tail(H_LIST, TEST_DAYS)\n",
    "print(f\"✓ Tail-only CSVs saved in: {OUT_DIR}\")\n",
    "\n",
    "# Visualizations for (A)\n",
    "plot_panels(viz_tail, H=14, n_panels=6)\n",
    "plot_panels(viz_tail, H=7,  n_panels=6)\n",
    "for c in TARGET_COMMUNES:\n",
    "    plot_last_origin(c, H=14, viz_cache=viz_tail)\n",
    "\n",
    "# --- (B) Selector-based backtesting (recommended for paper claims) ---\n",
    "# Regime-stratified: 4 origins per regime, >=14d spacing\n",
    "res_sel, agg_sel, overall_sel, viz_sel = evaluate_with_selector(\n",
    "    horizon_list=[7,14],\n",
    "    origin_selector=\"regime\",\n",
    "    per_regime=4, min_gap=14\n",
    ")\n",
    "print(f\"✓ Selector-based CSVs saved in: {OUT_DIR}\")\n",
    "\n",
    "# Visualizations for (B)\n",
    "plot_panels(viz_sel, H=14, n_panels=6)\n",
    "\n",
    "# ============================================================\n",
    "# Comparative summary table by commune (both horizons)\n",
    "# ============================================================\n",
    "def wide_summary(res_df, tag):\n",
    "    \"\"\"\n",
    "    Build a wide table with per-commune metrics for H=7 and H=14\n",
    "    for CVAE, ARIMA, and Persistence (MAE/CRPS/WIS + coverages).\n",
    "    \"\"\"\n",
    "    keep_cols = [\"Commune\",\"horizon\",\n",
    "                 \"MAE_CVAE\",\"CRPS_CVAE\",\"WIS_CVAE\",\"COV50_CVAE\",\"COV90_CVAE\",\n",
    "                 \"MAE_ARIMA\",\"CRPS_ARIMA\",\"WIS_ARIMA\",\"COV50_ARIMA\",\"COV90_ARIMA\",\n",
    "                 \"MAE_PERSIST\",\"CRPS_PERSIST\",\"WIS_PERSIST\",\"COV50_PERSIST\",\"COV90_PERSIST\"]\n",
    "    tmp = res_df[keep_cols].copy()\n",
    "    # Aggregate by commune-horizon (mean across origins)\n",
    "    tmp = tmp.groupby([\"Commune\",\"horizon\"]).mean(numeric_only=True).reset_index()\n",
    "    # Pivot to wide: columns like MAE_CVAE_H7, MAE_CVAE_H14, ...\n",
    "    out = []\n",
    "    for h in sorted(tmp[\"horizon\"].unique()):\n",
    "        dfh = tmp[tmp[\"horizon\"]==h].drop(columns=[\"horizon\"]).copy()\n",
    "        dfh = dfh.set_index(\"Commune\")\n",
    "        dfh = dfh.add_suffix(f\"_H{h}\")\n",
    "        out.append(dfh)\n",
    "    wide = pd.concat(out, axis=1).reset_index()\n",
    "    wide = wide.sort_values(\"Commune\")\n",
    "    path = os.path.join(OUT_DIR, f\"{tag}_summary_by_commune_wide.csv\")\n",
    "    wide.to_csv(path, index=False)\n",
    "    print(f\"✓ Saved table: {path}\")\n",
    "    return wide\n",
    "\n",
    "wide_tail = wide_summary(res_tail, tag=\"A_tail\")\n",
    "wide_sel  = wide_summary(res_sel,  tag=\"B_selector\")\n",
    "\n",
    "print(\"\\n=== Head of selector-based summary (by commune) ===\")\n",
    "print(wide_sel.head(10).round(3))\n",
    "\n",
    "# ============================================================\n",
    "# Free-run reconstruction (shape metrics) for a subset or all communes\n",
    "# ============================================================\n",
    "shape_rows = []\n",
    "for comm, dfc in df_scaled.groupby(\"Commune\"):\n",
    "    dfc = dfc.sort_values(\"Date\").reset_index(drop=True)\n",
    "    start = SEQ_LEN - 1  # warm-up\n",
    "    # CVAE free-run (ORIGINAL units)\n",
    "    cvae_pred = cvae_free_run(dfc, start_idx=start)\n",
    "    y_true    = dfc[OBS_COL].values[start+1:]\n",
    "    # ARIMA free-run\n",
    "    ar_pred   = arima_free_run(dfc[OBS_COL].values, start_idx=start)\n",
    "\n",
    "    # Align lengths\n",
    "    nmin = min(len(y_true), len(cvae_pred), len(ar_pred))\n",
    "    y_true = y_true[:nmin]; cvae_pred = cvae_pred[:nmin]; ar_pred = ar_pred[:nmin]\n",
    "\n",
    "    pm_c = peak_metrics(y_true, cvae_pred);  sm_c = series_shape_metrics(y_true, cvae_pred)\n",
    "    pm_a = peak_metrics(y_true, ar_pred);    sm_a = series_shape_metrics(y_true, ar_pred)\n",
    "\n",
    "    shape_rows.append({\n",
    "        \"Commune\": comm,\n",
    "        \"corr_CVAE\": sm_c[\"corr\"], \"MASE_CVAE\": sm_c[\"MASE\"],\n",
    "        \"peak_dt_CVAE\": pm_c[\"peak_timing_error_days\"], \"peak_err_CVAE\": pm_c[\"peak_mag_error\"],\n",
    "        \"corr_ARIMA\": sm_a[\"corr\"], \"MASE_ARIMA\": sm_a[\"MASE\"],\n",
    "        \"peak_dt_ARIMA\": pm_a[\"peak_timing_error_days\"], \"peak_err_ARIMA\": pm_a[\"peak_mag_error\"],\n",
    "    })\n",
    "\n",
    "shape_df = pd.DataFrame(shape_rows).sort_values(\"Commune\")\n",
    "shape_path = os.path.join(OUT_DIR, \"C_free_run_shape_metrics.csv\")\n",
    "shape_df.to_csv(shape_path, index=False)\n",
    "print(f\"\\n✓ Free-run shape metrics saved: {shape_path}\")\n",
    "print(shape_df.head(10).round(3))\n",
    "\n",
    "# Optional: quick plot of free-run for target communes\n",
    "def plot_free_run(commune):\n",
    "    dfc = df_scaled[df_scaled[\"Commune\"]==commune].sort_values(\"Date\").reset_index(drop=True)\n",
    "    start = SEQ_LEN - 1\n",
    "    pred_c = cvae_free_run(dfc, start_idx=start)\n",
    "    pred_a = arima_free_run(dfc[CASE_COL].values, start_idx=start)\n",
    "    y = dfc[OBS_COL].values[start+1:]\n",
    "    nmin = min(len(y), len(pred_c), len(pred_a))\n",
    "    dates = dfc[\"Date\"].iloc[start+1:start+1+nmin]\n",
    "\n",
    "    plt.figure(figsize=(11,5))\n",
    "    plt.plot(dates, y[:nmin], color=\"black\", lw=2.0, label=\"Observed\")\n",
    "    plt.plot(dates, pred_c[:nmin], lw=2.0, linestyle=\"--\", label=\"CVAE (free-run)\")\n",
    "    plt.plot(dates, pred_a[:nmin], lw=2.0, linestyle=\"-.\", label=\"ARIMA (free-run)\")\n",
    "    plt.title(f\"{commune} — Free-run reconstruction\")\n",
    "    plt.ylabel(\"Daily cases (7-day MA)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(frameon=False, ncol=3, loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    p = os.path.join(OUT_DIR, f\"free_run_{commune.replace(' ','_')}.png\")\n",
    "    plt.savefig(p, dpi=300); plt.show()\n",
    "    print(f\"✓ Saved: {p}\")\n",
    "\n",
    "for c in TARGET_COMMUNES:\n",
    "    plot_free_run(c)\n",
    "\n",
    "print(\"\\nAll done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0976d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Full-curve CVAE reconstructions (regime-conditioned) +\n",
    "# Metrics: AW/POC/AAD (90% & 50%), CRPS, MAE/RMSE (median) +\n",
    "# Plots & summary tables (per-commune and pooled).\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import math, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch  # needed for @torch.no_grad()\n",
    "\n",
    "# IMPORTANT: we assume the following objects already exist from your previous code:\n",
    "# - df_scaled  (with FEATURES scaled and an extra column OBS_COL=\"Observed_Cases\" in ORIGINAL units)\n",
    "# - model, DEVICE, scaler, enc_comm, K_STATES, FEATURES, CASE_COL, SEQ_LEN, OUT_DIR, TARGET_COMMUNES\n",
    "# If not, run the previous cells first (with the scale fix that creates OBS_COL).\n",
    "\n",
    "# --------- Config for simulations ----------\n",
    "N_SIMS_FULL   = 400        # ensemble size per day\n",
    "TEMP_Z_FULL   = 1.25       # latent temperature\n",
    "TEMP_OUT_FULL = 1.25       # output variance temperature\n",
    "OBS_COL       = \"Observed_Cases\"  # original-units cases column from the scale fix\n",
    "\n",
    "# --------- Helpers: quantiles, coverage, CRPS ----------\n",
    "def empirical_quants(a, qs=(0.05,0.25,0.5,0.75,0.95)):\n",
    "    a = np.asarray(a).ravel()\n",
    "    return {float(q): float(np.quantile(a, q)) for q in qs}\n",
    "\n",
    "def crps_ens(y, samples):\n",
    "    s = np.sort(np.asarray(samples).ravel())\n",
    "    n = len(s); y = float(y)\n",
    "    e1 = np.mean(np.abs(s - y))                              # E|X - y|\n",
    "    diffs = np.diff(s)\n",
    "    weights = np.arange(1, n) * (n - np.arange(1, n))\n",
    "    e2 = 2.0 * np.sum(weights * diffs) / (n*n)               # E|X - X'|\n",
    "    return float(e1 - 0.5*e2)\n",
    "\n",
    "def cov_indicator(y, lo, hi):\n",
    "    y = float(y)\n",
    "    return 1.0 if (lo <= y <= hi) else 0.0\n",
    "\n",
    "# --------- AAD (Average Asymmetry Degree) ----------\n",
    "def asymmetry_degree(lo, med, hi):\n",
    "    \"\"\"\n",
    "    Signed asymmetry degree in [-1,1]:\n",
    "      A = ((hi - med) - (med - lo)) / (hi - lo)\n",
    "    A = 0   → symmetric w.r.t. the median\n",
    "    A > 0   → longer upper tail\n",
    "    A < 0   → longer lower tail\n",
    "    \"\"\"\n",
    "    lo = float(lo); med = float(med); hi = float(hi)\n",
    "    width = max(hi - lo, 1e-12)\n",
    "    return ((hi - med) - (med - lo)) / width\n",
    "\n",
    "# --------- One-hot (hard) state encoding ----------\n",
    "def state_onehot_from_id(state_id, K):\n",
    "    v = np.zeros((K,), dtype=float)\n",
    "    v[int(state_id)] = 1.0\n",
    "    return v\n",
    "\n",
    "# --------- Full-curve regime-conditioned CVAE simulation ----------\n",
    "@torch.no_grad()\n",
    "def simulate_full_curve_cvae(df_comm_scaled,\n",
    "                             n_sims=N_SIMS_FULL,\n",
    "                             temp_z=TEMP_Z_FULL,\n",
    "                             temp_out=TEMP_OUT_FULL,\n",
    "                             use_soft_states=False):\n",
    "    \"\"\"\n",
    "    Produce full-length simulated trajectories for one commune by chaining day-by-day:\n",
    "      - Inputs (past window) start with observed features (scaled).\n",
    "      - At each day t, conditioning uses [comm-onehot, state(t)].\n",
    "      - Only CASE is sampled; mobility dims are replaced by the observed mobility at t.\n",
    "      - The sampled CASE feeds the autoregressive buffer for the next day.\n",
    "    Returns dict with per-time quantiles/mean and arrays of truth (ORIGINAL units).\n",
    "    \"\"\"\n",
    "    # Safety: need enough history for the first prediction\n",
    "    if len(df_comm_scaled) <= SEQ_LEN:\n",
    "        return None\n",
    "\n",
    "    # Precompute encodings & arrays\n",
    "    comm_name  = df_comm_scaled.iloc[0][\"Commune\"]\n",
    "    comm_onehot= enc_comm.transform([[comm_name]])[0]          # shape (C,)\n",
    "    X_all_scaled = df_comm_scaled[FEATURES].values.astype(float) # scaled features (cases+mobilities)\n",
    "    states_id    = df_comm_scaled[\"state_id\"].values.astype(int) # 0..K-1\n",
    "    dates        = df_comm_scaled[\"Date\"].tolist()\n",
    "\n",
    "    T = len(df_comm_scaled)\n",
    "    steps = T - SEQ_LEN\n",
    "\n",
    "    # Storage for simulated trajectories (ORIGINAL units)\n",
    "    sims  = np.zeros((n_sims, steps), dtype=float)\n",
    "\n",
    "    # Truth in ORIGINAL units (use OBS_COL, not CASE_COL which is scaled)\n",
    "    truth = df_comm_scaled[OBS_COL].values[SEQ_LEN:]\n",
    "    # Quick sanity check to avoid scale mistakes:\n",
    "    assert np.nanmax(truth) > 5.0, \"Observed_Cases seems too small (maybe scaled?). Check the scale fix.\"\n",
    "\n",
    "    # Build initial buffer for each simulation (observed scaled features)\n",
    "    init_buf = X_all_scaled[:SEQ_LEN, :].copy()\n",
    "\n",
    "    for m in range(n_sims):\n",
    "        x_buf = init_buf.copy()\n",
    "        for t in range(SEQ_LEN, T):\n",
    "            # Condition vector at day t (regime-conditioned reconstruction)\n",
    "            if use_soft_states:\n",
    "                # Optional: approximate soft state via LR using s_{t-1} and mobility(t)\n",
    "                pseudo = {\n",
    "                    \"state_id\": states_id[t-1],\n",
    "                    \"Internal_Mobility_Index\": df_comm_scaled.iloc[t][\"Internal_Mobility_Index\"],\n",
    "                    \"External_Mobility_Index\": df_comm_scaled.iloc[t][\"External_Mobility_Index\"],\n",
    "                    \"Commune\": comm_name\n",
    "                }\n",
    "                st_vec = transition_proba_soft(pd.Series(pseudo))  # soft probs\n",
    "            else:\n",
    "                st_vec = state_onehot_from_id(states_id[t], K_STATES)  # hard one-hot of observed state\n",
    "\n",
    "            cond_vec = np.concatenate([comm_onehot, st_vec], axis=0)[None, :]  # (1, C+K)\n",
    "\n",
    "            xb = torch.tensor(x_buf[None, :, :], dtype=torch.float32, device=DEVICE)\n",
    "            cb = torch.tensor(cond_vec,          dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "            mu_full, logvar_case, mu_z, logv_z = model(xb, cb, temp=temp_z)\n",
    "            mu_full = mu_full.cpu().numpy()[0]       # (D,)\n",
    "            logvar  = float(logvar_case.cpu().numpy()[0,0])\n",
    "\n",
    "            mu_case    = float(mu_full[0])\n",
    "            sigma_case = math.sqrt(max(1e-10, math.exp(logvar))) * temp_out\n",
    "\n",
    "            # Sample CASE in scaled space and clip to [0,1]\n",
    "            y_scaled  = float(np.clip(np.random.normal(mu_case, sigma_case), 0.0, 1.0))\n",
    "\n",
    "            # Build next-step vector for buffer:\n",
    "            #   - CASE: sampled\n",
    "            #   - MOBILITIES: observed (scaled) at day t (forces consistency)\n",
    "            next_vec_scaled = mu_full.copy()\n",
    "            next_vec_scaled[0]  = y_scaled\n",
    "            next_vec_scaled[1:] = X_all_scaled[t, 1:]  # IM/EM observed, scaled\n",
    "\n",
    "            # Inverse-scale CASE only for storage (ORIGINAL units)\n",
    "            tmp = np.zeros((1, len(FEATURES))); tmp[0,:] = next_vec_scaled\n",
    "            y_unscaled = float(scaler.inverse_transform(tmp)[0,0])\n",
    "            sims[m, t-SEQ_LEN] = y_unscaled\n",
    "\n",
    "            # AR buffer update (scaled)\n",
    "            x_buf = np.vstack([x_buf[1:], next_vec_scaled])\n",
    "\n",
    "    # Aggregate ensemble at each time\n",
    "    q05, q25, q50, q75, q95, mean = [], [], [], [], [], []\n",
    "    for k in range(steps):\n",
    "        qu = empirical_quants(sims[:,k], qs=(0.05,0.25,0.5,0.75,0.95))\n",
    "        q05.append(qu[0.05]); q25.append(qu[0.25]); q50.append(qu[0.5])\n",
    "        q75.append(qu[0.75]); q95.append(qu[0.95])\n",
    "        mean.append(float(np.mean(sims[:,k])))\n",
    "\n",
    "    out = {\n",
    "        \"dates\": dates[SEQ_LEN:],\n",
    "        \"truth\": truth,  # ORIGINAL units\n",
    "        \"q05\": np.array(q05), \"q25\": np.array(q25), \"q50\": np.array(q50),\n",
    "        \"q75\": np.array(q75), \"q95\": np.array(q95), \"mean\": np.array(mean),\n",
    "        \"sims\": sims  # (n_sims, steps)\n",
    "    }\n",
    "    return out\n",
    "\n",
    "# --------- Metrics over a full curve ----------\n",
    "def metrics_full_curve(bundle):\n",
    "    \"\"\"\n",
    "    bundle: dict from simulate_full_curve_cvae\n",
    "    Returns per-commune summary metrics (all in ORIGINAL units).\n",
    "    \"\"\"\n",
    "    y   = np.asarray(bundle[\"truth\"]).astype(float)\n",
    "    q05 = np.asarray(bundle[\"q05\"]); q50 = np.asarray(bundle[\"q50\"]); q95 = np.asarray(bundle[\"q95\"])\n",
    "    q25 = np.asarray(bundle[\"q25\"]); q75 = np.asarray(bundle[\"q75\"])\n",
    "    mean_sims = np.asarray(bundle[\"mean\"])\n",
    "\n",
    "    width90 = q95 - q05\n",
    "    width50 = q75 - q25\n",
    "    # AW (average width)\n",
    "    AW90 = float(np.mean(width90))\n",
    "    AW50 = float(np.mean(width50))\n",
    "\n",
    "    # Coverage (POC) for central 90% and 50%\n",
    "    COV90 = float(np.mean((y >= q05) & (y <= q95)))\n",
    "    COV50 = float(np.mean((y >= q25) & (y <= q75)))\n",
    "\n",
    "    # AAD (signed) and absolute AAD for 90% and 50%\n",
    "    AAD90 = float(np.mean([asymmetry_degree(lo, md, hi) for lo, md, hi in zip(q05, q50, q95)]))\n",
    "    AAD90_abs = float(np.mean([abs(asymmetry_degree(lo, md, hi)) for lo, md, hi in zip(q05, q50, q95)]))\n",
    "    AAD50 = float(np.mean([asymmetry_degree(lo, md, hi) for lo, md, hi in zip(q25, q50, q75)]))\n",
    "    AAD50_abs = float(np.mean([abs(asymmetry_degree(lo, md, hi)) for lo, md, hi in zip(q25, q50, q75)]))\n",
    "\n",
    "    # CRPS (ensemble) averaged over time\n",
    "    sims = bundle[\"sims\"]\n",
    "    CRPS = float(np.mean([crps_ens(y[t], sims[:,t]) for t in range(len(y))]))\n",
    "\n",
    "    # Point accuracy (median)\n",
    "    MAE_med  = float(np.mean(np.abs(y - q50)))\n",
    "    RMSE_med = float(np.sqrt(np.mean((y - q50)**2)))\n",
    "\n",
    "    return {\n",
    "        \"AW90\": AW90, \"COV90\": COV90, \"AAD90\": AAD90, \"AAD90_abs\": AAD90_abs,\n",
    "        \"AW50\": AW50, \"COV50\": COV50, \"AAD50\": AAD50, \"AAD50_abs\": AAD50_abs,\n",
    "        \"CRPS\": CRPS, \"MAE_median\": MAE_med, \"RMSE_median\": RMSE_med\n",
    "    }\n",
    "\n",
    "# --------- Run full-curve simulations for many communes ----------\n",
    "def run_full_curve_all(df_scaled, communes=None, n_sims=N_SIMS_FULL,\n",
    "                       temp_z=TEMP_Z_FULL, temp_out=TEMP_OUT_FULL,\n",
    "                       use_soft_states=False, out_dir=OUT_DIR):\n",
    "    \"\"\"\n",
    "    Simulate and score all requested communes. Saves CSV with per-time quantiles\n",
    "    (per commune) and a summary table with metrics per commune + pooled.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    metrics_rows = []\n",
    "\n",
    "    groups = df_scaled.groupby(\"Commune\")\n",
    "    if communes is None:\n",
    "        communes = list(groups.groups.keys())\n",
    "\n",
    "    for comm in communes:\n",
    "        dfc = groups.get_group(comm).sort_values(\"Date\").reset_index(drop=True)\n",
    "        bundle = simulate_full_curve_cvae(dfc, n_sims=n_sims,\n",
    "                                          temp_z=temp_z, temp_out=temp_out,\n",
    "                                          use_soft_states=use_soft_states)\n",
    "        if bundle is None:\n",
    "            print(f\"[skip] {comm}: not enough data for SEQ_LEN={SEQ_LEN}\")\n",
    "            continue\n",
    "\n",
    "        # Save per-time outputs (helpful for SI)\n",
    "        df_out = pd.DataFrame({\n",
    "            \"Date\": bundle[\"dates\"],\n",
    "            \"Observed\": bundle[\"truth\"],\n",
    "            \"Mean\": bundle[\"mean\"],\n",
    "            \"Q05\": bundle[\"q05\"], \"Q25\": bundle[\"q25\"], \"Q50\": bundle[\"q50\"],\n",
    "            \"Q75\": bundle[\"q75\"], \"Q95\": bundle[\"q95\"]\n",
    "        })\n",
    "        csv_path = os.path.join(out_dir, f\"fullcurve_{comm.replace(' ','_')}.csv\")\n",
    "        df_out.to_csv(csv_path, index=False)\n",
    "\n",
    "        # Metrics\n",
    "        m = metrics_full_curve(bundle)\n",
    "        m[\"Commune\"] = comm\n",
    "        metrics_rows.append(m)\n",
    "        results[comm] = {\"bundle\": bundle, \"metrics\": m, \"csv\": csv_path}\n",
    "        print(f\"✓ {comm}: saved per-time CSV → {csv_path}\")\n",
    "\n",
    "    # Summary tables\n",
    "    met_df = pd.DataFrame(metrics_rows).set_index(\"Commune\")\n",
    "    met_df = met_df[[\"AW90\",\"COV90\",\"AAD90\",\"AAD90_abs\",\"AW50\",\"COV50\",\"AAD50\",\"AAD50_abs\",\n",
    "                     \"CRPS\",\"MAE_median\",\"RMSE_median\"]]\n",
    "    met_path = os.path.join(out_dir, \"fullcurve_metrics_by_commune.csv\")\n",
    "    met_df.to_csv(met_path)\n",
    "\n",
    "    pooled = met_df.mean().to_frame().T\n",
    "    pooled.index = [\"PooledMean\"]\n",
    "    pooled_path = os.path.join(out_dir, \"fullcurve_metrics_pooled.csv\")\n",
    "    pooled.to_csv(pooled_path)\n",
    "\n",
    "    print(\"\\n=== Full-curve metrics: by commune ===\")\n",
    "    print(met_df.round(3))\n",
    "    print(\"\\n=== Full-curve metrics: pooled mean ===\")\n",
    "    print(pooled.round(3))\n",
    "    print(f\"\\n✓ Saved summary CSVs in {out_dir}\")\n",
    "\n",
    "    return results, met_df, pooled\n",
    "\n",
    "# --------- Visualization: full-curve panels (publication-ready) ----------\n",
    "def plot_fullcurve_panels(results, communes, ncols=2, band=\"90\", out_dir=OUT_DIR, fname=\"fullcurve_panels.png\"):\n",
    "    \"\"\"\n",
    "    Plot observed vs CVAE (median & mean) with central band (90% or 50%) for selected communes.\n",
    "    \"\"\"\n",
    "    assert band in (\"90\",\"50\")\n",
    "    n = len(communes)\n",
    "    nrows = int(math.ceil(n / ncols))\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(16, 5.0*nrows), sharex=False)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for ax, comm in zip(axes, communes):\n",
    "        b = results[comm][\"bundle\"]\n",
    "        dates = b[\"dates\"]; obs = b[\"truth\"]\n",
    "        med = b[\"q50\"]; mean = b[\"mean\"]\n",
    "        if band == \"90\":\n",
    "            lo, hi = b[\"q05\"], b[\"q95\"]; lab = \"CVAE 90%\"\n",
    "        else:\n",
    "            lo, hi = b[\"q25\"], b[\"q75\"]; lab = \"CVAE 50%\"\n",
    "\n",
    "        ax.plot(dates, obs, color=\"black\", lw=2.0, label=\"Observed\")\n",
    "        ax.fill_between(dates, lo, hi, alpha=0.2, label=lab, edgecolor='none')\n",
    "        ax.plot(dates, med,  lw=2.0, linestyle=\"--\", label=\"CVAE median\")\n",
    "        ax.plot(dates, mean, lw=1.8, linestyle=\"-.\", label=\"CVAE mean\")\n",
    "        ax.set_title(comm)\n",
    "        ax.set_ylabel(\"Daily cases (7-day MA)\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"lower center\", ncol=4, frameon=False)\n",
    "    fig.autofmt_xdate()\n",
    "    fig.tight_layout(rect=[0,0.05,1,1])\n",
    "    path = os.path.join(out_dir, fname)\n",
    "    fig.savefig(path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved: {path}\")\n",
    "\n",
    "# ============================================================\n",
    "# RUN: full-curve simulations + metrics + plots\n",
    "# (Select a subset for panels; all communes go to CSV + table)\n",
    "# ============================================================\n",
    "COMM_FOR_PANELS = TARGET_COMMUNES   # reuse your earlier selection\n",
    "\n",
    "results_full, table_full, pooled_full = run_full_curve_all(\n",
    "    df_scaled,\n",
    "    communes=None,                 # None -> all communes in df_scaled\n",
    "    n_sims=N_SIMS_FULL,\n",
    "    temp_z=TEMP_Z_FULL,\n",
    "    temp_out=TEMP_OUT_FULL,\n",
    "    use_soft_states=False,         # True: LR-projected soft states instead of observed\n",
    "    out_dir=OUT_DIR\n",
    ")\n",
    "\n",
    "# Plots (90% band by default; change band=\"50\" if needed)\n",
    "plot_fullcurve_panels(results_full, COMM_FOR_PANELS, ncols=2, band=\"90\",\n",
    "                      out_dir=OUT_DIR, fname=\"fullcurve_panels_90.png\")\n",
    "plot_fullcurve_panels(results_full, COMM_FOR_PANELS, ncols=2, band=\"50\",\n",
    "                      out_dir=OUT_DIR, fname=\"fullcurve_panels_50.png\")\n",
    "\n",
    "# Pretty table for the paper (rounded)\n",
    "display_tbl = table_full.copy().round({\n",
    "    \"AW90\":3,\"COV90\":3,\"AAD90\":3,\"AAD90_abs\":3,\n",
    "    \"AW50\":3,\"COV50\":3,\"AAD50\":3,\"AAD50_abs\":3,\n",
    "    \"CRPS\":3,\"MAE_median\":3,\"RMSE_median\":3\n",
    "})\n",
    "print(\"\\n=== Table (rounded) — Full-curve CVAE metrics by commune ===\")\n",
    "print(display_tbl)\n",
    "print(\"\\n=== Pooled mean (rounded) ===\")\n",
    "print(pooled_full.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL-CURVE (toda la serie) — CVAE & ARIMA con bandas\n",
    "# Métricas (AW/POC/AAD/CRPS/MAE/RMSE), tablas y paneles 600 dpi\n",
    "# Requiere que ya existan: df_scaled, FEATURES, CASE_COL, OBS_COL,\n",
    "# enc_comm, scaler, K_STATES, model, DEVICE, OUT_DIR, TARGET_COMMUNES.\n",
    "# ============================================================\n",
    "\n",
    "import os, math, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# ---------- Config ----------\n",
    "N_SIMS_FULL   = 400          # tamaño del ensemble por día (CVAE y ARIMA)\n",
    "TEMP_Z_FULL   = 1.25\n",
    "TEMP_OUT_FULL = 1.25\n",
    "OBS_COL       = \"Observed_Cases\"  # columna en unidades originales\n",
    "ARIMA_GRID    = [(1,0,0),(1,1,0),(0,1,1),(1,1,1),(2,1,1)]  # por si no estuviera definida\n",
    "\n",
    "# ---------- Utilidades ----------\n",
    "def empirical_quants(a, qs=(0.05,0.25,0.5,0.75,0.95)):\n",
    "    a = np.asarray(a).ravel()\n",
    "    return {float(q): float(np.quantile(a, q)) for q in qs}\n",
    "\n",
    "def crps_ens(y, samples):\n",
    "    s = np.sort(np.asarray(samples).ravel())\n",
    "    n = len(s); y = float(y)\n",
    "    e1 = np.mean(np.abs(s - y))\n",
    "    diffs = np.diff(s)\n",
    "    weights = np.arange(1, n) * (n - np.arange(1, n))\n",
    "    e2 = 2.0 * np.sum(weights * diffs) / (n*n)\n",
    "    return float(e1 - 0.5*e2)\n",
    "\n",
    "def asymmetry_degree(lo, med, hi):\n",
    "    lo = float(lo); med = float(med); hi = float(hi)\n",
    "    width = max(hi - lo, 1e-12)\n",
    "    return ((hi - med) - (med - lo)) / width\n",
    "\n",
    "def state_onehot_from_id(state_id, K):\n",
    "    v = np.zeros((K,), dtype=float)\n",
    "    v[int(state_id)] = 1.0\n",
    "    return v\n",
    "\n",
    "# ---------- Simulación CVAE a lo largo de TODA la curva ----------\n",
    "@torch.no_grad()\n",
    "def simulate_full_curve_cvae(df_comm_scaled,\n",
    "                             n_sims=N_SIMS_FULL,\n",
    "                             temp_z=TEMP_Z_FULL,\n",
    "                             temp_out=TEMP_OUT_FULL,\n",
    "                             use_soft_states=False):\n",
    "    if len(df_comm_scaled) <= SEQ_LEN:\n",
    "        return None\n",
    "\n",
    "    comm_name   = df_comm_scaled.iloc[0][\"Commune\"]\n",
    "    comm_onehot = enc_comm.transform([[comm_name]])[0]\n",
    "    X_all_scaled = df_comm_scaled[FEATURES].values.astype(float)     # SCALED\n",
    "    states_id    = df_comm_scaled[\"state_id\"].values.astype(int)\n",
    "    dates        = df_comm_scaled[\"Date\"].tolist()\n",
    "\n",
    "    T = len(df_comm_scaled)\n",
    "    steps = T - SEQ_LEN\n",
    "    sims  = np.zeros((n_sims, steps), dtype=float)\n",
    "    truth = df_comm_scaled[OBS_COL].values[SEQ_LEN:]\n",
    "    assert np.nanmax(truth) > 5.0, \"Observed_Cases parece escalado; revisa tu flujo de escala.\"\n",
    "\n",
    "    init_buf = X_all_scaled[:SEQ_LEN, :].copy()\n",
    "\n",
    "    for m in range(n_sims):\n",
    "        x_buf = init_buf.copy()\n",
    "        for t in range(SEQ_LEN, T):\n",
    "            if use_soft_states:\n",
    "                pseudo = {\n",
    "                    \"state_id\": states_id[t-1],\n",
    "                    \"Internal_Mobility_Index\": df_comm_scaled.iloc[t][\"Internal_Mobility_Index\"],\n",
    "                    \"External_Mobility_Index\": df_comm_scaled.iloc[t][\"External_Mobility_Index\"],\n",
    "                    \"Commune\": comm_name\n",
    "                }\n",
    "                st_vec = transition_proba_soft(pd.Series(pseudo))\n",
    "            else:\n",
    "                st_vec = state_onehot_from_id(states_id[t], K_STATES)\n",
    "\n",
    "            cond_vec = np.concatenate([comm_onehot, st_vec], axis=0)[None, :]\n",
    "\n",
    "            xb = torch.tensor(x_buf[None, :, :], dtype=torch.float32, device=DEVICE)\n",
    "            cb = torch.tensor(cond_vec,          dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "            mu_full, logvar_case, _, _ = model(xb, cb, temp=temp_z)\n",
    "            mu_full = mu_full.cpu().numpy()[0]\n",
    "            logvar  = float(logvar_case.cpu().numpy()[0,0])\n",
    "            mu_case = float(mu_full[0])\n",
    "            sigma_case = math.sqrt(max(1e-10, math.exp(logvar))) * temp_out\n",
    "\n",
    "            y_scaled = float(np.clip(np.random.normal(mu_case, sigma_case), 0.0, 1.0))\n",
    "            next_vec_scaled = mu_full.copy()\n",
    "            next_vec_scaled[0]  = y_scaled\n",
    "            next_vec_scaled[1:] = X_all_scaled[t, 1:]  # IM/EM observadas (SCALED)\n",
    "\n",
    "            tmp = np.zeros((1, len(FEATURES))); tmp[0,:] = next_vec_scaled\n",
    "            y_unscaled = float(scaler.inverse_transform(tmp)[0,0])\n",
    "            sims[m, t-SEQ_LEN] = y_unscaled\n",
    "\n",
    "            x_buf = np.vstack([x_buf[1:], next_vec_scaled])\n",
    "\n",
    "    # Agrega cuantiles y media\n",
    "    q05=q25=q50=q75=q95=mean=None\n",
    "    q05=[]; q25=[]; q50=[]; q75=[]; q95=[]; mean=[]\n",
    "    for k in range(steps):\n",
    "        qu = empirical_quants(sims[:,k], qs=(0.05,0.25,0.5,0.75,0.95))\n",
    "        q05.append(qu[0.05]); q25.append(qu[0.25]); q50.append(qu[0.5])\n",
    "        q75.append(qu[0.75]); q95.append(qu[0.95])\n",
    "        mean.append(float(np.mean(sims[:,k])))\n",
    "\n",
    "    return {\n",
    "        \"dates\": dates[SEQ_LEN:],\n",
    "        \"truth\": truth,\n",
    "        \"q05\": np.array(q05), \"q25\": np.array(q25), \"q50\": np.array(q50),\n",
    "        \"q75\": np.array(q75), \"q95\": np.array(q95), \"mean\": np.array(mean),\n",
    "        \"sims\": sims\n",
    "    }\n",
    "\n",
    "# ---------- ARIMA rolling 1-paso con bandas para TODA la curva ----------\n",
    "def arima_full_curve_with_bands(y_orig, dates, start_idx=None, n_sims=N_SIMS_FULL):\n",
    "    \"\"\"\n",
    "    y_orig: serie en UNIDADES ORIGINALES (usa OBS_COL!)\n",
    "    Devuelve bundle con q05/q25/q50/q75/q95 y muestras (Normal approx).\n",
    "    \"\"\"\n",
    "    if start_idx is None:\n",
    "        start_idx = SEQ_LEN - 1\n",
    "    y = np.asarray(y_orig, float)\n",
    "    n = len(y)\n",
    "    steps = n - SEQ_LEN\n",
    "    mean = np.zeros(steps)\n",
    "    q05  = np.zeros(steps); q25 = np.zeros(steps); q50 = np.zeros(steps)\n",
    "    q75  = np.zeros(steps); q95 = np.zeros(steps)\n",
    "    sims = np.zeros((n_sims, steps))\n",
    "\n",
    "    Z05, Z25 = -1.6448536269514729, -0.6744897501960817\n",
    "    Z75, Z95 =  0.6744897501960817,  1.6448536269514729\n",
    "\n",
    "    k = 0\n",
    "    for t in range(start_idx, n-1):\n",
    "        y_train = y[:t+1]\n",
    "        best, best_aic = None, np.inf\n",
    "        for order in ARIMA_GRID:\n",
    "            try:\n",
    "                res = ARIMA(y_train, order=order).fit()\n",
    "                if res.aic < best_aic:\n",
    "                    best, best_aic = res, res.aic\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if best is None:\n",
    "            mu = y_train[-1]\n",
    "            std = np.std(np.diff(y_train[-30:])) if len(y_train) > 2 else 1.0\n",
    "        else:\n",
    "            fc  = best.get_forecast(steps=1)\n",
    "            mu  = float(fc.predicted_mean[0])\n",
    "            var = float(fc.var_pred_mean[0])\n",
    "            std = math.sqrt(max(var, 1e-12))\n",
    "\n",
    "        mean[k] = mu; q50[k] = mu\n",
    "        q25[k] = max(0.0, mu + Z25*std); q75[k] = max(0.0, mu + Z75*std)\n",
    "        q05[k] = max(0.0, mu + Z05*std); q95[k] = max(0.0, mu + Z95*std)\n",
    "        sims[:, k] = np.random.normal(mu, std, size=n_sims)\n",
    "        k += 1\n",
    "\n",
    "    return {\n",
    "        \"dates\": dates[SEQ_LEN:],\n",
    "        \"truth\": y[SEQ_LEN:],\n",
    "        \"q05\": q05, \"q25\": q25, \"q50\": q50, \"q75\": q75, \"q95\": q95,\n",
    "        \"mean\": mean, \"sims\": sims\n",
    "    }\n",
    "\n",
    "# ---------- Métricas de curva completa ----------\n",
    "def metrics_full_curve(bundle):\n",
    "    y   = np.asarray(bundle[\"truth\"]).astype(float)\n",
    "    q05 = np.asarray(bundle[\"q05\"]); q50 = np.asarray(bundle[\"q50\"]); q95 = np.asarray(bundle[\"q95\"])\n",
    "    q25 = np.asarray(bundle[\"q25\"]); q75 = np.asarray(bundle[\"q75\"])\n",
    "    sims= bundle[\"sims\"]\n",
    "\n",
    "    width90 = q95 - q05; width50 = q75 - q25\n",
    "    out = {\n",
    "        \"AW90\": float(np.mean(width90)),\n",
    "        \"COV90\": float(np.mean((y >= q05) & (y <= q95))),\n",
    "        \"AAD90\": float(np.mean([asymmetry_degree(lo, md, hi) for lo, md, hi in zip(q05, q50, q95)])),\n",
    "        \"AAD90_abs\": float(np.mean([abs(asymmetry_degree(lo, md, hi)) for lo, md, hi in zip(q05, q50, q95)])),\n",
    "        \"AW50\": float(np.mean(width50)),\n",
    "        \"COV50\": float(np.mean((y >= q25) & (y <= q75))),\n",
    "        \"AAD50\": float(np.mean([asymmetry_degree(lo, md, hi) for lo, md, hi in zip(q25, q50, q75)])),\n",
    "        \"AAD50_abs\": float(np.mean([abs(asymmetry_degree(lo, md, hi)) for lo, md, hi in zip(q25, q50, q75)])),\n",
    "        \"CRPS\": float(np.mean([crps_ens(y[t], sims[:,t]) for t in range(len(y))])),\n",
    "        \"MAE_median\": float(np.mean(np.abs(y - q50))),\n",
    "        \"RMSE_median\": float(np.sqrt(np.mean((y - q50)**2)))\n",
    "    }\n",
    "    return out\n",
    "\n",
    "# ---------- Ejecuta simulación de TODA la curva para todas las comunas ----------\n",
    "def run_full_curve_both(df_scaled, communes=None, n_sims=N_SIMS_FULL,\n",
    "                        temp_z=TEMP_Z_FULL, temp_out=TEMP_OUT_FULL, out_dir=OUT_DIR):\n",
    "    results_cvae, results_arima = {}, {}\n",
    "    rows_cvae, rows_arima = [], []\n",
    "\n",
    "    groups = df_scaled.groupby(\"Commune\")\n",
    "    if communes is None:\n",
    "        communes = list(groups.groups.keys())\n",
    "\n",
    "    for comm in communes:\n",
    "        dfc = groups.get_group(comm).sort_values(\"Date\").reset_index(drop=True)\n",
    "        # CVAE\n",
    "        b_cvae = simulate_full_curve_cvae(dfc, n_sims=n_sims, temp_z=temp_z, temp_out=temp_out)\n",
    "        if b_cvae is None: \n",
    "            print(f\"[skip] {comm}: insufficient length\")\n",
    "            continue\n",
    "        results_cvae[comm] = b_cvae\n",
    "        m_c = metrics_full_curve(b_cvae); m_c[\"Commune\"] = comm; rows_cvae.append(m_c)\n",
    "        pd.DataFrame({\n",
    "            \"Date\": b_cvae[\"dates\"], \"Observed\": b_cvae[\"truth\"], \"Mean\": b_cvae[\"mean\"],\n",
    "            \"Q05\": b_cvae[\"q05\"], \"Q25\": b_cvae[\"q25\"], \"Q50\": b_cvae[\"q50\"], \"Q75\": b_cvae[\"q75\"], \"Q95\": b_cvae[\"q95\"]\n",
    "        }).to_csv(os.path.join(out_dir, f\"fullcurve_CVAE_{comm.replace(' ','_')}.csv\"), index=False)\n",
    "\n",
    "        # ARIMA (¡en OBS_COL!)\n",
    "        y_obs = dfc[OBS_COL].values\n",
    "        b_ari = arima_full_curve_with_bands(y_obs, dfc[\"Date\"].tolist(), start_idx=SEQ_LEN-1, n_sims=n_sims)\n",
    "        results_arima[comm] = b_ari\n",
    "        m_a = metrics_full_curve(b_ari); m_a[\"Commune\"] = comm; rows_arima.append(m_a)\n",
    "        pd.DataFrame({\n",
    "            \"Date\": b_ari[\"dates\"], \"Observed\": b_ari[\"truth\"], \"Mean\": b_ari[\"mean\"],\n",
    "            \"Q05\": b_ari[\"q05\"], \"Q25\": b_ari[\"q25\"], \"Q50\": b_ari[\"q50\"], \"Q75\": b_ari[\"q75\"], \"Q95\": b_ari[\"q95\"]\n",
    "        }).to_csv(os.path.join(out_dir, f\"fullcurve_ARIMA_{comm.replace(' ','_')}.csv\"), index=False)\n",
    "\n",
    "        print(f\"✓ {comm}: CSV CVAE/ARIMA guardados (curva completa)\")\n",
    "\n",
    "    # Tablas por comuna\n",
    "    tbl_c = pd.DataFrame(rows_cvae).set_index(\"Commune\").sort_index()\n",
    "    tbl_a = pd.DataFrame(rows_arima).set_index(\"Commune\").sort_index()\n",
    "    tbl_c.to_csv(os.path.join(out_dir, \"fullcurve_metrics_CVAE_by_commune.csv\"))\n",
    "    tbl_a.to_csv(os.path.join(out_dir, \"fullcurve_metrics_ARIMA_by_commune.csv\"))\n",
    "\n",
    "    # Tabla combinada (sufijos)\n",
    "    combo = tbl_c.add_suffix(\"_CVAE\").join(tbl_a.add_suffix(\"_ARIMA\"), how=\"inner\")\n",
    "    combo.to_csv(os.path.join(out_dir, \"fullcurve_metrics_COMBINED_by_commune.csv\"))\n",
    "\n",
    "    # Pooled\n",
    "    pooled_c = tbl_c.mean().to_frame().T; pooled_c.index = [\"PooledMean_CVAE\"]\n",
    "    pooled_a = tbl_a.mean().to_frame().T; pooled_a.index = [\"PooledMean_ARIMA\"]\n",
    "    pooled = pd.concat([pooled_c, pooled_a], axis=0)\n",
    "    pooled.to_csv(os.path.join(out_dir, \"fullcurve_metrics_pooled_CVAE_ARIMA.csv\"))\n",
    "\n",
    "    print(\"\\n=== Métricas por comuna — CVAE ===\"); print(tbl_c.round(3))\n",
    "    print(\"\\n=== Métricas por comuna — ARIMA ===\"); print(tbl_a.round(3))\n",
    "    print(\"\\n=== Pooled (medias) ===\"); print(pooled.round(3))\n",
    "    print(f\"\\n✓ Tablas guardadas en {out_dir}\")\n",
    "\n",
    "    return results_cvae, results_arima, combo, pooled\n",
    "\n",
    "# ---------- Paneles (ambas bandas y ambos modelos) ----------\n",
    "def plot_fullcurve_panels_both(results_cvae, results_arima, communes=None,\n",
    "                               band=\"90\", ncols=3, out_dir=OUT_DIR,\n",
    "                               fname=\"fullcurve_panels_CVAE_ARIMA.png\"):\n",
    "    assert band in (\"90\",\"50\")\n",
    "    if communes is None:\n",
    "        communes = sorted(list(results_cvae.keys()))\n",
    "\n",
    "    n = len(communes)\n",
    "    nrows = int(math.ceil(n / ncols))\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(5.6*ncols, 4.2*nrows), sharex=False)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for ax, comm in zip(axes, communes):\n",
    "        b_c = results_cvae[comm]; b_a = results_arima[comm]\n",
    "        dates = b_c[\"dates\"]; obs = b_c[\"truth\"]\n",
    "        med_c, mean_c = b_c[\"q50\"], b_c[\"mean\"]\n",
    "        mean_a = b_a[\"mean\"]\n",
    "\n",
    "        if band == \"90\":\n",
    "            lo_c, hi_c = b_c[\"q05\"], b_c[\"q95\"]; lab_c = \"CVAE 90%\"\n",
    "            lo_a, hi_a = b_a[\"q05\"], b_a[\"q95\"]; lab_a = \"ARIMA 90%\"\n",
    "        else:\n",
    "            lo_c, hi_c = b_c[\"q25\"], b_c[\"q75\"]; lab_c = \"CVAE 50%\"\n",
    "            lo_a, hi_a = b_a[\"q25\"], b_a[\"q75\"]; lab_a = \"ARIMA 50%\"\n",
    "\n",
    "        ax.plot(dates, obs, color=\"black\", lw=2.0, label=\"Observed\", zorder=5)\n",
    "        ax.fill_between(dates, lo_c, hi_c, alpha=0.18, label=lab_c, edgecolor='none', zorder=1)\n",
    "        ax.fill_between(dates, lo_a, hi_a, alpha=0.18, label=lab_a, edgecolor='none', zorder=2)\n",
    "        ax.plot(dates, med_c,  lw=2.0, linestyle=\"--\", label=\"CVAE median\", zorder=6)\n",
    "        ax.plot(dates, mean_a, lw=2.0, linestyle=\"-.\", label=\"ARIMA mean\",  zorder=6)\n",
    "        ax.set_title(comm)\n",
    "        ax.set_ylabel(\"Daily cases (7-day MA)\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # leyenda global\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"lower center\", ncol=5, frameon=False)\n",
    "    fig.autofmt_xdate()\n",
    "    fig.tight_layout(rect=[0,0.05,1,1])\n",
    "    path = os.path.join(out_dir, fname)\n",
    "    fig.savefig(path, dpi=600)   # alta resolución para publicación\n",
    "    plt.show()\n",
    "    print(f\"✓ Panel guardado: {path}\")\n",
    "\n",
    "# ============================\n",
    "# Ejecutar todo (curva completa)\n",
    "# ============================\n",
    "ALL_COMMUNES = sorted(df_scaled[\"Commune\"].unique().tolist())  # o usa TARGET_COMMUNES\n",
    "res_cvae, res_arima, table_combined, pooled = run_full_curve_both(\n",
    "    df_scaled,\n",
    "    communes=ALL_COMMUNES,\n",
    "    n_sims=N_SIMS_FULL,\n",
    "    temp_z=TEMP_Z_FULL, temp_out=TEMP_OUT_FULL,\n",
    "    out_dir=OUT_DIR\n",
    ")\n",
    "\n",
    "# Paneles 90% y 50% para TODAS las comunas (cuidado: figura larga si hay muchas)\n",
    "plot_fullcurve_panels_both(res_cvae, res_arima, communes=ALL_COMMUNES,\n",
    "                           band=\"90\", ncols=3, out_dir=OUT_DIR,\n",
    "                           fname=\"fullcurve_panels_90_CVAE_ARIMA.png\")\n",
    "\n",
    "plot_fullcurve_panels_both(res_cvae, res_arima, communes=ALL_COMMUNES,\n",
    "                           band=\"50\", ncols=3, out_dir=OUT_DIR,\n",
    "                           fname=\"fullcurve_panels_50_CVAE_ARIMA.png\")\n",
    "\n",
    "# Vista rápida de tabla combinada (redondeada) para el paper\n",
    "print(\"\\n=== Tabla combinada (redondeada) — métricas por comuna (curva completa) ===\")\n",
    "print(table_combined.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7dae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Paper figure: CVAE-only panels (Observed + CVAE median/mean + 50% band)\n",
    "# - Observed: black solid line\n",
    "# - CVAE 50% band: lighter blue, higher transparency\n",
    "# - CVAE median: blue dashed\n",
    "# - CVAE mean: orange solid (no dashes, per request)\n",
    "# - High-quality export (PNG + PDF)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# If you don't have this list in scope, (re)define it:\n",
    "TARGET_COMMUNES = [\"La Florida\", \"Cerrillos\", \"Vitacura\",\n",
    "                   \"Providencia\", \"Las Condes\", \"Santiago\"]\n",
    "\n",
    "def _ensure_bundles_for(communes,\n",
    "                        results_dict=None,\n",
    "                        n_sims=400,\n",
    "                        temp_z=1.25,\n",
    "                        temp_out=1.25,\n",
    "                        use_soft_states=False):\n",
    "    \"\"\"\n",
    "    Ensure we have the full-curve simulation bundles for each commune.\n",
    "    Uses 'results_full' if available; otherwise calls simulate_full_curve_cvae.\n",
    "    Returns: dict {commune: bundle}\n",
    "      bundle keys: dates, q05, q25, q50, q75, q95, mean, truth (np arrays)\n",
    "    \"\"\"\n",
    "    bundles = {}\n",
    "    # Try to reuse an existing dictionary produced by run_full_curve_all\n",
    "    if results_dict is None:\n",
    "        try:\n",
    "            results_dict = results_full  # must exist if already computed\n",
    "        except NameError:\n",
    "            results_dict = None\n",
    "\n",
    "    groups = df_scaled.groupby(\"Commune\")\n",
    "    for comm in communes:\n",
    "        if (results_dict is not None) and (comm in results_dict):\n",
    "            bundles[comm] = results_dict[comm][\"bundle\"]\n",
    "        else:\n",
    "            dfc = groups.get_group(comm).sort_values(\"Date\").reset_index(drop=True)\n",
    "            b = simulate_full_curve_cvae(\n",
    "                dfc, n_sims=n_sims, temp_z=temp_z, temp_out=temp_out,\n",
    "                use_soft_states=use_soft_states\n",
    "            )\n",
    "            if b is None:\n",
    "                raise RuntimeError(f\"Not enough data to simulate: {comm}\")\n",
    "            bundles[comm] = b\n",
    "    return bundles\n",
    "\n",
    "def plot_cvae_fullcurve_panels_cv_only(\n",
    "    communes=TARGET_COMMUNES,\n",
    "    results_dict=None,\n",
    "    ncols=2,\n",
    "    fig_width=16,\n",
    "    row_height=5.0,\n",
    "    save_basename=\"paper_panels_CVAE_only_50band_with_observed\",\n",
    "    title=\"Full-curve reconstructions — Observed & CVAE (median, mean, 50% band)\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a panel (3x2 for six communes) with:\n",
    "      - Observed curve (black solid)\n",
    "      - CVAE 50% band (lighter blue, more transparent)\n",
    "      - CVAE median (blue dashed)\n",
    "      - CVAE mean (orange solid; no dashed)\n",
    "    \"\"\"\n",
    "    bundles = _ensure_bundles_for(communes, results_dict)\n",
    "\n",
    "    n = len(communes)\n",
    "    ncols = max(1, ncols)\n",
    "    nrows = int(math.ceil(n / ncols))\n",
    "\n",
    "    # Publication-oriented aesthetics\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": \"DejaVu Sans\",\n",
    "        \"axes.titlesize\": 13,\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "        \"legend.fontsize\": 11,\n",
    "        \"savefig.dpi\": 600,   # high-resolution export\n",
    "    })\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows, ncols,\n",
    "        figsize=(fig_width, row_height * nrows),\n",
    "        sharex=False, sharey=False\n",
    "    )\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    # Colors & styles\n",
    "    col_obs   = \"#000000\"   # observed (black)\n",
    "    col_median= \"#1f77b4\"   # blue for median\n",
    "    col_mean  = \"#ff7f0e\"   # orange for mean\n",
    "    col_band  = \"#8bb6e8\"   # slightly lighter blue for the band\n",
    "    alpha50   = 0.20        # clearer (more transparent) band\n",
    "    lw_obs    = 2.0\n",
    "    lw_line   = 2.0\n",
    "\n",
    "    for ax, comm in zip(axes, communes):\n",
    "        b = bundles[comm]\n",
    "        dates = b[\"dates\"]\n",
    "\n",
    "        # 50% band (lighter + clearer)\n",
    "        ax.fill_between(dates, b[\"q25\"], b[\"q75\"],\n",
    "                        color=col_band, alpha=alpha50, edgecolor=\"none\")\n",
    "\n",
    "        # Observed\n",
    "        ax.plot(dates, b[\"truth\"], color=col_obs, lw=lw_obs, label=None)\n",
    "\n",
    "        # CVAE median (dashed) & mean (solid, no dashes)\n",
    "        ax.plot(dates, b[\"q50\"],  color=col_median, linestyle=\"--\", lw=lw_line, label=None)  # median\n",
    "        ax.plot(dates, b[\"mean\"], color=col_mean,   linestyle=\"-\",  lw=lw_line, label=None)  # mean (solid)\n",
    "\n",
    "        ax.set_title(comm)\n",
    "        ax.set_ylabel(\"Daily cases (7-day MA)\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Hide any unused subplots (if communes % ncols != 0)\n",
    "    for k in range(len(communes), len(axes)):\n",
    "        axes[k].axis(\"off\")\n",
    "\n",
    "    # Global legend\n",
    "    legend_elems = [\n",
    "        Patch(facecolor=col_band, edgecolor=\"none\", alpha=alpha50, label=\"CVAE 50% band\"),\n",
    "        Line2D([0], [0], color=col_median, lw=lw_line, linestyle=\"--\", label=\"CVAE median\"),\n",
    "        Line2D([0], [0], color=col_mean,   lw=lw_line, linestyle=\"-\",  label=\"CVAE mean\"),\n",
    "        Line2D([0], [0], color=col_obs,    lw=lw_obs,  linestyle=\"-\",  label=\"Observed\"),\n",
    "    ]\n",
    "    fig.legend(legend_elems, [h.get_label() for h in legend_elems],\n",
    "               loc=\"lower center\", ncol=4, frameon=False)\n",
    "\n",
    "    # Layout + optional super-title\n",
    "    if title:\n",
    "        fig.suptitle(title, y=0.99, fontsize=14)\n",
    "        fig.tight_layout(rect=[0, 0.07, 1, 0.96])\n",
    "    else:\n",
    "        fig.tight_layout(rect=[0, 0.07, 1, 1])\n",
    "\n",
    "    # Save both PNG and PDF (publication-ready vector)\n",
    "    png_path = os.path.join(OUT_DIR, f\"{save_basename}.png\")\n",
    "    pdf_path = os.path.join(OUT_DIR, f\"{save_basename}.pdf\")\n",
    "    fig.savefig(png_path, dpi=600, bbox_inches=\"tight\")\n",
    "    fig.savefig(pdf_path, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved: {png_path}\")\n",
    "    print(f\"✓ Saved: {pdf_path}\")\n",
    "\n",
    "# ---- Run it (generates the 3×2 panel for the six target communes) ----\n",
    "plot_cvae_fullcurve_panels_cv_only(\n",
    "    communes=TARGET_COMMUNES,\n",
    "    results_dict=None,        # pass results_full if you already have it; otherwise it simulates on the fly\n",
    "    ncols=2,                  # 2 columns × 3 rows\n",
    "    fig_width=16,\n",
    "    row_height=5.0,\n",
    "    save_basename=\"paper_panels_CVAE_only_50band_with_observed\",\n",
    "    title=\"Full-curve reconstructions — Observed & CVAE (median, mean, 50% band)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8e7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Paper figure: CVAE-only full-curve panels for target communes\n",
    "# - Shows: Observed (black), CVAE median (blue, dashed), CVAE mean (orange, solid)\n",
    "# - Band: central 50% (slightly lighter, aesthetic)\n",
    "# - Per-panel metrics card (top-right): AW50, COV50, AAD50, CRPS, MAE, RMSE\n",
    "# - High-quality export: PNG (600 dpi) and PDF\n",
    "# ============================================================\n",
    "\n",
    "import os, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# -------- Target communes (edit if needed) --------\n",
    "TARGET_COMMUNES = [\"La Florida\", \"Cerrillos\", \"Vitacura\",\n",
    "                   \"Providencia\", \"Las Condes\", \"Santiago\"]\n",
    "\n",
    "# -------- Safe OUT_DIR fallback --------\n",
    "try:\n",
    "    OUT_DIR  # if defined upstream, keep it\n",
    "except NameError:\n",
    "    OUT_DIR = \"./eval_out\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------- CRPS (ensemble) & asymmetry helpers ----------\n",
    "def crps_ens(y, samples):\n",
    "    s = np.sort(np.asarray(samples).ravel())\n",
    "    n = len(s); y = float(y)\n",
    "    e1 = np.mean(np.abs(s - y))  # E|X - y|\n",
    "    diffs = np.diff(s)\n",
    "    weights = np.arange(1, n) * (n - np.arange(1, n))\n",
    "    e2 = 2.0 * np.sum(weights * diffs) / (n*n)  # E|X - X'|\n",
    "    return float(e1 - 0.5*e2)\n",
    "\n",
    "def asymmetry_degree(lo, med, hi):\n",
    "    \"\"\"Signed asymmetry in [-1,1]; >0 = longer upper tail.\"\"\"\n",
    "    lo = float(lo); med = float(med); hi = float(hi)\n",
    "    width = max(hi - lo, 1e-12)\n",
    "    return ((hi - med) - (med - lo)) / width\n",
    "\n",
    "# --------- Metrics from a simulation bundle ----------\n",
    "def _metrics_from_bundle(bundle):\n",
    "    \"\"\"\n",
    "    Expects keys: truth, q25, q50, q75, sims\n",
    "    Returns dict with AW50, COV50, AAD50 (signed), CRPS, MAE, RMSE\n",
    "    \"\"\"\n",
    "    y   = np.asarray(bundle[\"truth\"], float)\n",
    "    q25 = np.asarray(bundle[\"q25\"],  float)\n",
    "    q50 = np.asarray(bundle[\"q50\"],  float)\n",
    "    q75 = np.asarray(bundle[\"q75\"],  float)\n",
    "\n",
    "    # Width & coverage\n",
    "    width50 = q75 - q25\n",
    "    AW50  = float(np.mean(width50))\n",
    "    COV50 = float(np.mean((y >= q25) & (y <= q75)))\n",
    "\n",
    "    # AAD50 (signed)\n",
    "    AAD50 = float(np.mean([asymmetry_degree(lo, md, hi) for lo, md, hi in zip(q25, q50, q75)]))\n",
    "\n",
    "    # CRPS averaged in time (requires sims)\n",
    "    sims = bundle.get(\"sims\", None)\n",
    "    if sims is not None and sims.size > 0:\n",
    "        CRPS = float(np.mean([crps_ens(y[t], sims[:, t]) for t in range(len(y))]))\n",
    "    else:\n",
    "        CRPS = float(\"nan\")\n",
    "\n",
    "    # Point accuracy (vs median)\n",
    "    MAE  = float(np.mean(np.abs(y - q50)))\n",
    "    RMSE = float(np.sqrt(np.mean((y - q50)**2)))\n",
    "\n",
    "    return {\"AW50\": AW50, \"COV50\": COV50, \"AAD50\": AAD50, \"CRPS\": CRPS, \"MAE\": MAE, \"RMSE\": RMSE}\n",
    "\n",
    "# --------- Ensure bundles for communes ----------\n",
    "def _ensure_bundles_for(communes,\n",
    "                        results_dict=None,\n",
    "                        n_sims=400,\n",
    "                        temp_z=1.25,\n",
    "                        temp_out=1.25,\n",
    "                        use_soft_states=False):\n",
    "    \"\"\"\n",
    "    Make sure we have the full-curve simulation bundles for each commune.\n",
    "    Uses 'results_full' if available; otherwise calls simulate_full_curve_cvae.\n",
    "    Returns: dict {commune: bundle}\n",
    "      bundle keys: dates, truth, q05, q25, q50, q75, q95, mean, sims\n",
    "    \"\"\"\n",
    "    # Need df_scaled and simulate_full_curve_cvae in scope\n",
    "    try:\n",
    "        df_scaled  # noqa\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"df_scaled not found in scope. Load and prepare your dataframe first.\")\n",
    "    try:\n",
    "        simulate_full_curve_cvae  # noqa\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"simulate_full_curve_cvae not found. Define/import it before plotting.\")\n",
    "\n",
    "    bundles = {}\n",
    "    # Reuse a previous dict produced by run_full_curve_all\n",
    "    if results_dict is None:\n",
    "        try:\n",
    "            results_dict = results_full  # optional global\n",
    "        except NameError:\n",
    "            results_dict = None\n",
    "\n",
    "    groups = df_scaled.groupby(\"Commune\")\n",
    "    for comm in communes:\n",
    "        if (results_dict is not None) and (comm in results_dict):\n",
    "            bundles[comm] = results_dict[comm][\"bundle\"]\n",
    "        else:\n",
    "            if comm not in groups.groups:\n",
    "                raise RuntimeError(f\"Commune '{comm}' not present in df_scaled.\")\n",
    "            dfc = groups.get_group(comm).sort_values(\"Date\").reset_index(drop=True)\n",
    "            b = simulate_full_curve_cvae(\n",
    "                dfc, n_sims=n_sims, temp_z=temp_z, temp_out=temp_out,\n",
    "                use_soft_states=use_soft_states\n",
    "            )\n",
    "            if b is None:\n",
    "                raise RuntimeError(f\"Not enough data to simulate: {comm}\")\n",
    "            bundles[comm] = b\n",
    "    return bundles\n",
    "\n",
    "# --------- Plot panels (with metrics box top-right) ----------\n",
    "def plot_cvae_fullcurve_panels_with_metrics(\n",
    "    communes=TARGET_COMMUNES,\n",
    "    results_dict=None,\n",
    "    ncols=2,\n",
    "    fig_width=16,\n",
    "    row_height=5.0,\n",
    "    save_basename=\"paper_panels_CVAE_50band_with_observed_metrics\",\n",
    "    title=\"Full-curve reconstructions — Observed & CVAE (median, mean, 50% band)\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a multi-panel figure (e.g., 3x2 for 6 communes) with:\n",
    "      - Observed (black solid), CVAE median (blue dashed), CVAE mean (orange solid)\n",
    "      - 50% central band (light blue, subtle)\n",
    "      - Metrics card (AW50, COV50, AAD50, CRPS, MAE, RMSE) top-right in each panel\n",
    "    Saves PNG (600 dpi) and PDF to OUT_DIR.\n",
    "    \"\"\"\n",
    "    bundles = _ensure_bundles_for(communes, results_dict)\n",
    "\n",
    "    n = len(communes)\n",
    "    ncols = max(1, ncols)\n",
    "    nrows = int(math.ceil(n / ncols))\n",
    "\n",
    "    # Publication-oriented aesthetics\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": \"DejaVu Sans\",\n",
    "        \"axes.titlesize\": 13,\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "        \"legend.fontsize\": 11,\n",
    "        \"savefig.dpi\": 600,   # high-resolution export\n",
    "    })\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows, ncols,\n",
    "        figsize=(fig_width, row_height * nrows),\n",
    "        sharex=False, sharey=False\n",
    "    )\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    # Colors & styles\n",
    "    col_obs    = \"#000000\"   # observed\n",
    "    col_median = \"#1f77b4\"   # median\n",
    "    col_mean   = \"#ff7f0e\"   # mean (solid)\n",
    "    col_band   = \"#b7c1c9\"   # slightly lighter than before\n",
    "    alpha50    = 0.40\n",
    "    lw_obs     = 2.0\n",
    "    lw_line    = 2.0\n",
    "\n",
    "    for ax, comm in zip(axes, communes):\n",
    "        b = bundles[comm]\n",
    "        dates = b[\"dates\"]\n",
    "\n",
    "        # 50% band (lighter & clean)\n",
    "        ax.fill_between(dates, b[\"q25\"], b[\"q75\"],\n",
    "                        color=col_band, alpha=alpha50, edgecolor=\"none\")\n",
    "\n",
    "        # Lines: observed (solid), median (dashed), mean (solid)\n",
    "        ax.plot(dates, b[\"truth\"], color=col_obs,   lw=lw_obs)\n",
    "        ax.plot(dates, b[\"q50\"],   color=col_median, linestyle=\"--\", lw=lw_line)\n",
    "        ax.plot(dates, b[\"mean\"],  color=col_mean,   linestyle=\"-\",  lw=lw_line)\n",
    "\n",
    "        ax.set_title(comm)\n",
    "        ax.set_ylabel(\"Daily cases (7-day MA)\")\n",
    "        ax.grid(True, alpha=0.7)\n",
    "\n",
    "        # ---------- Metrics card (top-right) ----------\n",
    "        m = _metrics_from_bundle(b)\n",
    "        lines = [\n",
    "            f\"AW50: {m['AW50']:.1f}\",\n",
    "            f\"COV50: {m['COV50']:.3f}\",\n",
    "            f\"AAD50: {m['AAD50']:.3f}\",\n",
    "            f\"CRPS: {m['CRPS']:.2f}\" if not np.isnan(m[\"CRPS\"]) else \"CRPS: —\",\n",
    "            f\"MAE: {m['MAE']:.2f}\",\n",
    "            f\"RMSE: {m['RMSE']:.2f}\",\n",
    "        ]\n",
    "        txt = \"\\n\".join(lines)\n",
    "\n",
    "        ax.text(\n",
    "            0.98, 0.98, txt,                 # top-right corner in axes coords\n",
    "            transform=ax.transAxes,\n",
    "            va=\"top\", ha=\"right\",\n",
    "            fontsize=12.0,                   # slightly larger\n",
    "            color=\"#222222\",\n",
    "            bbox=dict(boxstyle=\"round,pad=0.35,rounding_size=0.12\",\n",
    "                      fc=\"white\", ec=\"#4c4d4f\", lw=0.9, alpha=0.88)\n",
    "        )\n",
    "\n",
    "    # Hide any unused subplots (if communes % ncols != 0)\n",
    "    for k in range(len(communes), len(axes)):\n",
    "        axes[k].axis(\"off\")\n",
    "\n",
    "    # Global legend\n",
    "    legend_elems = [\n",
    "        Patch(facecolor=col_band, edgecolor=\"none\", alpha=alpha50, label=\"CVAE 50% band\"),\n",
    "        Line2D([0], [0], color=col_median, lw=lw_line, linestyle=\"--\", label=\"CVAE median\"),\n",
    "        Line2D([0], [0], color=col_mean,   lw=lw_line, linestyle=\"-\",  label=\"CVAE mean\"),\n",
    "        Line2D([0], [0], color=col_obs,    lw=lw_obs,  linestyle=\"-\",  label=\"Observed\"),\n",
    "    ]\n",
    "    fig.legend(legend_elems, [h.get_label() for h in legend_elems],\n",
    "               loc=\"lower center\", ncol=4, frameon=False)\n",
    "\n",
    "    # Layout and export\n",
    "    if title:\n",
    "        fig.suptitle(title, y=0.99, fontsize=14)\n",
    "        fig.tight_layout(rect=[0, 0.07, 1, 0.96])\n",
    "    else:\n",
    "        fig.tight_layout(rect=[0, 0.07, 1, 1])\n",
    "\n",
    "    png_path = os.path.join(OUT_DIR, f\"{save_basename}.png\")\n",
    "    pdf_path = os.path.join(OUT_DIR, f\"{save_basename}.pdf\")\n",
    "    fig.savefig(png_path, dpi=600, bbox_inches=\"tight\")\n",
    "    fig.savefig(pdf_path, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved: {png_path}\")\n",
    "    print(f\"✓ Saved: {pdf_path}\")\n",
    "\n",
    "# ---- Run it (generates the 3×2 panel for the six target communes) ----\n",
    "plot_cvae_fullcurve_panels_with_metrics(\n",
    "    communes=TARGET_COMMUNES,\n",
    "    results_dict=None,        # pass results_full if you have it; else it will simulate on the fly\n",
    "    ncols=2,                  # 2 columns × 3 rows\n",
    "    fig_width=16,\n",
    "    row_height=5.0,\n",
    "    save_basename=\"paper_panels_CVAE_50band_with_observed_metrics\",\n",
    "    title=\"Full-curve reconstructions — Observed & CVAE (median, mean, 50% band)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6cb7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cori Rt (EpiEstim-like) from observed & CVAE-simulated curves\n",
    "# - Discretized serial interval (Gamma) with mean/sd configurable\n",
    "# - Sliding-window Cori posterior (Gamma) with user priors\n",
    "# - Rt for observed series + Rt across CVAE simulations (quantiles)\n",
    "# - CSV export + publication-quality panel plots (3×2)\n",
    "# ============================================================\n",
    "\n",
    "import os, math, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration (you can tweak)\n",
    "# -----------------------------\n",
    "SI_MEAN = 5.0          # mean of serial interval (days)\n",
    "SI_SD   = 2.0          # std  of serial interval (days)\n",
    "SI_MAX  = 30           # support truncation for w_s (days)\n",
    "\n",
    "TAU     = 7            # Cori sliding window length\n",
    "A0      = 1.0          # Gamma prior shape for R_t\n",
    "B0      = 1.0          # Gamma prior rate  for R_t (mean = A0/B0)\n",
    "\n",
    "ALPHA_CR = 0.05        # 95% credible intervals\n",
    "\n",
    "# Colors for Rt plots\n",
    "COL_OBS_LINE  = \"#111111\"  # observed Rt median (black-ish)\n",
    "COL_OBS_BAND  = \"#777777\"  # observed 95% band (grey)\n",
    "COL_CVAE_LINE = \"#1f77b4\"  # CVAE Rt median\n",
    "COL_CVAE_BAND = \"#1f77b433\" # CVAE band (transparent-ish blue)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# (Optional) ensure we have CVAE bundles if results_full is missing\n",
    "# ----------------------------------------------------------\n",
    "def ensure_cvae_bundles(communes, n_sims=400, temp_z=1.25, temp_out=1.25, use_soft_states=False):\n",
    "    bundles = {}\n",
    "    try:\n",
    "        _res = results_full  # try global\n",
    "    except NameError:\n",
    "        _res = None\n",
    "    groups = df_scaled.groupby(\"Commune\")\n",
    "    for comm in communes:\n",
    "        if (_res is not None) and (comm in _res):\n",
    "            bundles[comm] = _res[comm][\"bundle\"]\n",
    "        else:\n",
    "            dfc = groups.get_group(comm).sort_values(\"Date\").reset_index(drop=True)\n",
    "            b = simulate_full_curve_cvae(\n",
    "                dfc, n_sims=n_sims, temp_z=temp_z, temp_out=temp_out, use_soft_states=use_soft_states\n",
    "            )\n",
    "            if b is None:\n",
    "                raise RuntimeError(f\"Not enough data to simulate: {comm}\")\n",
    "            bundles[comm] = b\n",
    "    return bundles\n",
    "\n",
    "# ----------------------------------------\n",
    "# Discretized Gamma PMF for serial interval\n",
    "# ----------------------------------------\n",
    "def _gamma_k_mean_sd(mean, sd):\n",
    "    # shape k, scale theta for Gamma(k, theta) with given mean/sd\n",
    "    theta = (sd**2) / mean\n",
    "    k = mean / theta\n",
    "    return k, theta\n",
    "\n",
    "def discretized_serial_interval(mean=SI_MEAN, sd=SI_SD, max_days=SI_MAX, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Return w[1..max_days], where w_s = P(serial interval = s).\n",
    "    Discretization as probability mass at integer days via Gamma CDF differences.\n",
    "    \"\"\"\n",
    "    from math import erf, sqrt\n",
    "    # We do Gamma(k, theta) via scipy-like CDF approximation using numpy's gammainc? Not available.\n",
    "    # Implement with numpy's gammainc: lower incomplete gamma normalized is available via math? Not.\n",
    "    # => Use numerical integration by differences on continuous pdf at half-integers (midpoint rule).\n",
    "    # Robust and accurate enough for daily bins.\n",
    "    k, theta = _gamma_k_mean_sd(mean, sd)\n",
    "    xs = np.arange(0.5, max_days + 0.5)  # midpoints of bins [s-0.5, s+0.5]\n",
    "    # Gamma pdf: f(x) = x^{k-1} e^{-x/theta} / (Gamma(k) theta^k)\n",
    "    # Use log-domain for stability\n",
    "    from math import lgamma\n",
    "    logZ = lgamma(k) + k * math.log(theta)\n",
    "    pdf = np.exp((k - 1) * np.log(np.clip(xs, eps, None)) - xs / theta - logZ)\n",
    "    w = pdf / (pdf.sum() + eps)\n",
    "    # shift to w[1..max_days]\n",
    "    return w\n",
    "\n",
    "# ----------------------------------------\n",
    "# Infectivity term Λ_t by discrete convolution\n",
    "# ----------------------------------------\n",
    "def infectivity_lambda(I, w):\n",
    "    \"\"\"\n",
    "    I: incidence array (length T), non-negative floats\n",
    "    w: serial interval pmf for lags 1..L\n",
    "    Returns Λ of length T, with Λ[t] = sum_{s=1}^{min(t, L)} I[t-s] * w[s]\n",
    "    \"\"\"\n",
    "    T = len(I)\n",
    "    L = len(w)\n",
    "    lam = np.zeros(T, dtype=float)\n",
    "    for t in range(T):\n",
    "        smax = min(t, L)\n",
    "        if smax > 0:\n",
    "            past = I[t-smax:t][::-1]      # I[t-1], I[t-2], ...\n",
    "            lam[t] = float(np.dot(past, w[:smax]))\n",
    "    return lam\n",
    "\n",
    "# ----------------------------------------\n",
    "# Cori posterior for a sliding window\n",
    "# ----------------------------------------\n",
    "def cori_posterior_rt(I, w, tau=TAU, a0=A0, b0=B0, alpha=ALPHA_CR, eps=1e-12):\n",
    "    \"\"\"\n",
    "    EpiEstim-like Cori method with Gamma(a0,b0) prior (rate parameterization).\n",
    "    For each day t>=tau:\n",
    "       Posterior is Gamma(a_post, b_post) with\n",
    "       a_post = a0 + sum_{k=t-tau+1..t} I_k\n",
    "       b_post = b0 + sum_{k=t-tau+1..t} Λ_k,    Λ_k = sum_{s>=1} I_{k-s} w_s\n",
    "    Returns DataFrame with posterior mean/median and CI.\n",
    "    \"\"\"\n",
    "    I = np.asarray(I, dtype=float)\n",
    "    w = np.asarray(w, dtype=float)\n",
    "    T = len(I)\n",
    "    lam = infectivity_lambda(I, w) + 0.0\n",
    "\n",
    "    # precompute cumulative sums\n",
    "    cI = np.cumsum(I)\n",
    "    cL = np.cumsum(lam)\n",
    "\n",
    "    means  = np.full(T, np.nan)\n",
    "    med    = np.full(T, np.nan)\n",
    "    qlo    = np.full(T, np.nan)\n",
    "    qhi    = np.full(T, np.nan)\n",
    "    ashape = np.full(T, np.nan)\n",
    "    arate  = np.full(T, np.nan)\n",
    "\n",
    "    from scipy.stats import gamma as sg  # if SciPy not available, use numpy.random.gamma quantile fallback\n",
    "    have_scipy = True\n",
    "    try:\n",
    "        _ = sg.cdf(1.0, 1.0, scale=1.0)\n",
    "    except Exception:\n",
    "        have_scipy = False\n",
    "\n",
    "    for t in range(tau-1, T):\n",
    "        i_sum = cI[t] - (cI[t - tau] if t - tau >= 0 else 0.0)\n",
    "        l_sum = cL[t] - (cL[t - tau] if t - tau >= 0 else 0.0)\n",
    "        a_post = a0 + max(i_sum, 0.0)\n",
    "        b_post = b0 + max(l_sum, 0.0) + eps  # avoid zero-rate\n",
    "\n",
    "        means[t]  = a_post / b_post\n",
    "        ashape[t] = a_post\n",
    "        arate[t]  = b_post\n",
    "\n",
    "        if have_scipy:\n",
    "            # SciPy uses shape k, scale = 1/rate\n",
    "            scale = 1.0 / b_post\n",
    "            med[t] = sg.ppf(0.5, a_post, scale=scale)\n",
    "            qlo[t] = sg.ppf(alpha/2.0, a_post, scale=scale)\n",
    "            qhi[t] = sg.ppf(1.0 - alpha/2.0, a_post, scale=scale)\n",
    "        else:\n",
    "            # Fallback: approx median ~ k*(1 - 1/(9k))^3 / rate for large k\n",
    "            k = a_post\n",
    "            rate = b_post\n",
    "            med[t] = (k * (1 - 1/(9*max(k,1e-6)))**3) / rate\n",
    "            # crude quantiles via Wilson-Hilferty; acceptable for large k, just a fallback\n",
    "            from math import sqrt\n",
    "            z = 1.96\n",
    "            qlo[t] = (k * (1 - z*sqrt(2/(9*k)))**3) / rate\n",
    "            qhi[t] = (k * (1 + z*sqrt(2/(9*k)))**3) / rate\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"Rt_mean\": means, \"Rt_median\": med, \"Rt_qlo\": qlo, \"Rt_qhi\": qhi,\n",
    "        \"shape\": ashape, \"rate\": arate\n",
    "    })\n",
    "    return out\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Rt for observed series and for CVAE simulations\n",
    "# ----------------------------------------------------\n",
    "def rt_observed_for_commune(commune, w=None, tau=TAU, a0=A0, b0=B0):\n",
    "    dfc = df_scaled[df_scaled[\"Commune\"]==commune].sort_values(\"Date\").reset_index(drop=True)\n",
    "    dates = dfc[\"Date\"].values\n",
    "    I = np.asarray(dfc[OBS_COL].values, dtype=float)  # observed incidence (7d MA in your data)\n",
    "    if w is None:\n",
    "        w = discretized_serial_interval()\n",
    "    R = cori_posterior_rt(I, w, tau=tau, a0=a0, b0=b0)\n",
    "    R.insert(0, \"Date\", dates)\n",
    "    R.insert(1, \"Commune\", commune)\n",
    "    return R\n",
    "\n",
    "def rt_cvae_for_commune(commune, bundles=None, w=None, tau=TAU, a0=A0, b0=B0,\n",
    "                        qlo=0.025, qhi=0.975):\n",
    "    if bundles is None:\n",
    "        bundles = ensure_cvae_bundles([commune])\n",
    "    b = bundles[commune]\n",
    "    sims  = np.asarray(b[\"sims\"])          # (M, steps) steps = T-SEQ_LEN\n",
    "    steps = sims.shape[1]\n",
    "    # Build full incidence per sim: observed prefix (length SEQ_LEN) + simulated path\n",
    "    dfc = df_scaled[df_scaled[\"Commune\"]==commune].sort_values(\"Date\").reset_index(drop=True)\n",
    "    I_prefix = np.asarray(dfc[OBS_COL].values[:SEQ_LEN], dtype=float)\n",
    "    T_full   = len(dfc)\n",
    "\n",
    "    if w is None:\n",
    "        w = discretized_serial_interval()\n",
    "\n",
    "    # For each simulation, compute Rt posterior summary and keep the **posterior mean** time-series\n",
    "    Rt_mean_all = np.zeros((sims.shape[0], T_full), dtype=float) * np.nan\n",
    "    for m in range(sims.shape[0]):\n",
    "        I_full = np.concatenate([I_prefix, sims[m,:]], axis=0)  # length T_full\n",
    "        Rm = cori_posterior_rt(I_full, w, tau=tau, a0=a0, b0=b0)\n",
    "        Rt_mean_all[m, :] = Rm[\"Rt_mean\"].values\n",
    "\n",
    "    # Aggregate across simulations (drop the prefix for reporting, align with b[\"dates\"])\n",
    "    Rt_mean_sims = Rt_mean_all[:, :]  # keep full for plotting observed vs cvae if needed\n",
    "    Rt_qlo = np.nanquantile(Rt_mean_sims[:, SEQ_LEN:], qlo, axis=0)\n",
    "    Rt_med = np.nanquantile(Rt_mean_sims[:, SEQ_LEN:], 0.5, axis=0)\n",
    "    Rt_qhi = np.nanquantile(Rt_mean_sims[:, SEQ_LEN:], qhi, axis=0)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"Date\": np.asarray(b[\"dates\"]),         # SEQ_LEN..end\n",
    "        \"Commune\": commune,\n",
    "        \"Rt_cvae_median\": Rt_med,\n",
    "        \"Rt_cvae_qlo\": Rt_qlo,\n",
    "        \"Rt_cvae_qhi\": Rt_qhi\n",
    "    })\n",
    "    return out\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Batch: compute & save CSVs for target communes\n",
    "# ----------------------------------------------------\n",
    "def compute_and_save_rt_all(communes=TARGET_COMMUNES,\n",
    "                            tau=TAU, a0=A0, b0=B0,\n",
    "                            si_mean=SI_MEAN, si_sd=SI_SD, si_max=SI_MAX,\n",
    "                            out_dir=OUT_DIR):\n",
    "    w = discretized_serial_interval(mean=si_mean, sd=si_sd, max_days=si_max)\n",
    "    bundles = ensure_cvae_bundles(communes)\n",
    "\n",
    "    rows_obs, rows_cvae = [], []\n",
    "    for comm in communes:\n",
    "        R_obs = rt_observed_for_commune(comm, w=w, tau=tau, a0=a0, b0=b0)\n",
    "        R_obs.to_csv(os.path.join(out_dir, f\"rt_observed_{comm.replace(' ','_')}.csv\"), index=False)\n",
    "        rows_obs.append(R_obs)\n",
    "\n",
    "        R_cv  = rt_cvae_for_commune(comm, bundles=bundles, w=w, tau=tau, a0=a0, b0=b0)\n",
    "        R_cv.to_csv(os.path.join(out_dir, f\"rt_cvae_{comm.replace(' ','_')}.csv\"), index=False)\n",
    "        rows_cvae.append(R_cv)\n",
    "\n",
    "    obs_all  = pd.concat(rows_obs, ignore_index=True)\n",
    "    cvae_all = pd.concat(rows_cvae, ignore_index=True)\n",
    "    obs_all.to_csv(os.path.join(out_dir, \"rt_observed_all.csv\"), index=False)\n",
    "    cvae_all.to_csv(os.path.join(out_dir, \"rt_cvae_all.csv\"), index=False)\n",
    "    print(f\"✓ Saved Rt CSVs in {out_dir}\")\n",
    "    return obs_all, cvae_all, w\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Plot panels (3×2) of Rt for target communes\n",
    "# ----------------------------------------------------\n",
    "def plot_rt_panels(communes=TARGET_COMMUNES, tau=TAU, a0=A0, b0=B0,\n",
    "                   si_mean=SI_MEAN, si_sd=SI_SD, si_max=SI_MAX,\n",
    "                   out_path=None, title=r\"Time-varying reproduction number $R_t$ (Cori)\"):\n",
    "    w = discretized_serial_interval(mean=si_mean, sd=si_sd, max_days=si_max)\n",
    "    bundles = ensure_cvae_bundles(communes)\n",
    "\n",
    "    # Aesthetics for paper\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": \"DejaVu Sans\",\n",
    "        \"axes.titlesize\": 13,\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "        \"legend.fontsize\": 11,\n",
    "        \"savefig.dpi\": 600,\n",
    "    })\n",
    "\n",
    "    n = len(communes); ncols=2; nrows=int(math.ceil(n/ncols))\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(16, 4.6*nrows), sharex=False)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    legend_elems = [\n",
    "        Patch(facecolor=COL_OBS_BAND, edgecolor=\"none\", alpha=0.25, label=\"Observed 95% CI\"),\n",
    "        Line2D([0],[0], color=COL_OBS_LINE, lw=2.0, label=\"Observed median\"),\n",
    "        Patch(facecolor=COL_CVAE_BAND, edgecolor=\"none\", alpha=1.0, label=\"CVAE band (sim median)\"),\n",
    "        Line2D([0],[0], color=COL_CVAE_LINE, lw=2.0, label=\"CVAE median\"),\n",
    "    ]\n",
    "\n",
    "    for ax, comm in zip(axes, communes):\n",
    "        # observed\n",
    "        R_obs = rt_observed_for_commune(comm, w=w, tau=tau, a0=a0, b0=b0)\n",
    "        ax.fill_between(R_obs[\"Date\"], R_obs[\"Rt_qlo\"], R_obs[\"Rt_qhi\"],\n",
    "                        color=COL_OBS_BAND, alpha=0.25, edgecolor=\"none\")\n",
    "        ax.plot(R_obs[\"Date\"], R_obs[\"Rt_median\"], color=COL_OBS_LINE, lw=2.0)\n",
    "\n",
    "        # cvae (across sims)\n",
    "        R_cv  = rt_cvae_for_commune(comm, bundles=bundles, w=w, tau=tau, a0=a0, b0=b0)\n",
    "        ax.fill_between(R_cv[\"Date\"], R_cv[\"Rt_cvae_qlo\"], R_cv[\"Rt_cvae_qhi\"],\n",
    "                        color=COL_CVAE_BAND, edgecolor=\"none\")\n",
    "        ax.plot(R_cv[\"Date\"], R_cv[\"Rt_cvae_median\"], color=COL_CVAE_LINE, lw=2.0)\n",
    "\n",
    "        ax.axhline(1.0, color=\"#444444\", lw=1.2, ls=\"--\", alpha=0.7)\n",
    "        ax.set_title(comm)\n",
    "        ax.set_ylabel(r\"$R_t$\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # hide unused axes\n",
    "    for k in range(len(communes), len(axes)):\n",
    "        axes[k].axis(\"off\")\n",
    "\n",
    "    fig.legend(legend_elems, [h.get_label() for h in legend_elems],\n",
    "               loc=\"lower center\", ncol=4, frameon=False)\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title, y=0.99, fontsize=14)\n",
    "        fig.tight_layout(rect=[0,0.05,1,0.96])\n",
    "    else:\n",
    "        fig.tight_layout(rect=[0,0.05,1,1])\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = os.path.join(OUT_DIR, \"rt_cori_panels.png\")\n",
    "    fig.savefig(out_path, dpi=600, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved: {out_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# Run it\n",
    "# -------------------------\n",
    "obs_all, cvae_all, w_si = compute_and_save_rt_all(\n",
    "    communes=TARGET_COMMUNES,\n",
    "    tau=TAU, a0=A0, b0=B0,\n",
    "    si_mean=SI_MEAN, si_sd=SI_SD, si_max=SI_MAX,\n",
    "    out_dir=OUT_DIR\n",
    ")\n",
    "plot_rt_panels(\n",
    "    communes=TARGET_COMMUNES,\n",
    "    tau=TAU, a0=A0, b0=B0,\n",
    "    si_mean=SI_MEAN, si_sd=SI_SD, si_max=SI_MAX,\n",
    "    out_path=os.path.join(OUT_DIR, \"rt_cori_panels.png\"),\n",
    "    title=r\"Time-varying reproduction number $R_t$ (Cori): observed vs. CVAE\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87dc0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Robust setup for OBS_COL + Wallinga–Teunis run ------------------\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 1) Asegura una columna con incidencia observada en unidades originales\n",
    "#    Usa tu columna fuente (p.ej. CASE_COL) para poblarla si aún no existe.\n",
    "if 'OBS_COL' not in locals():\n",
    "    OBS_COL = \"Observed_Cases\"   # nombre que usaremos para la columna de observados\n",
    "\n",
    "# Si df_scaled aún no tiene la columna de observados en unidades originales, créala.\n",
    "if OBS_COL not in df_scaled.columns:\n",
    "    # Asumimos que df (no escalado) y CASE_COL existen y están en unidades originales\n",
    "    # (como configuraste en tu flujo corregido).\n",
    "    if ('df' in locals()) and (CASE_COL in df.columns):\n",
    "        df_scaled[OBS_COL] = df[CASE_COL].values\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f\"No encuentro datos originales para crear {OBS_COL}. \"\n",
    "            f\"Asegúrate de tener 'df' sin escalar y la columna CASE_COL='{CASE_COL}'.\"\n",
    "        )\n",
    "\n",
    "# Limpiezas y checks mínimos\n",
    "df_scaled[OBS_COL] = np.asarray(df_scaled[OBS_COL], float)\n",
    "df_scaled[OBS_COL] = np.clip(df_scaled[OBS_COL], a_min=0.0, a_max=None)  # sin negativos\n",
    "assert df_scaled.groupby(\"Commune\")[OBS_COL].apply(lambda s: s.notna().all()).all(), \\\n",
    "    f\"Hay NaNs en {OBS_COL} para alguna comuna.\"\n",
    "\n",
    "# 2) Parámetros SI y paths\n",
    "SI_MEAN = 4.7\n",
    "SI_SD   = 2.9\n",
    "SI_MAX  = 28\n",
    "B_SAMPLES = 400\n",
    "\n",
    "WT_DIR = os.path.join(OUT_DIR, \"WT\")\n",
    "os.makedirs(WT_DIR, exist_ok=True)\n",
    "\n",
    "# 3) Ejecuta WT usando la columna OBS_COL (¡sin comillas!)\n",
    "WT_RES = run_wt_all(\n",
    "    df_scaled,\n",
    "    communes=TARGET_COMMUNES,\n",
    "    si_mean=SI_MEAN,\n",
    "    si_sd=SI_SD,\n",
    "    si_max=SI_MAX,\n",
    "    mode=\"sampled\",               # \"expected\" = más rápido (curva puntual), \"sampled\" = con bandas\n",
    "    B=B_SAMPLES,\n",
    "    correct_right_censor=True,\n",
    "    out_dir=WT_DIR,\n",
    "    use_column=OBS_COL            # <-- variable, no string literal\n",
    ")\n",
    "\n",
    "# 4) Panel de alta calidad (2x3)\n",
    "WT_FIG_PATH = os.path.join(WT_DIR, \"wt_panels.png\")\n",
    "plot_wt_panels(\n",
    "    WT_RES,\n",
    "    TARGET_COMMUNES,\n",
    "    ncols=2,\n",
    "    fig_width=16,\n",
    "    row_height=5.0,\n",
    "    title=\"Wallinga–Teunis case reproduction number (observed incidence)\",\n",
    "    save_path=WT_FIG_PATH\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Listo. CSVs en: {WT_DIR}\")\n",
    "print(f\"✓ Figura: {WT_FIG_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4648bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Unified Rt panels: Cori (observed), Cori (CVAE), WT — per commune\n",
    "# - Bands: 95% (Cori-observed, gray), 90% (Cori-CVAE, blue), 90% (WT, green)\n",
    "# - Lines: medians (solid), plus Rt=1 reference\n",
    "# - Publication quality (high DPI)\n",
    "# ============================================================\n",
    "\n",
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from datetime import datetime\n",
    "\n",
    "# Example list (adjust as needed)\n",
    "TARGET_COMMUNES = [\"La Florida\", \"Cerrillos\", \"Vitacura\",\n",
    "                   \"Providencia\", \"Las Condes\", \"Santiago\"]\n",
    "\n",
    "def _series_from(dict_, keys, name):\n",
    "    \"\"\"Build a DataFrame (Date index) from dictionary arrays with given keys.\"\"\"\n",
    "    # keys: {\"date\":\"dates\", \"med\":\"Rt_median\", \"lo\":\"Rt_lo\", \"hi\":\"Rt_hi\"}\n",
    "    ds = pd.to_datetime(pd.Series(dict_[keys[\"date\"]], dtype=\"object\"))\n",
    "    df = pd.DataFrame({\n",
    "        f\"{name}_med\": dict_[keys[\"med\"]],\n",
    "        f\"{name}_lo\":  dict_[keys[\"lo\"]],\n",
    "        f\"{name}_hi\":  dict_[keys[\"hi\"]],\n",
    "    }, index=ds)\n",
    "    df = df[~df.index.duplicated(keep=\"first\")].sort_index()\n",
    "    return df\n",
    "\n",
    "def _last_invalid_span(valid_arr):\n",
    "    \"\"\"Return start date of the last invalid (0) run, else None.\"\"\"\n",
    "    v = np.asarray(valid_arr, int)\n",
    "    if v.size == 0 or v[-1] != 0:\n",
    "        return None\n",
    "    # find last transition from 1 -> 0\n",
    "    i = len(v) - 1\n",
    "    while i >= 0 and v[i] == 0:\n",
    "        i -= 1\n",
    "    return i + 1  # first invalid index\n",
    "\n",
    "def plot_rt_unified_panels(\n",
    "    RT_RES, WT_RES, communes=TARGET_COMMUNES,\n",
    "    ncols=2, fig_width=16, row_height=5.0,\n",
    "    save_path=None, title=\"Time-varying reproduction number $R_t$: Cori (obs & CVAE) and WT\"\n",
    "):\n",
    "    \"\"\"\n",
    "    RT_RES: dict[commune] -> {dates, Rt_obs_median, Rt_obs_lo, Rt_obs_hi,\n",
    "                              Rt_cvae_median, Rt_cvae_lo, Rt_cvae_hi}\n",
    "    WT_RES: dict[commune] -> {dates, Rt_median, Rt_lo, Rt_hi, valid(optional)}\n",
    "    \"\"\"\n",
    "    # Aesthetics\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": \"DejaVu Sans\",\n",
    "        \"axes.titlesize\": 12.5,\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "        \"legend.fontsize\": 10.5,\n",
    "        \"savefig.dpi\": 600,\n",
    "    })\n",
    "\n",
    "    # Colors & alpha\n",
    "    col_obs_band = \"#808080\"   # gray\n",
    "    col_obs_line = \"#222222\"   # near-black\n",
    "    col_cvae_band = \"#4C78A8\"  # blue-ish\n",
    "    col_cvae_line = \"#2F5B8B\"\n",
    "    col_wt_band  = \"#59A14F\"   # green-ish\n",
    "    col_wt_line  = \"#2E7D32\"\n",
    "    a_obs, a_cvae, a_wt = 0.18, 0.22, 0.22\n",
    "    lw = 2.0\n",
    "\n",
    "    n = len(communes)\n",
    "    nrows = int(math.ceil(n / ncols))\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(fig_width, row_height*nrows), sharex=False, sharey=False)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for ax, comm in zip(axes, communes):\n",
    "        if comm not in RT_RES or comm not in WT_RES:\n",
    "            ax.text(0.5, 0.5, f\"No data for {comm}\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "            ax.axis(\"off\"); continue\n",
    "\n",
    "        # Build aligned series (inner join on dates)\n",
    "        rt = RT_RES[comm]\n",
    "        wt = WT_RES[comm]\n",
    "\n",
    "        df_obs = _series_from(rt,\n",
    "                              {\"date\":\"dates\",\"med\":\"Rt_obs_median\",\"lo\":\"Rt_obs_lo\",\"hi\":\"Rt_obs_hi\"},\n",
    "                              \"obs\")\n",
    "        df_cvae = _series_from(rt,\n",
    "                               {\"date\":\"dates\",\"med\":\"Rt_cvae_median\",\"lo\":\"Rt_cvae_lo\",\"hi\":\"Rt_cvae_hi\"},\n",
    "                               \"cvae\")\n",
    "        df_wt = _series_from(wt,\n",
    "                             {\"date\":\"dates\",\"med\":\"Rt_median\",\"lo\":\"Rt_lo\",\"hi\":\"Rt_hi\"},\n",
    "                             \"wt\")\n",
    "\n",
    "        # Align by intersection of dates (safer to avoid edge artifacts)\n",
    "        df = df_obs.join(df_cvae, how=\"inner\").join(df_wt, how=\"inner\")\n",
    "        if df.empty:\n",
    "            ax.text(0.5, 0.5, f\"No aligned dates for {comm}\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "            ax.axis(\"off\"); continue\n",
    "\n",
    "        dates = df.index\n",
    "\n",
    "        # Bands\n",
    "        ax.fill_between(dates, df[\"obs_lo\"],  df[\"obs_hi\"],  color=col_obs_band,  alpha=a_obs,  edgecolor=\"none\", label=None)\n",
    "        ax.fill_between(dates, df[\"cvae_lo\"], df[\"cvae_hi\"], color=col_cvae_band, alpha=a_cvae, edgecolor=\"none\", label=None)\n",
    "        ax.fill_between(dates, df[\"wt_lo\"],   df[\"wt_hi\"],   color=col_wt_band,   alpha=a_wt,   edgecolor=\"none\", label=None)\n",
    "\n",
    "        # Medians\n",
    "        ax.plot(dates, df[\"obs_med\"],  color=col_obs_line,  lw=lw, label=None)\n",
    "        ax.plot(dates, df[\"cvae_med\"], color=col_cvae_line, lw=lw, label=None)\n",
    "        ax.plot(dates, df[\"wt_med\"],   color=col_wt_line,   lw=lw, label=None)\n",
    "\n",
    "        # Rt=1 reference\n",
    "        ax.axhline(1.0, color=\"#000000\", lw=1.0, ls=\":\", alpha=0.8)\n",
    "\n",
    "        # Right-censor shading (if WT provides validity mask)\n",
    "        if \"valid\" in wt:\n",
    "            valid = np.asarray(wt[\"valid\"], int)\n",
    "            if len(valid) == len(wt[\"dates\"]):\n",
    "                idx0 = _last_invalid_span(valid)\n",
    "                if idx0 is not None and idx0 < len(wt[\"dates\"]):\n",
    "                    t0 = pd.to_datetime(wt[\"dates\"][idx0])\n",
    "                    t1 = pd.to_datetime(wt[\"dates\"][-1])\n",
    "                    ax.axvspan(t0, t1, color=\"#9E9E9E\", alpha=0.12, lw=0, zorder=0)\n",
    "\n",
    "        ax.set_title(comm)\n",
    "        ax.set_ylabel(r\"$R_t$\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Hide unused axes\n",
    "    for k in range(len(communes), len(axes)):\n",
    "        axes[k].axis(\"off\")\n",
    "\n",
    "    # Legend\n",
    "    legend_elems = [\n",
    "        Patch(facecolor=col_obs_band,  edgecolor=\"none\", alpha=a_obs,  label=\"Cori (obs) 95% band\"),\n",
    "        Line2D([0],[0], color=col_obs_line,  lw=lw, label=\"Cori (obs) median\"),\n",
    "        Patch(facecolor=col_cvae_band, edgecolor=\"none\", alpha=a_cvae, label=\"Cori (CVAE) 90% band\"),\n",
    "        Line2D([0],[0], color=col_cvae_line, lw=lw, label=\"Cori (CVAE) median\"),\n",
    "        Patch(facecolor=col_wt_band,   edgecolor=\"none\", alpha=a_wt,   label=\"WT 90% band\"),\n",
    "        Line2D([0],[0], color=col_wt_line,   lw=lw, label=\"WT median\"),\n",
    "        Line2D([0],[0], color=\"#000000\", lw=1.0, ls=\":\", label=r\"$R_t=1$\")\n",
    "    ]\n",
    "    fig.legend(legend_elems, [h.get_label() for h in legend_elems],\n",
    "               loc=\"lower center\", ncol=4, frameon=False)\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title, y=0.995, fontsize=14)\n",
    "\n",
    "    fig.tight_layout(rect=[0,0.05,1,0.98])\n",
    "    if save_path is None:\n",
    "        save_path = os.path.join(OUT_DIR, \"rt_unified_panels.png\")\n",
    "    fig.savefig(save_path, dpi=600, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved: {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a329b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Discretize serial interval as a Gamma-based discrete PMF\n",
    "# (mid-point approximation; numerically stable, SciPy-free)\n",
    "# ------------------------------------------------------------\n",
    "def discretize_si_gamma(mean_si=4.7, sd_si=2.9, max_days=28):\n",
    "    \"\"\"Return w with shape (max_days+1,), w[0]=0 by convention, sum(w)=1 over 1..max_days.\"\"\"\n",
    "    mu, sd = float(mean_si), float(sd_si)\n",
    "    k = (mu / sd) ** 2             # shape\n",
    "    theta = (sd ** 2) / mu         # scale\n",
    "    # Mid-point rule for discrete days 1..max_days\n",
    "    s = np.arange(1, max_days + 1, dtype=float)\n",
    "    # Gamma pdf at mid-points (s - 0.5), clipped to >= 1e-6 to avoid 0 underflow\n",
    "    x = np.clip(s - 0.5, 1e-6, None)\n",
    "    pdf_mid = (x**(k-1) * np.exp(-x/theta)) / (np.math.gamma(k) * (theta**k))\n",
    "    w = pdf_mid / pdf_mid.sum()\n",
    "    # prepend w[0]=0 for convenience in convolutions\n",
    "    w = np.concatenate([[0.0], w])\n",
    "    return w\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Renewal equation pieces: lambda_t and Cori posterior\n",
    "# ------------------------------------------------------------\n",
    "def _renewal_lambda(I, w):\n",
    "    \"\"\"\n",
    "    I: incidence array (T,)\n",
    "    w: PMF array with w[0]=0, len = L+1 (L = si_max)\n",
    "    Returns lambda array (T,), lambda[t] = sum_{s=1..min(t,L)} I[t-s]*w[s]\n",
    "    \"\"\"\n",
    "    T = len(I)\n",
    "    L = len(w) - 1\n",
    "    lam = np.zeros(T, dtype=float)\n",
    "    for t in range(T):\n",
    "        smax = min(t, L)\n",
    "        if smax > 0:\n",
    "            # reversed slice of last smax incidences * w[1:smax+1]\n",
    "            lam[t] = np.dot(I[t-smax:t][::-1], w[1:smax+1])\n",
    "    return lam\n",
    "\n",
    "def _cori_posterior(I, w, tau=7, a0=1.0, b0=5.0, draws=400, rng=None):\n",
    "    \"\"\"\n",
    "    Cori et al. posterior with Gamma(shape=a, rate=b).\n",
    "    Window [t-tau+1, t] assumes Rt is constant in that window.\n",
    "    Returns dict with mean,q05,q50,q95,valid (NaN outside valid).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(123)\n",
    "    I = np.asarray(I, float)\n",
    "    T = len(I)\n",
    "    lam = _renewal_lambda(I, w)\n",
    "\n",
    "    Rt_mean = np.full(T, np.nan, float)\n",
    "    q05 = np.full(T, np.nan, float)\n",
    "    q50 = np.full(T, np.nan, float)\n",
    "    q95 = np.full(T, np.nan, float)\n",
    "    valid = np.zeros(T, dtype=bool)\n",
    "\n",
    "    # rolling sums for speed\n",
    "    csum_I = np.cumsum(I)\n",
    "    csum_lam = np.cumsum(lam)\n",
    "\n",
    "    for t in range(T):\n",
    "        if t < tau - 1:\n",
    "            continue\n",
    "        i0 = t - tau + 1\n",
    "        sum_I = csum_I[t] - (csum_I[i0-1] if i0 > 0 else 0.0)\n",
    "        sum_lam = csum_lam[t] - (csum_lam[i0-1] if i0 > 0 else 0.0)\n",
    "\n",
    "        a = a0 + sum_I\n",
    "        b = b0 + max(0.0, sum_lam)\n",
    "        if b <= 0.0:\n",
    "            continue\n",
    "\n",
    "        Rt_mean[t] = a / b\n",
    "        # Sample draws from Gamma(shape=a, scale=1/b)\n",
    "        samp = rng.gamma(shape=a, scale=1.0/b, size=draws)\n",
    "        q05[t], q50[t], q95[t] = np.quantile(samp, [0.05, 0.50, 0.95])\n",
    "        valid[t] = True\n",
    "\n",
    "    return {\"mean\": Rt_mean, \"q05\": q05, \"q50\": q50, \"q95\": q95, \"valid\": valid}\n",
    "\n",
    "def _cori_posterior_from_sims(I_obs_full, sims, w, tau=7, a0=1.0, b0=5.0,\n",
    "                              draws_per_sim=1, rng=None):\n",
    "    \"\"\"\n",
    "    Aggregate Cori posterior over CVAE simulations.\n",
    "    I_obs_full: observed incidence over full length T (array)\n",
    "    sims: array (M, steps) = trajectories simulated por el CVAE\n",
    "    We stitch: I_full^(m) = concat(I_obs_full[:T-steps], sims[m,:])\n",
    "    Returns dict with aggregated mean (across sims) and quantiles (from posterior draws across sims).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(123)\n",
    "    M, steps = sims.shape\n",
    "    T = len(I_obs_full)\n",
    "    hist = T - steps\n",
    "    assert hist >= 1, \"Sim length inconsistent with observed length.\"\n",
    "\n",
    "    # Storage\n",
    "    means_acc = np.zeros(T, dtype=float)\n",
    "    draws_acc = [[] for _ in range(T)]\n",
    "    valid_any = np.zeros(T, dtype=bool)\n",
    "\n",
    "    for m in range(M):\n",
    "        I_full = np.concatenate([I_obs_full[:hist], sims[m, :]], axis=0)\n",
    "        lam = _renewal_lambda(I_full, w)\n",
    "\n",
    "        csum_I = np.cumsum(I_full)\n",
    "        csum_lam = np.cumsum(lam)\n",
    "\n",
    "        for t in range(T):\n",
    "            if t < tau - 1:\n",
    "                continue\n",
    "            i0 = t - tau + 1\n",
    "            sum_I = csum_I[t] - (csum_I[i0-1] if i0 > 0 else 0.0)\n",
    "            sum_lam = csum_lam[t] - (csum_lam[i0-1] if i0 > 0 else 0.0)\n",
    "            a = a0 + sum_I\n",
    "            b = b0 + max(0.0, sum_lam)\n",
    "            if b <= 0.0:\n",
    "                continue\n",
    "            means_acc[t] += a / b\n",
    "            # one (or few) posterior draw(s) per sim to build an ensemble of draws\n",
    "            if draws_per_sim > 0:\n",
    "                samp = np.atleast_1d(np.random.gamma(shape=a, scale=1.0/b, size=draws_per_sim))\n",
    "                draws_acc[t].extend(samp.tolist())\n",
    "            valid_any[t] = True\n",
    "\n",
    "    # Aggregate\n",
    "    Rt_mean = np.full(T, np.nan, float)\n",
    "    q05 = np.full(T, np.nan, float)\n",
    "    q50 = np.full(T, np.nan, float)\n",
    "    q95 = np.full(T, np.nan, float)\n",
    "\n",
    "    nz = (valid_any & (np.array([len(x) for x in draws_acc]) > 0))\n",
    "    if np.any(valid_any):\n",
    "        Rt_mean[valid_any] = means_acc[valid_any] / float(M)\n",
    "    for t in np.where(nz)[0]:\n",
    "        arr = np.asarray(draws_acc[t], float)\n",
    "        q05[t], q50[t], q95[t] = np.quantile(arr, [0.05, 0.50, 0.95])\n",
    "\n",
    "    return {\"mean\": Rt_mean, \"q05\": q05, \"q50\": q50, \"q95\": q95, \"valid\": valid_any}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Public runner: Cori over communes (observed / CVAE / both)\n",
    "# ------------------------------------------------------------\n",
    "def run_rt_cori_all(df_scaled,\n",
    "                    results_dict=None,         # dict tipo results_full (para \"sim\" o \"both\")\n",
    "                    communes=None,\n",
    "                    si_mean=4.7, si_sd=2.9, si_max=28,\n",
    "                    tau=7,\n",
    "                    mode=\"both\",               # \"obs\" | \"sim\" | \"both\"\n",
    "                    B=400,                     # draws por día para \"obs\"; y 1 draw/sim para \"sim\"\n",
    "                    use_column=\"Observed_Cases\",\n",
    "                    out_dir=None,\n",
    "                    a0=1.0, b0=5.0,\n",
    "                    seed=123):\n",
    "    \"\"\"\n",
    "    Devuelve un dict:\n",
    "      RT_RES[comm] = {\n",
    "         \"dates\": np.array(datetime64),\n",
    "         \"Rt_obs\": {mean,q05,q50,q95,valid}  (si mode incluye obs)\n",
    "         \"Rt_sim\": {mean,q05,q50,q95,valid}  (si mode incluye sim)\n",
    "      }\n",
    "    Si out_dir no es None, guarda CSV por comuna.\n",
    "    \"\"\"\n",
    "    assert mode in (\"obs\", \"sim\", \"both\")\n",
    "    if communes is None:\n",
    "        communes = sorted(df_scaled[\"Commune\"].unique().tolist())\n",
    "    groups = df_scaled.groupby(\"Commune\")\n",
    "    w = discretize_si_gamma(mean_si=si_mean, sd_si=si_sd, max_days=si_max)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    results = {}\n",
    "    for comm in communes:\n",
    "        dfc = groups.get_group(comm).sort_values(\"Date\").reset_index(drop=True)\n",
    "        if use_column not in dfc.columns:\n",
    "            raise KeyError(f\"{use_column} not found in df for commune {comm}.\")\n",
    "        dates = dfc[\"Date\"].values\n",
    "        I_obs = np.asarray(dfc[use_column].values, float)\n",
    "        out_comm = {\"dates\": dates}\n",
    "\n",
    "        if mode in (\"obs\", \"both\"):\n",
    "            post_obs = _cori_posterior(I_obs, w, tau=tau, a0=a0, b0=b0, draws=B, rng=rng)\n",
    "            out_comm[\"Rt_obs\"] = post_obs\n",
    "\n",
    "        if mode in (\"sim\", \"both\"):\n",
    "            if (results_dict is None) or (comm not in results_dict):\n",
    "                raise ValueError(f\"results_dict missing CVAE sims for {comm}.\")\n",
    "            sims = np.asarray(results_dict[comm][\"bundle\"][\"sims\"], float)  # (M, steps)\n",
    "            post_sim = _cori_posterior_from_sims(\n",
    "                I_obs_full=I_obs, sims=sims, w=w, tau=tau, a0=a0, b0=b0,\n",
    "                draws_per_sim=1, rng=rng\n",
    "            )\n",
    "            out_comm[\"Rt_sim\"] = post_sim\n",
    "\n",
    "        results[comm] = out_comm\n",
    "\n",
    "        # Optional CSVs\n",
    "        if out_dir is not None:\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            df_out = pd.DataFrame({\"Date\": dates})\n",
    "            if \"Rt_obs\" in out_comm:\n",
    "                df_out[\"Rt_obs_mean\"] = out_comm[\"Rt_obs\"][\"mean\"]\n",
    "                df_out[\"Rt_obs_q05\"]  = out_comm[\"Rt_obs\"][\"q05\"]\n",
    "                df_out[\"Rt_obs_q50\"]  = out_comm[\"Rt_obs\"][\"q50\"]\n",
    "                df_out[\"Rt_obs_q95\"]  = out_comm[\"Rt_obs\"][\"q95\"]\n",
    "                df_out[\"Rt_obs_valid\"]= out_comm[\"Rt_obs\"][\"valid\"].astype(int)\n",
    "            if \"Rt_sim\" in out_comm:\n",
    "                df_out[\"Rt_sim_mean\"] = out_comm[\"Rt_sim\"][\"mean\"]\n",
    "                df_out[\"Rt_sim_q05\"]  = out_comm[\"Rt_sim\"][\"q05\"]\n",
    "                df_out[\"Rt_sim_q50\"]  = out_comm[\"Rt_sim\"][\"q50\"]\n",
    "                df_out[\"Rt_sim_q95\"]  = out_comm[\"Rt_sim\"][\"q95\"]\n",
    "                df_out[\"Rt_sim_valid\"]= out_comm[\"Rt_sim\"][\"valid\"].astype(int)\n",
    "            df_out.to_csv(os.path.join(out_dir, f\"rt_cori_{comm.replace(' ','_')}.csv\"), index=False)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b515a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def _get_quant_series_from_nested(d, prefix=\"Rt_obs\"):\n",
    "    \"\"\"\n",
    "    Extrae (med, lo, hi) desde un dict anidado con estructura:\n",
    "      d[prefix] = {\"mean\",\"q05\",\"q50\",\"q95\",\"valid\"}\n",
    "    Devuelve dict con keys: med, lo, hi (o None si no existe).\n",
    "    \"\"\"\n",
    "    if d is None or prefix not in d:\n",
    "        return None\n",
    "    blk = d[prefix]\n",
    "    if not all(k in blk for k in (\"q05\",\"q50\",\"q95\")):\n",
    "        return None\n",
    "    return {\"med\": np.asarray(blk[\"q50\"], float),\n",
    "            \"lo\":  np.asarray(blk[\"q05\"], float),\n",
    "            \"hi\":  np.asarray(blk[\"q95\"], float)}\n",
    "\n",
    "def _get_quant_series_from_flat(d, med_key, lo_key, hi_key):\n",
    "    \"\"\"\n",
    "    Extrae (med, lo, hi) desde claves planas (p.ej. WT con Rt_q50, Rt_q05, Rt_q95)\n",
    "    \"\"\"\n",
    "    if not all(k in d for k in (med_key, lo_key, hi_key)):\n",
    "        return None\n",
    "    return {\"med\": np.asarray(d[med_key], float),\n",
    "            \"lo\":  np.asarray(d[lo_key],  float),\n",
    "            \"hi\":  np.asarray(d[hi_key],  float)}\n",
    "\n",
    "def _dates_array(d):\n",
    "    # d[\"dates\"] puede venir como numpy datetime64 o lista de timestamps\n",
    "    return pd.to_datetime(pd.Series(d[\"dates\"], dtype=\"object\")).values\n",
    "\n",
    "def plot_rt_unified_panels(RT_RES, WT_RES, communes,\n",
    "                           ncols=2,\n",
    "                           fig_width=16,\n",
    "                           row_height=5.0,\n",
    "                           save_path=None,\n",
    "                           title=r\"Time-varying reproduction number $R_t$: Cori (obs \\& CVAE) and WT\"):\n",
    "    \"\"\"\n",
    "    Panel unificado por comuna:\n",
    "      - Cori (observado): banda 90% + mediana sólida\n",
    "      - Cori (CVAE sims): banda 90% + mediana discontinua\n",
    "      - Wallinga–Teunis: mediana (línea punto-trazo)\n",
    "      - Línea horizontal R_t = 1\n",
    "    Soporta estructuras anidadas (run_rt_cori_all) y planas (WT).\n",
    "    \"\"\"\n",
    "    # Estética de publicación\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": \"DejaVu Sans\",\n",
    "        \"axes.titlesize\": 13,\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "        \"legend.fontsize\": 11,\n",
    "        \"savefig.dpi\": 600,\n",
    "    })\n",
    "\n",
    "    n = len(communes)\n",
    "    ncols = max(1, ncols)\n",
    "    nrows = int(math.ceil(n / ncols))\n",
    "    fig, axes = plt.subplots(nrows, ncols,\n",
    "                             figsize=(fig_width, row_height*nrows),\n",
    "                             sharex=False, sharey=False)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    # Colores/estilos\n",
    "    col_obs   = \"#1f77b4\"  # azul: Cori (obs)\n",
    "    col_sim   = \"#ff7f0e\"  # naranjo: Cori (CVAE)\n",
    "    col_wt    = \"#2ca02c\"  # verde: WT\n",
    "    alpha_obs = 0.20\n",
    "    alpha_sim = 0.15\n",
    "    lw_med    = 2.0\n",
    "\n",
    "    for ax, comm in zip(axes, communes):\n",
    "        # ----- Cori (obs/sim) desde RT_RES -----\n",
    "        if comm not in RT_RES:\n",
    "            ax.text(0.5, 0.5, f\"Missing in RT_RES:\\n{comm}\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        rt = RT_RES[comm]\n",
    "        dates = _dates_array(rt)\n",
    "\n",
    "        # Cori observado (anidado)\n",
    "        s_obs = _get_quant_series_from_nested(rt, \"Rt_obs\")\n",
    "        # Cori CVAE (anidado)\n",
    "        s_sim = _get_quant_series_from_nested(rt, \"Rt_sim\")\n",
    "\n",
    "        # ----- WT desde WT_RES (puede venir con claves planas) -----\n",
    "        s_wt = None\n",
    "        if (WT_RES is not None) and (comm in WT_RES):\n",
    "            wt = WT_RES[comm]\n",
    "            # Admite dos convenciones: (\"q50\",\"q05\",\"q95\") o (\"Rt_q50\",\"Rt_q05\",\"Rt_q95\")\n",
    "            if all(k in wt for k in (\"q50\",\"q05\",\"q95\")):\n",
    "                s_wt = _get_quant_series_from_flat(wt, \"q50\",\"q05\",\"q95\")\n",
    "            else:\n",
    "                s_wt = _get_quant_series_from_flat(wt, \"Rt_q50\",\"Rt_q05\",\"Rt_q95\")\n",
    "            dates_wt = _dates_array(wt) if \"dates\" in wt else dates\n",
    "        else:\n",
    "            dates_wt = dates\n",
    "\n",
    "        # ----- Plots -----\n",
    "        # Banda y mediana Cori (obs)\n",
    "        if s_obs is not None:\n",
    "            ax.fill_between(dates, s_obs[\"lo\"], s_obs[\"hi\"],\n",
    "                            color=col_obs, alpha=alpha_obs, edgecolor=\"none\",\n",
    "                            label=\"Cori (obs) 90%\")\n",
    "            ax.plot(dates, s_obs[\"med\"], color=col_obs, lw=lw_med, linestyle=\"-\",\n",
    "                    label=\"Cori (obs) median\")\n",
    "\n",
    "        # Banda y mediana Cori (CVAE)\n",
    "        if s_sim is not None:\n",
    "            ax.fill_between(dates, s_sim[\"lo\"], s_sim[\"hi\"],\n",
    "                            color=col_sim, alpha=alpha_sim, edgecolor=\"none\",\n",
    "                            label=\"Cori (CVAE) 90%\")\n",
    "            ax.plot(dates, s_sim[\"med\"], color=col_sim, lw=lw_med, linestyle=\"--\",\n",
    "                    label=\"Cori (CVAE) median\")\n",
    "\n",
    "        # WT mediana\n",
    "        if s_wt is not None:\n",
    "            ax.plot(dates_wt, s_wt[\"med\"], color=col_wt, lw=1.8, linestyle=\"-.\",\n",
    "                    label=\"WT median\")\n",
    "\n",
    "        # Línea crítica R_t = 1\n",
    "        ax.axhline(1.0, color=\"gray\", lw=1.2, linestyle=\":\", alpha=0.8)\n",
    "\n",
    "        ax.set_title(comm)\n",
    "        ax.set_ylabel(r\"$R_t$\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Ocultar subplots vacíos\n",
    "    for k in range(len(communes), len(axes)):\n",
    "        axes[k].axis(\"off\")\n",
    "\n",
    "    # Leyenda global\n",
    "    legend_elems = [\n",
    "        Patch(facecolor=col_obs, edgecolor=\"none\", alpha=alpha_obs, label=\"Cori (obs) 90%\"),\n",
    "        Line2D([0],[0], color=col_obs, lw=lw_med, linestyle=\"-\", label=\"Cori (obs) median\"),\n",
    "        Patch(facecolor=col_sim, edgecolor=\"none\", alpha=alpha_sim, label=\"Cori (CVAE) 90%\"),\n",
    "        Line2D([0],[0], color=col_sim, lw=lw_med, linestyle=\"--\", label=\"Cori (CVAE) median\"),\n",
    "        Line2D([0],[0], color=col_wt,  lw=1.8,   linestyle=\"-.\", label=\"WT median\"),\n",
    "        Line2D([0],[0], color=\"gray\",  lw=1.2,   linestyle=\":\",  label=r\"$R_t=1$\")\n",
    "    ]\n",
    "    fig.legend(legend_elems, [h.get_label() for h in legend_elems],\n",
    "               loc=\"lower center\", ncol=6, frameon=False)\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title, y=0.99, fontsize=14)\n",
    "        fig.tight_layout(rect=[0, 0.06, 1, 0.965])\n",
    "    else:\n",
    "        fig.tight_layout(rect=[0, 0.06, 1, 1])\n",
    "\n",
    "    if save_path is None:\n",
    "        save_path = \"rt_unified_panels.png\"\n",
    "    fig.savefig(save_path, dpi=600, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "RT_RES = run_rt_cori_all(\n",
    "    df_scaled=df_scaled,\n",
    "    results_dict=results_for_rt,    # o results_full si ya lo tienes\n",
    "    communes=TARGET_COMMUNES,\n",
    "    si_mean=SI_MEAN, si_sd=SI_SD, si_max=SI_MAX,\n",
    "    tau=TAU,\n",
    "    mode=\"both\",\n",
    "    B=B_SAMPLES,\n",
    "    use_column=OBS_COL,\n",
    "    out_dir=RT_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6df28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) Asegurar OBS_COL en df_scaled (incidencia en unidades originales)\n",
    "# ============================================================\n",
    "OBS_COL = \"Observed_Cases\"\n",
    "\n",
    "def attach_observed_cases(df_scaled_in, CASE_COL=CASE_COL):\n",
    "    \"\"\"Devuelve df_scaled con OBS_COL en unidades originales.\n",
    "    Estrategia:\n",
    "      1) Si 'df' (datos crudos) existe → copia directa.\n",
    "      2) Si hay 'scaler' → inverse_transform.\n",
    "      3) Fallback con CASE_MIN/CASE_MAX (MinMax).\n",
    "    \"\"\"\n",
    "    df_s = df_scaled_in.copy()\n",
    "    if OBS_COL in df_s.columns:\n",
    "        return df_s\n",
    "\n",
    "    # (1) Copia desde df crudo si está en memoria\n",
    "    try:\n",
    "        if ('df' in globals()) and (CASE_COL in df.columns):\n",
    "            df_s[OBS_COL] = df[CASE_COL].values\n",
    "            return df_s\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # (2) Inversa vía scaler si está disponible\n",
    "    try:\n",
    "        _ = scaler.data_min_  # comprobar que es el MinMax ya entrenado\n",
    "        idx = FEATURES.index(CASE_COL)\n",
    "        X_scaled = df_s[FEATURES].values\n",
    "        X_unscaled = scaler.inverse_transform(X_scaled)\n",
    "        df_s[OBS_COL] = X_unscaled[:, idx]\n",
    "        return df_s\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # (3) Fallback solo con min/max del caso\n",
    "    try:\n",
    "        rng = CASE_MAX - CASE_MIN\n",
    "        df_s[OBS_COL] = CASE_MIN + df_s[CASE_COL].values * rng\n",
    "        return df_s\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"Cannot reconstruct OBS_COL; re-ejecuta el bloque de carga que define 'df' y 'scaler'.\"\n",
    "        ) from e\n",
    "\n",
    "# Aplicar el fix\n",
    "df_scaled = attach_observed_cases(df_scaled, CASE_COL=CASE_COL)\n",
    "\n",
    "# ============================================================\n",
    "# 1) Bundles CVAE (si no están en memoria) para las comunas objetivo\n",
    "# ============================================================\n",
    "TARGET_COMMUNES = [\"La Florida\", \"Cerrillos\", \"Vitacura\",\n",
    "                   \"Providencia\", \"Las Condes\", \"Santiago\"]\n",
    "\n",
    "import os\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    _ = [results_full[c] for c in TARGET_COMMUNES]\n",
    "    results_for_rt = results_full\n",
    "except Exception:\n",
    "    results_for_rt, _, _ = run_full_curve_all(\n",
    "        df_scaled,\n",
    "        communes=TARGET_COMMUNES,\n",
    "        n_sims=400,\n",
    "        temp_z=1.25,\n",
    "        temp_out=1.25,\n",
    "        use_soft_states=False,\n",
    "        out_dir=OUT_DIR\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# 2) Cori (observado + CVAE) -> RT_RES\n",
    "# ============================================================\n",
    "SI_MEAN, SI_SD, SI_MAX = 4.7, 2.9, 28\n",
    "TAU = 7\n",
    "B_SAMPLES = 400\n",
    "\n",
    "RT_DIR = os.path.join(OUT_DIR, \"RT_Cori\")\n",
    "os.makedirs(RT_DIR, exist_ok=True)\n",
    "\n",
    "RT_RES = run_rt_cori_all(\n",
    "    df_scaled=df_scaled,\n",
    "    results_dict=results_for_rt,\n",
    "    communes=TARGET_COMMUNES,\n",
    "    si_mean=SI_MEAN, si_sd=SI_SD, si_max=SI_MAX,\n",
    "    tau=TAU,\n",
    "    mode=\"both\",            # 'obs' | 'sim' | 'both'\n",
    "    B=B_SAMPLES,\n",
    "    use_column=OBS_COL,\n",
    "    out_dir=RT_DIR\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 3) Wallinga–Teunis (observado) -> WT_RES\n",
    "# ============================================================\n",
    "WT_DIR = os.path.join(OUT_DIR, \"RT_WT\")\n",
    "os.makedirs(WT_DIR, exist_ok=True)\n",
    "\n",
    "WT_RES = run_wt_all(\n",
    "    df_scaled=df_scaled,\n",
    "    communes=TARGET_COMMUNES,\n",
    "    si_mean=SI_MEAN, si_sd=SI_SD, si_max=SI_MAX,\n",
    "    mode=\"sampled\",         # 'expected' | 'sampled'\n",
    "    B=B_SAMPLES,\n",
    "    correct_right_censor=True,\n",
    "    out_dir=WT_DIR,\n",
    "    use_column=OBS_COL\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 4) Panel unificado: Cori (obs & CVAE) + WT\n",
    "# ============================================================\n",
    "plot_rt_unified_panels(\n",
    "    RT_RES=RT_RES,\n",
    "    WT_RES=WT_RES,\n",
    "    communes=TARGET_COMMUNES,\n",
    "    ncols=2,\n",
    "    fig_width=16,\n",
    "    row_height=5.0,\n",
    "    save_path=os.path.join(OUT_DIR, \"rt_unified_panels.png\"),\n",
    "    title=r\"Time-varying reproduction number $R_t$: Cori (obs \\& CVAE) and WT\"\n",
    ")\n",
    "\n",
    "print(\"✓ Panel guardado en:\", os.path.join(OUT_DIR, \"rt_unified_panels.png\"))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
